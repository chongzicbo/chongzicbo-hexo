{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/hexo-theme-snippet/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/assets/highlight.pack.js","path":"assets/highlight.pack.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/assets/tagcanvas.min.js","path":"assets/tagcanvas.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/assets/valine.min.js","path":"assets/valine.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/css/bootstrap.min.css","path":"css/bootstrap.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/css/font-awesome.min.css","path":"css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/css/style.css","path":"css/style.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/fonts/FontAwesome.otf","path":"fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.eot","path":"fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.svg","path":"fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.ttf","path":"fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.woff","path":"fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.woff2","path":"fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/js/app.js","path":"js/app.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/img/avatar.jpg","path":"img/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/img/branding.png","path":"img/branding.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/img/head-img.jpg","path":"img/head-img.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/img/reward-wepay.jpg","path":"img/reward-wepay.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/img/timeline-clock.gif","path":"img/timeline-clock.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/img/timeline-dot.gif","path":"img/timeline-dot.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-snippet/source/img/timeline.gif","path":"img/timeline.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/css/gitalk.css","path":"css/gitalk.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/css/highlight-dark.styl","path":"css/highlight-dark.styl","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/boot.js","path":"js/boot.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/color-schema.js","path":"js/color-schema.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/events.js","path":"js/events.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/img-lazyload.js","path":"js/img-lazyload.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/leancloud.js","path":"js/leancloud.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/plugins.js","path":"js/plugins.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/umami-view.js","path":"js/umami-view.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/img/default.png","path":"img/default.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/img/fluid.png","path":"img/fluid.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/img/police_beian.png","path":"img/police_beian.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-fluid/source/xml/local-search.xml","path":"xml/local-search.xml","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"2d08e621defcf2c16c392dd8104cd4e1dee41793","modified":1733571535143},{"_id":"source/_posts/computer-vision/CV007-YOLO11详解.md","hash":"a8ca92d1a2dfe22a67316483933ce1c7578a1be8","modified":1733571717743},{"_id":"source/_posts/computer-vision/CV008-评估 YOLO （You Only Look Once） 模型的演变：YOLO11 及其前身的全面基准研究.md","hash":"27a272f3e1bc7c7b6baebda166c564ab5668326a","modified":1733576839881},{"_id":"source/_posts/computer-vision/CV010-YOLO V10详解.md","hash":"13a4369132301d685c8beae0867ff34862dce695","modified":1733571727464},{"_id":"themes/hexo-theme-snippet/.gitignore","hash":"3edbd274f96d4d4e42b295862adc64cbb08f1603","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/LICENSE","hash":"c976aafe3bc44246a978850b18102966afe3eeb5","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/.travis.yml","hash":"3d38c11d32f1f3910fddc6489973c4a1fbbd0fa8","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/_config.yml","hash":"806f4ce18274632697f2768e23775c102863254f","modified":1733572666028},{"_id":"themes/hexo-theme-snippet/README.md","hash":"ec6e2cc0483e18dce1214182c1e174c7dba149b3","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/_travis.sh","hash":"653c1b22509c614828977f49aa4e0c22b7d37fa6","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/gulpfile.js","hash":"7a0fa22f93c1b652f5d7a61e26bf2bccd5f0cda4","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/package-lock.json","hash":"bbc103a90e0e1b06528092c0292cc24382e5f448","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/package.json","hash":"63e9cb99767cc6bdf685732215287429bab1bae9","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/languages/default.yml","hash":"61533eb5f4c3e75da584faa54758ad11af9050f7","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/.github/ISSUE_TEMPLATE.md","hash":"5fe52bc34b90f676fe1825dc50a12d92f581b047","modified":1733556632333},{"_id":"themes/hexo-theme-snippet/languages/ja.yml","hash":"c9deaf06b0edffdcc12c3e48fa649bac094e3f8c","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/languages/zh-CN.yml","hash":"70f843ab2c39ded6c30bc6c31e85a3b82f2e3c93","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/languages/zh-TW.yml","hash":"e440a8a29dc4266d9ae06ed3601796f7ef3dddee","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/layout/archive.ejs","hash":"dcf0a4d13e124f61920e92d5f73c3f0493c062eb","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/category.ejs","hash":"c97be36b33bb44957778587f00c978f2d28016f8","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/layout.ejs","hash":"8cd7e22e8dde838f9043dbfab86333de6a4d46e2","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/layout/index.ejs","hash":"105a03a545aa7d9e0603a1d04713bcae8e572107","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/layout/post.ejs","hash":"17f94fd5a4ddcb033b7114920de7eaba1c623b67","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/layout/page.ejs","hash":"d9a84e5de6085ac0925d54e3ac0d7927c5ce27f7","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/layout/tag.ejs","hash":"e7062d08cfff13dfe8b8c8915a2eb0fafb0ae567","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/scripts/helper.js","hash":"08f588a1d5e4608c6975acd85aa925d454f568d6","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/source/favicon.ico","hash":"65f26147ea3433ffd64d3f18bf281af48dc5d06e","modified":1733556632406},{"_id":"themes/hexo-theme-snippet/scripts/process.js","hash":"c0dcb3cb73034edb8409cf71a2aa7bb29fdcff06","modified":1733556632381},{"_id":"themes/hexo-theme-snippet/.github/ISSUE_TEMPLATE/feature_request.md","hash":"f416164e6e26891b3e9a7c78fe19c522c8c9ec86","modified":1733556632333},{"_id":"themes/hexo-theme-snippet/.github/ISSUE_TEMPLATE/bug_report.md","hash":"1be2729fc613c83b3f18c204bad4f3f3d35b22f2","modified":1733556632333},{"_id":"themes/hexo-theme-snippet/layout/_partial/archive.ejs","hash":"18cd984a3f736daacd8024b21b6a9d189272155e","modified":1733556632360},{"_id":"themes/hexo-theme-snippet/layout/_partial/busuanzi.ejs","hash":"77a6230ac178d84b1d39cacec85edd466fbf35da","modified":1733556632363},{"_id":"themes/hexo-theme-snippet/layout/_partial/copyright.ejs","hash":"1900fe930dc2b6824633644b8ebcd5e681fe0013","modified":1733556632363},{"_id":"themes/hexo-theme-snippet/layout/_partial/article.ejs","hash":"0c46f4477032f252e2cca2ca35ba0ce0075321e9","modified":1733556632360},{"_id":"themes/hexo-theme-snippet/layout/_partial/article-meta.ejs","hash":"47e39819428faae621734d4d827759cb4cc7c421","modified":1733556632360},{"_id":"themes/hexo-theme-snippet/layout/_partial/footer.ejs","hash":"ea2feb5bf783ec81fc1e3bb6ed275425012d1442","modified":1733556632363},{"_id":"themes/hexo-theme-snippet/layout/_partial/gallery.ejs","hash":"f5289f3c7678deacd20172579bc555c53f64f31f","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_partial/nav.ejs","hash":"24e257ff73d49e39bed2cc78a28df217d06dc670","modified":1733572466974},{"_id":"themes/hexo-theme-snippet/layout/_partial/header.ejs","hash":"3b2121ba50d6d6062c4b3f7a30098e3a5fe9b98b","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_partial/head.ejs","hash":"0989947615af26a82a2fe68d4bbdb62cf9d1a307","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_partial/pagination.ejs","hash":"926a50a7b3caec3c97602292af5b265b7a806045","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_partial/reward.ejs","hash":"15fb7b5aaff434cf7db16742871016ece3e6d152","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_partial/script.ejs","hash":"e9d77511fca8cc7a4236e0675474eb4a28e40a8c","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_partial/toc.ejs","hash":"d4fcdffd19820c8becf8e292c8cfae66efb93541","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_widget/archive.ejs","hash":"8a54a852fe493babe755764ca1708279fa2c271f","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_partial/sidebar.ejs","hash":"aa149d9cc4a05447b03330b2ab2d9a7ebbc3a31b","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_widget/category.ejs","hash":"4cbfe8880e0df0bf072b4bfae3887a53dc1650c9","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_widget/friends.ejs","hash":"86f1d7148bc4b561811437644106d70f265546ff","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_widget/notification.ejs","hash":"1e77328d7052f4b841fb6f064b175f59ffe6fc00","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_widget/search.ejs","hash":"1582212858af4333695a28784df6aa1d51acc025","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_widget/social.ejs","hash":"ce6ac2c61e1c526814a4da74baa095577a7c323a","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_widget/stick.ejs","hash":"ba6e8eaffe9a6f89f8147983d1f60c513cda617a","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_vendor/baidu_sitemap.ejs","hash":"0c23db3dadb74669d04363fc6f18c6f76c0bc9c5","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_widget/tagcloud.ejs","hash":"1cb757275d81f12b03a86cbd3d77f8f6b40e6e1b","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/source/assets/highlight.pack.js","hash":"b93558f0b97e233132a8ccf7f0275053f2df4628","modified":1733556632392},{"_id":"themes/hexo-theme-snippet/source/assets/tagcanvas.min.js","hash":"8102d7651a1f65b1bd8f8dd62f2d68cdd6b746dd","modified":1733556632392},{"_id":"themes/hexo-theme-snippet/source/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/css/style.css","hash":"2b95afa6a29c9b2fafb2879f3a4163c698d60c43","modified":1733556632406},{"_id":"themes/hexo-theme-snippet/source/js/search.js","hash":"7effddd062086185bf53455afe97de3a8a0daec5","modified":1733556632430},{"_id":"themes/hexo-theme-snippet/source/js/app.js","hash":"f4a0c39f86f504c6c46e581bfe602559245ba6d9","modified":1733556632430},{"_id":"themes/hexo-theme-snippet/source/img/avatar.jpg","hash":"512025710a8c6c938ab636fcb3c9da1c8df0eec0","modified":1733556632422},{"_id":"themes/hexo-theme-snippet/source/img/branding.png","hash":"18bee49d6a4c521ad230047c0b416245e009c2c9","modified":1733556632422},{"_id":"themes/hexo-theme-snippet/source/img/head-img.jpg","hash":"a318d304665c2f410f79e6d2eb1b98119f675b4b","modified":1733556632430},{"_id":"themes/hexo-theme-snippet/source/img/reward-wepay.jpg","hash":"2a56391f2b6282b49e724b72bf610b0dabf53742","modified":1733556632430},{"_id":"themes/hexo-theme-snippet/source/img/timeline-clock.gif","hash":"2bec1bf5efd948ab2e5942b6da8164faa41b62b2","modified":1733556632430},{"_id":"themes/hexo-theme-snippet/source/img/timeline-dot.gif","hash":"c85ef87be5b631c009e7c5737d33b61dfe580a4d","modified":1733556632430},{"_id":"themes/hexo-theme-snippet/source/img/timeline.gif","hash":"b7c7aac44e618df19626d882dc46db48a4aa3673","modified":1733556632430},{"_id":"themes/hexo-theme-snippet/layout/_partial/_head-sections/IE.ejs","hash":"9244b4d56884e8bedcff62aab74ef7bb025cdd7a","modified":1733556632352},{"_id":"themes/hexo-theme-snippet/layout/_partial/_head-sections/seo.ejs","hash":"ac931754998d13f8b65c43e542e71b03d779b97f","modified":1733556632360},{"_id":"themes/hexo-theme-snippet/layout/_partial/_head-sections/style.ejs","hash":"a7711ea1a0d55d08b57f6acbea7adbf107269200","modified":1733556632360},{"_id":"themes/hexo-theme-snippet/layout/_partial/_head-sections/title.ejs","hash":"fe7be7d5ba935e52063be2f3e94643a9e1e8417d","modified":1733556632360},{"_id":"themes/hexo-theme-snippet/layout/_vendor/analytics/baidu.ejs","hash":"7818d02890dc3aba795d850613eccab3b65836eb","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/analytics/index.ejs","hash":"68b3680acd34f62b0b4e0017e3d8bab20d4e459f","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/analytics/tencent.ejs","hash":"93ad3f5a68f1c2e432500c350ff97aad399d90a6","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/changyan.ejs","hash":"9526589626a038d6fb6da7d2ced7eb03ee18282f","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/disqus.ejs","hash":"5330d3413f25d3b0d404d5339111c0f57e06d124","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/analytics/google.ejs","hash":"69d06f6d8587d10daccc9702f0f142ebe7c615b9","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/analytics/cnzz.ejs","hash":"0f3f14339587f8be960ef7b6f47760ad7d6cfad1","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/gitment.ejs","hash":"35225e58fcb0a28876d8968a5c40139e695ca9aa","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/livere.ejs","hash":"5d2a612b16a5ae05abaddc4af7a12a4d42ee191d","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/gitalk.ejs","hash":"a53858839e432ca6ddd07f59496a89784b485b6b","modified":1733556632365},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/index.ejs","hash":"df8f8df7b392f917c0369328fedc17b445bdeee0","modified":1733556632372},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/utterances.ejs","hash":"1bd9c5b242509f469aac919b3558086aa4b8a25a","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/uyan.ejs","hash":"4d5e43c69eb28d2f45bba3c1cee25d773b02f0d0","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/layout/_vendor/comments/valine.ejs","hash":"be26a7d25d5244a711b1e14b277f714795d876aa","modified":1733556632373},{"_id":"themes/hexo-theme-snippet/source/css/less/_highlight.less","hash":"e91f9fb4dcb56ef1fb3750d4c81e6f5449b2de12","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/css/less/_reward.less","hash":"0691efb4fd3e361e24feb1a77fa16e3cd73ba633","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/css/less/_mixins.less","hash":"e52351351ae5388cbc658becd608c733c199c4a5","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/css/less/_scrollbar.less","hash":"11e85aaf8e0d532afc75b99ff897e6029515d0af","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/css/less/_style.less","hash":"35234f7da8deaaddf97ad1265ca2fd4d07bc7d04","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/css/less/_timeline.less","hash":"ebb24c9be11df98d40c6a736454e03db88ac1809","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/css/less/_variable.less","hash":"35a72a871d40b42cd5a6eb9d60eaf89a5bef5f56","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/assets/valine.min.js","hash":"61d8a2678c19153cda6bbd45746648816cc3400f","modified":1733556632392},{"_id":"themes/hexo-theme-snippet/source/css/bootstrap.min.css","hash":"224c9f9ad11b495358aa61dbd53e838e9b61015b","modified":1733556632397},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1733556632422},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1733556632422},{"_id":"themes/hexo-theme-snippet/source/img/loading.gif","hash":"6cea4dc953ac09fb744c7fedc12a1f4736faf8ec","modified":1733556632430},{"_id":"themes/hexo-theme-snippet/source/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1733556632406},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1733556632413},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1733556632422},{"_id":"themes/hexo-theme-snippet/source/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1733556632413},{"_id":"public/2024/12/07/computer-vision/CV007-YOLO11详解/index.html","hash":"befd5febce92caffe77f7b61477949aea2fc082a","modified":1733577333025},{"_id":"public/2024/12/07/computer-vision/CV010-YOLO V10详解/index.html","hash":"1e335fb2e08b65588f9e153c73b96b315683ead8","modified":1733577333025},{"_id":"public/2024/12/07/computer-vision/CV008-评估 YOLO （You Only Look Once） 模型的演变：YOLO11 及其前身的全面基准研究/index.html","hash":"d916da10d6273fcd606cbdce1ba93531aa4522af","modified":1733577333025},{"_id":"public/2024/12/06/hello-world/index.html","hash":"3311f67bfd76645bfd84296e46bcc79c733a41d9","modified":1733577333025},{"_id":"public/categories/computer-vision/index.html","hash":"4244f74105d8debee8fb6c060ab845003d2885ea","modified":1733577333025},{"_id":"public/archives/index.html","hash":"e4c5ad91b9c11ebc651437c2504ac5579574e1fb","modified":1733577333025},{"_id":"public/archives/2024/index.html","hash":"cb378b3fe8f9ef51f67a0fe0d82807a3fe31b6cc","modified":1733577333025},{"_id":"public/index.html","hash":"c4180cf982dc93526e5ae35bd01d461ff9046009","modified":1733577333025},{"_id":"public/archives/2024/12/index.html","hash":"d24b911c23105c545869a9ec344ef9099de47a5b","modified":1733577333025},{"_id":"public/tags/yolo/index.html","hash":"b06d74a047309253c71ae1473f4182ba9f7e5b4d","modified":1733577333025},{"_id":"public/tags/目标检测/index.html","hash":"49e1ded9b367b12416e5840d84c8ceded4138c01","modified":1733577333025},{"_id":"public/favicon.ico","hash":"65f26147ea3433ffd64d3f18bf281af48dc5d06e","modified":1733572801144},{"_id":"public/assets/highlight.pack.js","hash":"f39840759ec9afe56ec1ca30579d1f1c9d1f6026","modified":1733572801144},{"_id":"public/assets/tagcanvas.min.js","hash":"a14280f4e924428ac0cdbac7635a7279cab1b6bf","modified":1733572801144},{"_id":"public/css/style.css","hash":"427e307fbbd45b4f6d9228244ef2939678272813","modified":1733572801144},{"_id":"public/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1733572801144},{"_id":"public/assets/valine.min.js","hash":"d9d4a8f81f288b77b3d0a3866d004e2b5eb30038","modified":1733572801144},{"_id":"public/css/bootstrap.min.css","hash":"6527d8bf3e1e9368bab8c7b60f56bc01fa3afd68","modified":1733572801144},{"_id":"public/img/head-img.jpg","hash":"a318d304665c2f410f79e6d2eb1b98119f675b4b","modified":1733572801144},{"_id":"public/img/avatar.jpg","hash":"512025710a8c6c938ab636fcb3c9da1c8df0eec0","modified":1733572801144},{"_id":"public/img/branding.png","hash":"18bee49d6a4c521ad230047c0b416245e009c2c9","modified":1733572801144},{"_id":"public/img/reward-wepay.jpg","hash":"2a56391f2b6282b49e724b72bf610b0dabf53742","modified":1733572801144},{"_id":"public/img/timeline-dot.gif","hash":"c85ef87be5b631c009e7c5737d33b61dfe580a4d","modified":1733572801144},{"_id":"public/img/timeline-clock.gif","hash":"2bec1bf5efd948ab2e5942b6da8164faa41b62b2","modified":1733572801144},{"_id":"public/img/timeline.gif","hash":"b7c7aac44e618df19626d882dc46db48a4aa3673","modified":1733572801144},{"_id":"public/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1733572801144},{"_id":"public/js/search.js","hash":"c8a9033d7317927bf29f4ba33d152d7d33397ca6","modified":1733572801144},{"_id":"public/js/app.js","hash":"45cba4ab03cc65db7c80e50e1eb84f8c45fa4d26","modified":1733572801144},{"_id":"public/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1733572801144},{"_id":"public/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1733572801144},{"_id":"public/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1733573963348},{"_id":"public/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1733572801144},{"_id":"public/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1733572801144},{"_id":"public/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1733572801144},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1733573904203},{"_id":"themes/hexo-theme-fluid/.editorconfig","hash":"b595159772f3ee1ef5e6780ce307270e741cb309","modified":1733573904143},{"_id":"themes/hexo-theme-fluid/.eslintrc","hash":"3df89453e1f63051fafc90f16a8d83951050e316","modified":1733573904144},{"_id":"themes/hexo-theme-fluid/.gitattributes","hash":"3e00e1fb043438cd820d94ee3dc9ffb6718996f3","modified":1733573904144},{"_id":"themes/hexo-theme-fluid/.gitignore","hash":"9ede98b8d9dca802f82f651afb3b0366d69f05f1","modified":1733573904149},{"_id":"themes/hexo-theme-fluid/README_en.md","hash":"85656f2a23cc5e31420f0118bf5541b6e9f058b6","modified":1733573904151},{"_id":"themes/hexo-theme-fluid/README.md","hash":"34e3223851da1596b9ff051f2b29c6004edca8a1","modified":1733573904150},{"_id":"themes/hexo-theme-fluid/LICENSE","hash":"511e49f0bd8282a0d002c527474da8e1e5add393","modified":1733573904150},{"_id":"themes/hexo-theme-fluid/_config.yml","hash":"5d3cb2b715172401545f2cfe3f5c922c7d0017a9","modified":1733577291789},{"_id":"themes/hexo-theme-fluid/layout/404.ejs","hash":"c49974dcbda02fe720498398e9778826335459c0","modified":1733573904156},{"_id":"themes/hexo-theme-fluid/package.json","hash":"fd6756866314aaf4b15d734a83b85aa09aa0b5ed","modified":1733573904176},{"_id":"themes/hexo-theme-fluid/layout/about.ejs","hash":"2f3ea36713f0fa91d8d61d39fcf9e584372de818","modified":1733573904172},{"_id":"themes/hexo-theme-fluid/layout/categories.ejs","hash":"838a68e210bddfca6d4ba070e1e2f1ca53cb7d06","modified":1733573904173},{"_id":"themes/hexo-theme-fluid/layout/archive.ejs","hash":"c524ce76747042ec2f9ed8d5025f80e01b462b3b","modified":1733573904172},{"_id":"themes/hexo-theme-fluid/layout/category.ejs","hash":"264f68cbf826787e683a30e1377c56c0895c7386","modified":1733573904173},{"_id":"themes/hexo-theme-fluid/layout/index.ejs","hash":"dde1f6a27c8d09c38850a691089937f181b6c035","modified":1733573904173},{"_id":"themes/hexo-theme-fluid/layout/page.ejs","hash":"8ba210724c023d45a4564415762f3da299bd1d0e","modified":1733573904174},{"_id":"themes/hexo-theme-fluid/layout/layout.ejs","hash":"d4ffeb7eff398dea154340794bd277f75ddeedef","modified":1733573904174},{"_id":"themes/hexo-theme-fluid/layout/links.ejs","hash":"fbed4b3d1e475b3de9d8ce05362abcc658a53408","modified":1733573904174},{"_id":"themes/hexo-theme-fluid/layout/post.ejs","hash":"c8da695dc1b01b715909ae6f1052ccaebdf9db4c","modified":1733573904174},{"_id":"themes/hexo-theme-fluid/layout/tag.ejs","hash":"e87fc58829ea214ac16e8e4f13cd5c389133697b","modified":1733573904175},{"_id":"themes/hexo-theme-fluid/layout/tags.ejs","hash":"b7c1a6d8fc1097fc16d2300260297013cb692153","modified":1733573904176},{"_id":"themes/hexo-theme-fluid/languages/de.yml","hash":"f814263ded504cb4c50a8b66157bdd71f553be1b","modified":1733573904152},{"_id":"themes/hexo-theme-fluid/languages/eo.yml","hash":"314b97a7e68093328675acfd308d839b1d772ac9","modified":1733573904153},{"_id":"themes/hexo-theme-fluid/languages/en.yml","hash":"415e3403182e1282386f28b9d61343f147519163","modified":1733573904153},{"_id":"themes/hexo-theme-fluid/languages/es.yml","hash":"0ad94ddf1ca868a67b5b84aed257a30572962210","modified":1733573904154},{"_id":"themes/hexo-theme-fluid/languages/ja.yml","hash":"65a90f294f6c73245e8250e87d124630ad10b389","modified":1733573904154},{"_id":"themes/hexo-theme-fluid/languages/zh-CN.yml","hash":"497b3dea5058f718da225a7a443e916da895ea10","modified":1733573904155},{"_id":"themes/hexo-theme-fluid/languages/ru.yml","hash":"998112b384b574e0e29c6ea16e4c1ebce1c15a4c","modified":1733573904155},{"_id":"themes/hexo-theme-fluid/languages/zh-HK.yml","hash":"05418d0bca261de386872be65027bf4498758788","modified":1733573904155},{"_id":"themes/hexo-theme-fluid/languages/zh-TW.yml","hash":"ded0621e63b1f8b241be21f6e9b52d4f36edbcd0","modified":1733573904156},{"_id":"themes/hexo-theme-fluid/.github/ISSUE_TEMPLATE/bug_report_zh.md","hash":"fea63a9a5c3befd8783705eed09adf1b596a6203","modified":1733573904145},{"_id":"themes/hexo-theme-fluid/.github/ISSUE_TEMPLATE/bug_report.md","hash":"7d7c1e5a1da6b4f7be6685beb4798ec76d5efd31","modified":1733573904145},{"_id":"themes/hexo-theme-fluid/.github/ISSUE_TEMPLATE/feature_request_zh.md","hash":"7db378613df2b7d13e8c428c006399a879a4a852","modified":1733573904146},{"_id":"themes/hexo-theme-fluid/.github/ISSUE_TEMPLATE/feature_request.md","hash":"5cc30e7b6e7b77c8b40b182ba02a5d93d37d2fc2","modified":1733573904146},{"_id":"themes/hexo-theme-fluid/.github/ISSUE_TEMPLATE/question.md","hash":"102213e5d6790d060c0e26b4a3a7ec744d753c52","modified":1733573904146},{"_id":"themes/hexo-theme-fluid/.github/ISSUE_TEMPLATE/question_zh.md","hash":"07e24578c25fcaca94618fd86569887dadf7a276","modified":1733573904147},{"_id":"themes/hexo-theme-fluid/.github/workflows/cr.yaml","hash":"fc31c7c6692424af1e08cd5e273a5a5814f9c577","modified":1733573904147},{"_id":"themes/hexo-theme-fluid/.github/workflows/limit.yaml","hash":"bdbdb66da69ab7353b546f02150a6792f4787975","modified":1733573904148},{"_id":"themes/hexo-theme-fluid/layout/_partials/archive-list.ejs","hash":"78c34e32746041f23678669bbadfbede15e4c6d2","modified":1733573904157},{"_id":"themes/hexo-theme-fluid/.github/workflows/publish.yaml","hash":"dcdbe1698a6ee61f741c29ef560f859f66ffa32c","modified":1733573904148},{"_id":"themes/hexo-theme-fluid/layout/_partials/category-chains.ejs","hash":"508254a648d8597e62e4012c8beab44bfa82e904","modified":1733573904157},{"_id":"themes/hexo-theme-fluid/layout/_partials/category-list.ejs","hash":"0c14869e15f7dc615c8353765569644238f38f2d","modified":1733573904158},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments.ejs","hash":"1ce9094faec6204949cdaf604aaf9200787e4218","modified":1733573904159},{"_id":"themes/hexo-theme-fluid/layout/_partials/css.ejs","hash":"901280e6fb3194c30542751d04f27e78b42d3c6f","modified":1733573904163},{"_id":"themes/hexo-theme-fluid/layout/_partials/head.ejs","hash":"a0bcbbfc34efaef3b23c6b531e7f3201f2eab2dd","modified":1733573904164},{"_id":"themes/hexo-theme-fluid/layout/_partials/footer.ejs","hash":"6bb3335b5486d4bee2ed42f8bef57903066bc234","modified":1733573904163},{"_id":"themes/hexo-theme-fluid/layout/_partials/markdown-plugins.ejs","hash":"b5cd435b27f090939b6051bef41a38a3376044ac","modified":1733573904165},{"_id":"themes/hexo-theme-fluid/layout/_partials/paginator.ejs","hash":"0d443f23c459787338917900f50fec1c8b3b3bdd","modified":1733573904165},{"_id":"themes/hexo-theme-fluid/layout/_partials/scripts.ejs","hash":"89fc9f663a1091911b79ab9697c09446d16184f9","modified":1733573904171},{"_id":"themes/hexo-theme-fluid/layout/_partials/header.ejs","hash":"3668304d08c48b68d532532921a12069a2736150","modified":1733573904164},{"_id":"themes/hexo-theme-fluid/scripts/events/index.js","hash":"6c3b24207e4ea3ae4edeb715af40ef23711b92b9","modified":1733573904177},{"_id":"themes/hexo-theme-fluid/layout/_partials/search.ejs","hash":"57a0f61242d9ce2bd2c51b2f84193f6dc1377ef9","modified":1733573904172},{"_id":"themes/hexo-theme-fluid/scripts/filters/default-injects.js","hash":"3d30c722b9e24c33577d6fab822628841fadf992","modified":1733573904180},{"_id":"themes/hexo-theme-fluid/scripts/filters/locals.js","hash":"2340a576635b16fd2456b3494f5afe89cd7764db","modified":1733573904180},{"_id":"themes/hexo-theme-fluid/scripts/helpers/date.js","hash":"9bc9ba08d1d871394ee1c3a1cc2f21dc343f515a","modified":1733573904181},{"_id":"themes/hexo-theme-fluid/scripts/helpers/engine.js","hash":"96af7e55fdbe0819bacc554ecbfe42375a088df6","modified":1733573904182},{"_id":"themes/hexo-theme-fluid/scripts/filters/post-filter.js","hash":"67637461e3f94f9e9675369eb7ff015355d9ec54","modified":1733573904180},{"_id":"themes/hexo-theme-fluid/scripts/helpers/export-config.js","hash":"14a207a7d4e329382ab5d4e1da1ef85ff043daba","modified":1733573904182},{"_id":"themes/hexo-theme-fluid/scripts/helpers/import.js","hash":"f9821f7789ea6f069977a8c642aa5ccb6d19077c","modified":1733573904182},{"_id":"themes/hexo-theme-fluid/scripts/helpers/injects.js","hash":"9219d59c51930c7a82fcde918d6efbc5aa572ea2","modified":1733573904182},{"_id":"themes/hexo-theme-fluid/scripts/helpers/scope.js","hash":"3b67d50050158423c8fa47f1de6aedcfe916637b","modified":1733573904183},{"_id":"themes/hexo-theme-fluid/scripts/helpers/page.js","hash":"49b2c6449d7be35739c6cfea3cab4e790580983a","modified":1733573904183},{"_id":"themes/hexo-theme-fluid/scripts/helpers/url.js","hash":"f713ddb6c8018ec7b96d3567057f1f932609beea","modified":1733573904183},{"_id":"themes/hexo-theme-fluid/scripts/helpers/wordcount.js","hash":"0bb33314aa5cfe326ab9bb14b545e343e4db4193","modified":1733573904184},{"_id":"themes/hexo-theme-fluid/scripts/helpers/utils.js","hash":"f57be245e6e7228673e1dec3a3477e731492c5c1","modified":1733573904183},{"_id":"themes/hexo-theme-fluid/scripts/generators/pages.js","hash":"3fb72d3c2224c32d861a6e8a85e78a8b67e6a244","modified":1733573904181},{"_id":"themes/hexo-theme-fluid/scripts/generators/index-generator.js","hash":"3550976efc94500284795f13485f5a1765fc120b","modified":1733573904180},{"_id":"themes/hexo-theme-fluid/scripts/generators/local-search.js","hash":"33427308ca29f1d76336c83e704571c9de75df02","modified":1733573904181},{"_id":"themes/hexo-theme-fluid/scripts/tags/button.js","hash":"e1d0caed12e7cd9a35cf64272c41854b2901a58f","modified":1733573904184},{"_id":"themes/hexo-theme-fluid/scripts/tags/checkbox.js","hash":"1ff4ea054f2c735dfaccb0be90f1708a2a750bc8","modified":1733573904184},{"_id":"themes/hexo-theme-fluid/scripts/tags/fold.js","hash":"a93e2603021ad38714e870399767bea24e7cbe3e","modified":1733573904184},{"_id":"themes/hexo-theme-fluid/scripts/tags/group-image.js","hash":"cc176cc1d7e7cc28cedf8397ae748c691d140be2","modified":1733573904185},{"_id":"themes/hexo-theme-fluid/scripts/tags/label.js","hash":"6c5916d86c63795c7e910bf614b0e7ece5073702","modified":1733573904185},{"_id":"themes/hexo-theme-fluid/scripts/tags/mermaid.js","hash":"dbfe59fde77d87b1d7d0c46480a2a729010988eb","modified":1733573904185},{"_id":"themes/hexo-theme-fluid/scripts/tags/note.js","hash":"e300ec4ee6c63464859ab000e987bf8dd7db4025","modified":1733573904186},{"_id":"themes/hexo-theme-fluid/scripts/utils/compare-versions.js","hash":"37f90bd4e35ce49457dc2a348b9f66e0b242c014","modified":1733573904187},{"_id":"themes/hexo-theme-fluid/scripts/utils/crypto.js","hash":"474b00a57f43dbe7bc2876d637ece4214d016c06","modified":1733573904187},{"_id":"themes/hexo-theme-fluid/scripts/utils/url-join.js","hash":"dbdb10b23fcd3928e86a4cb46fa3455e060b4aa0","modified":1733573904188},{"_id":"themes/hexo-theme-fluid/scripts/utils/object.js","hash":"3e03b534e2e92a6e17567b006d7e3eaad4b37598","modified":1733573904187},{"_id":"themes/hexo-theme-fluid/scripts/utils/resolve.js","hash":"a5d70005913ab03cea0a0dc601097628b4dbd5a8","modified":1733573904188},{"_id":"themes/hexo-theme-fluid/source/css/gitalk.css","hash":"1fe60b2ab1d704f5a4f55e700dca5b8785fb390e","modified":1733573904205},{"_id":"themes/hexo-theme-fluid/source/css/highlight-dark.styl","hash":"c74d7aed425d20f2fa096f386a9521b67b9ab269","modified":1733573904205},{"_id":"themes/hexo-theme-fluid/source/css/highlight.styl","hash":"57ce8b8f95ab1f40612a9dce1793de5ab9b4bbfc","modified":1733573904205},{"_id":"themes/hexo-theme-fluid/source/css/main.styl","hash":"9e9171325bb7148c11ceee283d00c137c8a1c5c5","modified":1733573904205},{"_id":"themes/hexo-theme-fluid/source/js/color-schema.js","hash":"e7addcc88eb73dec4a9a8641a4bb68966a38a65d","modified":1733573904210},{"_id":"themes/hexo-theme-fluid/source/js/boot.js","hash":"33bb7c8255d2e3c93a1bea8c9221399b3a868a63","modified":1733573904210},{"_id":"themes/hexo-theme-fluid/source/js/events.js","hash":"3efd602cdb694902d6e74c4eb1e5bd70120ac5b1","modified":1733573904211},{"_id":"themes/hexo-theme-fluid/source/js/img-lazyload.js","hash":"67f6250f98b36a6599ea982d11cbb060c5ffb92a","modified":1733573904211},{"_id":"themes/hexo-theme-fluid/source/js/leancloud.js","hash":"e9ad1b5659f0af867174687daa0ecf4375e40b75","modified":1733573904212},{"_id":"themes/hexo-theme-fluid/source/js/local-search.js","hash":"491021125d2579e841c83f36d3ab790d1eab9d1e","modified":1733573904212},{"_id":"themes/hexo-theme-fluid/source/js/umami-view.js","hash":"370ab30ab88c596d85327dbd7db3bafd49489fdd","modified":1733573904213},{"_id":"themes/hexo-theme-fluid/source/js/plugins.js","hash":"753c2cf95f2659fef80277b895f4da10c8888c72","modified":1733573904212},{"_id":"themes/hexo-theme-fluid/source/js/utils.js","hash":"9d0423db40a787f3b19968205b9ed92a848c9153","modified":1733573904213},{"_id":"themes/hexo-theme-fluid/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1733573904206},{"_id":"themes/hexo-theme-fluid/source/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1733573904209},{"_id":"themes/hexo-theme-fluid/source/img/fluid.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1733573904208},{"_id":"themes/hexo-theme-fluid/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1733573904209},{"_id":"themes/hexo-theme-fluid/source/xml/local-search.xml","hash":"85fcc23b4db654a7f91fc55b6fb0442bb3ed3a9a","modified":1733573904213},{"_id":"themes/hexo-theme-fluid/layout/_partials/footer/beian.ejs","hash":"77d0c9df31a22ed8a3e341637bde4165a11a7ce9","modified":1733573904163},{"_id":"themes/hexo-theme-fluid/layout/_partials/footer/statistics.ejs","hash":"047bece1db5cdf96cb78a44c6420ce3e92e6a9ca","modified":1733573904164},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/changyan.ejs","hash":"0c410ef79785897c8de3da333b057a2936fd569b","modified":1733573904159},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/discuss.ejs","hash":"d400e5721af28cefecaf50b46c82dcdde4cda4a8","modified":1733573904160},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/cusdis.ejs","hash":"1e93ca89777e4beb0f0e5cb70e03aab48e958542","modified":1733573904160},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/giscus.ejs","hash":"66995ec9dab10ed35c2a775010c447113c6848d4","modified":1733573904160},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/disqus.ejs","hash":"79ec17eec6e15076c685688e740230e92c66efa9","modified":1733573904160},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/gitalk.ejs","hash":"7f04e5c22821bb94da791973d9c6692b03bac81d","modified":1733573904161},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/livere.ejs","hash":"bcceafab01fe695c59951d939f7cef502f3d7b48","modified":1733573904161},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/remark42.ejs","hash":"45c879768b40ba56af62e18ad54bffbf73a6f3a1","modified":1733573904162},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/twikoo.ejs","hash":"938eb60413ae8af83ffeaba4d85df88387cdd5be","modified":1733573904162},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/utterances.ejs","hash":"d7bcc183fc31af643e7835b13da10fe2ab8614ce","modified":1733573904162},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/valine.ejs","hash":"ef04d4fc3f26588ae9d8712938d648304fc05455","modified":1733573904162},{"_id":"themes/hexo-theme-fluid/layout/_partials/comments/waline.ejs","hash":"df6bae1a93827991049f7a33f6a69681c60eab0e","modified":1733573904163},{"_id":"themes/hexo-theme-fluid/layout/_partials/header/banner.ejs","hash":"a84d8dcb44f5f6289ef09db4d02ab14de72c2c87","modified":1733573904165},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/analytics.ejs","hash":"f8fe8e58b83f627db82c0dbeb663389efc33c1c6","modified":1733573904166},{"_id":"themes/hexo-theme-fluid/layout/_partials/header/navigation.ejs","hash":"e5219b14410066bf8ab491379aca797304b4a914","modified":1733573904165},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/anchorjs.ejs","hash":"8a4ea62c46f9a75c94096a27b2d3f5c10a2f82e5","modified":1733573904166},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/code-widget.ejs","hash":"03c7c69fbb1754fdccfa18671aac23b8637b869e","modified":1733573904166},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/encrypt.ejs","hash":"018cab52ff696a6c78ebc01e10237a90a0c33603","modified":1733573904167},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/fancybox.ejs","hash":"3900e54ade140e0e49c571a1955f0b1f3a59b281","modified":1733573904167},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/highlight.ejs","hash":"502b99e19e496825df7032ca2b0b1a95ebb2b357","modified":1733573904167},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/mermaid.ejs","hash":"110e45e2d3433178f00f482adc863110f90c46d6","modified":1733573904168},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/math.ejs","hash":"d0f06fb482e3a8f9a53dfd94c4e4a65a43f1ff34","modified":1733573904168},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/moment.ejs","hash":"acc72c3284fe906a4505132c3d9a4720d80e6fcb","modified":1733573904168},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/typed.ejs","hash":"42850952e8f5858497fe774c2aff87b6563ab01e","modified":1733573904169},{"_id":"themes/hexo-theme-fluid/layout/_partials/plugins/nprogress.ejs","hash":"47c1df255aa552ad71ef3e57deca46530a8f2802","modified":1733573904168},{"_id":"themes/hexo-theme-fluid/layout/_partials/post/category-bar.ejs","hash":"551ffae43844925beb099c85a9e6d8d9fcbf8086","modified":1733573904169},{"_id":"themes/hexo-theme-fluid/layout/_partials/post/copyright.ejs","hash":"26905d5862b1531ebcc175af15178dabeecc81c8","modified":1733573904169},{"_id":"themes/hexo-theme-fluid/layout/_partials/post/sidebar-left.ejs","hash":"db4ecdcc762bb1b1bae5060f0baa6115174779ff","modified":1733573904170},{"_id":"themes/hexo-theme-fluid/layout/_partials/post/meta-bottom.ejs","hash":"f0cb813cd03642c9b68cff8b6669f73a61dd10f8","modified":1733573904169},{"_id":"themes/hexo-theme-fluid/layout/_partials/post/meta-top.ejs","hash":"73827074db4e0fc3d52c51a76285df87aa5e5a7f","modified":1733573904170},{"_id":"themes/hexo-theme-fluid/scripts/events/lib/footnote.js","hash":"9b1934c61dc78622a07da554413f6ad31854576d","modified":1733573904178},{"_id":"themes/hexo-theme-fluid/layout/_partials/post/sidebar-right.ejs","hash":"2507cdad08f61cf8c1d9b0ca7f4f1dc8c4e5841b","modified":1733573904170},{"_id":"themes/hexo-theme-fluid/layout/_partials/post/toc.ejs","hash":"1b1eb4c8e163a5d909e86da76ef778948e0e0b77","modified":1733573904170},{"_id":"themes/hexo-theme-fluid/scripts/events/lib/compatible-configs.js","hash":"31208a0db986ba864f756a8ec806b7d254440f9b","modified":1733573904177},{"_id":"themes/hexo-theme-fluid/scripts/events/lib/hello.js","hash":"da987411ae4a4e6896a9b8af1fce6209192af28e","modified":1733573904178},{"_id":"themes/hexo-theme-fluid/scripts/events/lib/highlight.js","hash":"d103e4bf612b2445bb136712d57b81e784a313e2","modified":1733573904178},{"_id":"themes/hexo-theme-fluid/scripts/events/lib/injects.js","hash":"92123b7280695b4ac6650f5e1d7fa0d772c71f5b","modified":1733573904179},{"_id":"themes/hexo-theme-fluid/scripts/events/lib/merge-configs.js","hash":"ec6bf395ccad3dd41f29dc0080aeabf413e30fd9","modified":1733573904179},{"_id":"themes/hexo-theme-fluid/scripts/events/lib/lazyload.js","hash":"c9696633f77dd8055e900497469f9e64eca4d97f","modified":1733573904179},{"_id":"themes/hexo-theme-fluid/source/css/_mixins/base.styl","hash":"046979dbd8cdabd21d89f9c1d8f1bb3f2fd06d6f","modified":1733573904189},{"_id":"themes/hexo-theme-fluid/source/css/_functions/base.styl","hash":"171697018fd384fce0834875ca94b91f16564cac","modified":1733573904189},{"_id":"themes/hexo-theme-fluid/source/css/_pages/pages.styl","hash":"92c062cf55457b6549497244d09ec34e9c0c95c2","modified":1733573904204},{"_id":"themes/hexo-theme-fluid/source/css/_variables/base.styl","hash":"9ea66cf79f1e4356b6b402bc3dc5fb55c9862f1f","modified":1733573904204},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_about/about.styl","hash":"8ba5fb6a8ced1de6f7893184bf12f4021fe22595","modified":1733573904190},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_archive/archive.styl","hash":"e3846fb429f6732bd15fde40f7c28b3492d786c8","modified":1733573904191},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/color-schema.styl","hash":"66d5b045c0e54001d3c98c5901d72590fe08acc4","modified":1733573904197},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/base.styl","hash":"cd255079553985722ee80fb1833f6507dde52194","modified":1733573904197},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/keyframes.styl","hash":"58a7f8f2baea2d58cf5f7edfc91314ee5d7156ca","modified":1733573904198},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/inline.styl","hash":"96c3bb95dea4b3d3ecd20b810a674bfcef04870c","modified":1733573904198},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_category/category-bar.styl","hash":"f35415bd86b5c26fbc71728048d9e1481263554f","modified":1733573904199},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/print.styl","hash":"571bd018e914bd0f7c5f89df874b5937937e5fa6","modified":1733573904198},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_category/category-chain.styl","hash":"4263f7b930e6b57e13295d17fd3745a9e5c52494","modified":1733573904199},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_category/category-list.styl","hash":"d3aeb7bf22d52d7dde59b292090ef8b46943718a","modified":1733573904200},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_index/index.styl","hash":"bac20c8fb20276b08972df5ecc7a5850a72393f4","modified":1733573904200},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_links/links.styl","hash":"d3ef491fd449d89a1b95801dee788a5d9bec4320","modified":1733573904201},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_tag/tags.styl","hash":"29e9b72cfda2f2baf9cf2597fcd7f9e66303a9bd","modified":1733573904203},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_post/comment.styl","hash":"1fc96d09d52d9502e84e4e2a8d482ea45e8b81ea","modified":1733573904201},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_post/highlight.styl","hash":"d73cccb65eaa804910884df17442e34736b3f4fb","modified":1733573904202},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_post/markdown.styl","hash":"2d12f23b46d0ce07ae810bc4f5635c490a098fa4","modified":1733573904202},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_post/post-page.styl","hash":"6a35a450bd0a12f68fd92aac3f88b23475a98d46","modified":1733573904202},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_post/post-tag.styl","hash":"31c64c3fae4a0fc4747d8afeb72f7a9667c5326c","modified":1733573904203},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/anchorjs.styl","hash":"26d65475b1c52a61115044db8883df6739c3a473","modified":1733573904192},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/banner.styl","hash":"80301db38e448e40e88bb34d0128628b0809b243","modified":1733573904192},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/board.styl","hash":"1068d71721baeed76bf0176f9b964d36b5764c9f","modified":1733573904193},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/code-widget.styl","hash":"417a7388b39c0203178b0032e151febd66a0e9f3","modified":1733573904194},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/footer.styl","hash":"e6f5921ff9009c1853e7db30c482bc1682433ed9","modified":1733573904194},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/copyright.styl","hash":"3ac1eb36e124adef607775aa505386d5680960e2","modified":1733573904194},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/footnote.styl","hash":"41935973a66c14ab2bea0539d4b1f15c62534fa4","modified":1733573904195},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/header.styl","hash":"88c3c2d99a097142a87eeec0c7c65a3789f25117","modified":1733573904195},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/modal.styl","hash":"0ca6171ce262339e0e36cfea0978b554d87ae7fc","modified":1733573904195},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/noscript.styl","hash":"8fad325e411bc83c8ebdc4115015477eed5f60da","modified":1733573904195},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/ngrogress.styl","hash":"48799d3148ef6493be0e05897c635124e9b05d03","modified":1733573904195},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/qrcode.styl","hash":"04447d3b673be84a1af1dc57933a3c41dd7c0cfe","modified":1733573904196},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/pagination.styl","hash":"f4ae7cbf2f10f459de7864f8e642553b587df889","modified":1733573904196},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/scroll-btn.styl","hash":"e4dbbbb1a2508a72bc04680552d7ebbea0eed0fe","modified":1733573904196},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/search.styl","hash":"1f4e678d7219815ab62de1b92ec75e021247f90b","modified":1733573904196},{"_id":"themes/hexo-theme-fluid/source/css/_pages/_base/_widget/toc.styl","hash":"5defef321e3e933fe84f3f2ca481c88f55381fb0","modified":1733573904197},{"_id":"themes/hexo-theme-fluid/source/img/default.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1733573904208},{"_id":"public/local-search.xml","hash":"5ae34a918311722aa9944aca97018a7d67022a1d","modified":1733577333025},{"_id":"public/404.html","hash":"07ad5913ce3785967e4205eb687d9b5b91754abb","modified":1733577333025},{"_id":"public/tags/index.html","hash":"acfc85a2074f968b124c9371cbdf90c081ddf910","modified":1733577333025},{"_id":"public/categories/index.html","hash":"1fa135d51a6da88d905ed4c8c41fd9b0dc8cd2ec","modified":1733577333025},{"_id":"public/links/index.html","hash":"52f9b651e85bade29b70f10308fb30168bd66be5","modified":1733577333025},{"_id":"public/img/fluid.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1733573963348},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1733573963348},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1733573963348},{"_id":"public/xml/local-search.xml","hash":"85fcc23b4db654a7f91fc55b6fb0442bb3ed3a9a","modified":1733573963348},{"_id":"public/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1733573963348},{"_id":"public/css/highlight.css","hash":"04d4ddbb5e1d1007447c2fe293ee05aae9b9563e","modified":1733573963348},{"_id":"public/css/highlight-dark.css","hash":"902294bada4323c0f51502d67cba8c3a0298952f","modified":1733573963348},{"_id":"public/css/main.css","hash":"14ebd9b515085666cee29bbcbe362ad3604ab62a","modified":1733573963348},{"_id":"public/js/boot.js","hash":"38bd26c6b7acdafda86dda3560e6a3ca488d3c76","modified":1733573963348},{"_id":"public/js/color-schema.js","hash":"1ef88c881b9f942deadde3d890387b94c617342a","modified":1733573963348},{"_id":"public/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1733573963348},{"_id":"public/js/events.js","hash":"6869811f67e4c3de3edfa4b08464bb242b97a402","modified":1733573963348},{"_id":"public/js/leancloud.js","hash":"eff77c7a5c399fcaefda48884980571e15243fc9","modified":1733573963348},{"_id":"public/js/local-search.js","hash":"b9945f76f8682f3ec32edfb285b26eb559f7b7e8","modified":1733573963348},{"_id":"public/js/plugins.js","hash":"c34916291e392a774ff3e85c55badb83e8661297","modified":1733573963348},{"_id":"public/js/umami-view.js","hash":"33c4b3883fa747604074ad3921606eeeaeb50716","modified":1733573963348},{"_id":"public/js/utils.js","hash":"b82e7c289a66dfd36064470fd41c0e96fc598b43","modified":1733573963348},{"_id":"public/img/default.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1733573963348},{"_id":"source/about/index.md","hash":"1e14cfabc963b7584cc3f24c1c2c4acbec02a852","modified":1733577044566},{"_id":"source/_posts/nlp/NLP025-huggingface自定义数据集和模型下载存储目录.md","hash":"fdda59b75c0c8a415d689ea40c8b0d6b02bab05b","modified":1733576706814},{"_id":"public/about/index.html","hash":"2e36eb692796721b205cb136e502761d3900cd1d","modified":1733577333025},{"_id":"public/2024/12/07/nlp/NLP025-huggingface自定义数据集和模型下载存储目录/index.html","hash":"66ef6ec7d8fc8d8e098765dd407dff968f113fff","modified":1733577333025},{"_id":"public/categories/nlp/index.html","hash":"5b50a14a65bf5330acaee1ee77943d24dded1b31","modified":1733577333025},{"_id":"public/tags/huggingface/index.html","hash":"07beae3d247838880ff71c100493e3017b9270f3","modified":1733577333025},{"_id":"public/tags/transformers/index.html","hash":"120017c3c9590754be6df382a51bb2d632bca2d3","modified":1733577333025}],"Category":[{"name":"computer-vision","_id":"cm4e4hinn0003t4hi8ovwfpfg"},{"name":"nlp","_id":"cm4e6saox000a4ohif4f17c09"}],"Data":[],"Page":[{"title":"about me","layout":"about","date":"2024-12-07T13:09:27.000Z","_content":"","source":"about/index.md","raw":"---\ntitle: about me\nlayout: about\ndate: 2024-12-07 21:09:27\n---\n","updated":"2024-12-07T13:10:44.566Z","path":"about/index.html","_id":"cm4e6ysxd000k4ohi4mcjadkr","comments":1,"content":"","excerpt":"","more":""}],"Post":[{"_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2024-12-06T07:50:39.751Z","updated":"2024-12-07T11:38:55.143Z","title":"","comments":1,"layout":"post","photos":[],"_id":"cm4e4hing0000t4hi319bhvm3","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"YOLO11 详解","date":"2024-12-07T10:30:00.000Z","_content":"\n\n\n2024 年是 YOLO 模型的一年。在 2023 年发布 Ultralytics YOLOv8 之后， YOLOv9 和 YOLOv10也在2024年发布了。但等等，这还不是结束！Ultralytics YOLO11 终于来了，在激动人心的 YOLO Vision 2024 （YV24） 活动中亮相。\n\n![](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/feature.gif)\n\nYOLO11 系列是 YOLO 系列中最先进的 （SOTA）、最轻、最高效的型号，性能优于其前代产品。它由 Ultralytics 创建，该组织发布了 YOLOv8，这是迄今为止最稳定和使用最广泛的 YOLO 变体。现在，YOLO11 将继续 YOLO 系列的传统。在本文中，我们将探讨：\n\n- **什么是 YOLO11？**\n- **YOLO11 能做什么？**\n- **YOLO11 比其他 YOLO 变体更高效吗？**\n- **YOLO11 架构有哪些改进？**\n- **YOLO11 的代码pipeline是如何工作的？**\n- **YOLO11 的基准测试**\n- **YOLO11 快速回顾**\n\n# 什么是YOLO11\n\n![](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-1.png)\n\nYOLO11 是 Ultralytics 的 YOLO 系列的最新版本。YOLO11 配备了超轻量级型号，比以前的 YOLO 更快、更高效。YOLO11 能够执行更广泛的计算机视觉任务。Ultralytics 根据大小发布了 5 个 YOLO11 模型，并在**所有任务中发布了 25 个模型**：\n\n- **YOLO11n** – Nano 适用于小型和轻量级任务。\n- **YOLO11s** – Nano 的小升级，具有一些额外的准确性。\n- **YOLO11m** – 通用型。\n- **YOLO11l** – 大，精度更高，计算量更高。\n- **YOLO11x** – 超大尺寸，可实现最大精度和性能。\n\n![](https://learnopencv.com/wp-content/uploads/2024/10/yolo11-model-table.png)\n\nYOLO11 构建在 Ultralytics YOLOv8 代码库之上，并进行了一些架构修改。它还集成了以前 YOLO（如 YOLOv9 和 YOLOv10）的新功能（改进这些功能）以提高性能。我们将在博客文章的后面部分探讨架构和代码库中的新变化。\n\n# YOLO11的应用\n\nYOLO 以其对象检测模型而闻名。但是，YOLO11 可以执行多个计算机视觉任务，例如 YOLOv8。它包括：\n\n- **对象检测**\n- **实例分段**\n- **图像分类**\n- **姿势估计**\n- **定向目标检测 （OBB）**\n\n让我们来探索所有这些。\n\n### 对象检测\n\n![yolo11-对象检测](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-object-detection.gif)\n\nYOLO11 通过将输入图像传递到 CNN 以提取特征来执行对象检测。然后，网络预测这些网格中对象的边界框和类概率。为了处理多尺度检测，使用图层来确保检测到各种大小的物体。然后使用非极大值抑制 （NMS） 来优化这些预测，以过滤掉重复或低置信度的框，从而获得更准确的对象检测。YOLO11 在 MS-COCO 数据集上进行对象检测训练，其中包括 80 个预训练类。\n\n### 实例分割\n\n![ ](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-instance-segmentation-1-1733102326734-1.png)\n\n除了检测对象之外，YOLO11 还通过添加掩码预测分支扩展到实例分割。这些模型在 MS-COCO 数据集上进行训练，其中包括 80 个预训练类。此分支为每个检测到的对象生成像素级分割掩码，使模型能够区分重叠的对象并提供其形状的精确轮廓。head 中的蒙版分支处理特征映射并输出对象蒙版，从而在识别和区分图像中的对象时实现像素级精度。\n\n### 姿势估计\n\n![YOLO11 姿势](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-pose-estimation.gif)\n\nYOLO11 通过检测和预测物体上的关键点（例如人体的关节）来执行姿态估计。关键点连接起来形成骨架结构，该结构表示姿势。这些模型在 COCO 上进行训练，其中包括一个预先训练的类“person”。\n\n在头部添加姿态估计层，并训练网络预测关键点的坐标。后处理步骤将点连接起来以形成骨架结构，从而实现实时姿势识别。\n\n### 图像分类\n\n![YOLO11 图像分类](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-image-classification.gif)\n\n对于图像分类，YOLO11 使用其深度神经网络从输入图像中提取高级特征，并将其分配给多个预定义类别之一。这些模型在 ImageNet 上进行训练，其中包括 1000 个预训练类。该网络通过多层卷积和池化处理图像，在增强基本特征的同时减少空间维度。网络顶部的分类头输出预测的类，使其适用于需要识别图像整体类别的任务。\n\n### 定向目标检测 （OBB）\n\n![YOLO11-OBB](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-detection-1.gif)\n\nYOLO11 通过整合 OBB 扩展了常规对象检测，使模型能够检测和分类旋转或不规则方向的物体。这对于航空影像分析等应用程序特别有用。这些模型在 DOTAv1 上进行训练，其中包括 15 个预训练类。\n\n![YOLO11-OBB](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-logic-1-1024x615.png)\n\nOBB 模型不仅输出边界框坐标，还输出旋转角度 （θ） 或四个角点。这些坐标用于创建与对象方向对齐的边界框，从而提高旋转对象的检测准确性。\n\n# YOLO11 架构和 YOLO11 中的新增功能\n\nYOLO11 架构是对 YOLOv8 架构的升级，具有一些新的集成和参数调整。在我们继续主要部分之前，您可以查看我们关于 [**YOLOv8**](https://learnopencv.com/ultralytics-yolov8/) 的详细文章以大致了解架构。现在，如果你看一下 YOLO11 的配置文件：\n\n```yaml\n# Parameters\nnc: 80 # number of classes\nscales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\n  # [depth, width, max_channels]\n  n: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs\n  s: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\n  m: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs\n  l: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs\n  x: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs\n \n# YOLO11n backbone\nbackbone:\n  # [from, repeats, module, args]\n  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n  - [-1, 2, C3k2, [256, False, 0.25]]\n  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n  - [-1, 2, C3k2, [512, False, 0.25]]\n  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n  - [-1, 2, C3k2, [512, True]]\n  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n  - [-1, 2, C3k2, [1024, True]]\n  - [-1, 1, SPPF, [1024, 5]] # 9\n  - [-1, 2, C2PSA, [1024]] # 10\n \n# YOLO11n head\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n  - [-1, 2, C3k2, [512, False]] # 13\n \n  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n  - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\n \n  - [-1, 1, Conv, [256, 3, 2]]\n  - [[-1, 13], 1, Concat, [1]] # cat head P4\n  - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\n \n  - [-1, 1, Conv, [512, 3, 2]]\n  - [[-1, 10], 1, Concat, [1]] # cat head P5\n  - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\n \n  - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\n```\n\n架构级别的变化：\n\n### **1. 骨干**\n\n主干是模型的一部分，用于从多个比例的输入图像中提取特征。它通常涉及堆叠卷积层和块以创建不同分辨率的特征图。\n\n**卷积层：**YOLO11 具有类似的结构，带有初始卷积层来对图像进行下采样：\n\n```\n- [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n- [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n```\n\n- **C3k2 区块：**YOLO11 引入了 **C3k2 块，而不是 C2f**，它在计算方面效率更高。此块是 **CSP 瓶颈**的自定义实现，它使用两个卷积，而不是一个大型卷积（如 YOLOv8 中所示）。\n\n  - **CSP （Cross Stage Partial）：**CSP 网络拆分特征图并通过瓶颈层处理一部分，同时将另一部分与瓶颈的输出合并。这减少了计算负载并改善了特征表示。\n\n  ```\n  - [-1, 2, C3k2, [256, False, 0.25]]\n  ```\n\n  - C3k2 块还使用较小的内核大小（由 k2 表示），使其更快，同时保持性能。\n\n  **SPPF 和 C2PSA：**YOLO11 保留了 SPPF 块，但在 SPPF 之后添加了一个新的 **C2PSA** 块：\n\n```\n- [-1, 1, SPPF, [1024, 5]]\n- [-1, 2, C2PSA, [1024]\n```\n\n- **C2PSA （Cross Stage Partial with Spatial Attention）** 模块增强了特征图中的空间注意力，从而提高了模型对图像重要部分的关注。这使模型能够通过在空间上池化特征来更有效地关注特定的感兴趣区域。\n\n### **2. neck**\n\nneck 负责聚合来自不同分辨率的特征，并将它们传递给头部进行预测。它通常涉及来自不同级别的特征图的上采样和连接。\n\n**C3k2 区块：**YOLO11 用 **C3k2** 块替换了颈部的 C2f 块。如前所述，C3k2 是一个更快、更高效的区块。例如，在上采样和串联后，YOLO11 中的 neck 如下所示：\n\n```\n\t\n- [-1, 2, C3k2, [512, False]] # P4/16-medium\n```\n\n- 此更改提高了要素聚合过程的速度和性能。\n- **注意力机制：**YOLO11 通过 **C2PSA** 更侧重于空间注意力，这有助于模型专注于图像中的关键区域，以便更好地检测。这在 YOLOv8 中是缺失的，这使得 YOLO11 在检测较小或被遮挡的对象时可能更准确。\n\n------\n\n### **3. head**\n\nhead 是模型中负责生成最终预测的部分。在对象检测中，这通常意味着生成边界框并对这些框内的对象进行分类。\n\n**C3k2 区块：**与颈部类似，YOLO11 取代了头部的 **C2f** 块。\n\n```\n\t\n- [-1, 2, C3k2, [512, False]] # P4/16-medium\n\n```\n\n**检测层：**最终的 Detect 层与 YOLOv8 中的层相同：\n\n```\n- [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\n\n```\n\n使用 C3k2 块使模型在推理方面更快，在参数方面更高效。\n那么，让我们看看新块（层）在代码中的样子：\n\n------\n\n那么，让我们看看新块（层）在代码中的样子：\n\n1. **C3k2 区块（从** **blocks.py** 开始**）：**\n   - **C3k2** 是 **CSP 瓶颈**的更快、更高效的变体。它使用两个卷积而不是一个大型卷积，从而加快了特征提取速度。\n\n```\nclass C3k2(C2f):\n    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        self.m = nn.ModuleList(\n            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)\n        )\n```\n\n2. **C3k 块（从** **blocks.py** 开始**）**：\n\n- **C3k** 是一个更灵活的瓶颈模块，允许自定义内核大小。这对于提取图像中更详细的特征非常有用。\n\n```\nclass C3k(C3):\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)  # hidden channels\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n```\n\n3. **C2PSA 块（从** **blocks.py** 年起**）：**\n\n- **C2PSA** （Cross Stage Partial with Spatial Attention） 增强了模型的空间注意力能力。此模块增加了对特征图的关注，帮助模型专注于图像的重要区域。\n\n```\nclass C2PSA(nn.Module):\n    def __init__(self, c1, c2, e=0.5):\n        super().__init__()\n        c_ = int(c2 * e)\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)\n     \n    def forward(self, x):\n        return self.cv3(torch.cat((self.cv1(x), self.cv2(x)), 1))\n```\n\n# YOLO11 pipeline\n\n在 [**ultralytics**](https://github.com/ultralytics/ultralytics) GitHub 仓库中，我们将主要关注：\n\n1. **nn/modules/** 中的模块\n   - **block.py**\n   - **conv.py**\n   - **head.py**\n   - **transformer.py**\n   - **utils.py**\n2. **nn/tasks.py** 文件\n\n### 1. 代码库概述\n\n代码库被构建为多个模块，这些模块定义了 YOLO11 模型中使用的各种神经网络组件。这些组件在 nn/modules/ 目录中被组织到不同的文件中：\n\n- **block.py**：定义模型中使用的各种构建块（模块），例如瓶颈、CSP 模块和注意力机制。\n- **conv.py**：包含卷积模块，包括标准卷积、深度卷积和其他变体。\n- **head.py**：实现负责生成最终预测（例如，边界框、类概率）的模型头。\n- **transformer.py**：包括基于 transformer 的模块，用于注意力机制和高级特征提取。\n- **utils.py**：提供跨模块使用的实用程序函数和帮助程序类。\n\nnn/tasks.py 文件定义了不同的特定于任务的模型（例如，检测、分割、分类），这些模型将这些模块组合成完整的架构。\n\n### 2. nn/modules/ 中的模块\n\n如前所述，YOLO11 构建在 YOLOv8 代码库之上。因此，我们将主要关注更新的脚本：**block.py**、**conv.py** 和 **head.py** 在这里。\n\n#### **block.py**\n\n此文件定义 YOLO11 模型中使用的各种构建块。这些块是构成神经网络层的基本组件。\n\n##### **关键组件：**\n\n1. 瓶颈模块：\n   - **Bottleneck**：具有可选快捷方式连接的标准瓶颈模块。\n   - **Res**：使用一系列卷积和身份快捷方式的残差块。\n\n```\nclass Bottleneck(nn.Module):\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):\n        super().__init__()\n        c_ = int(c2 * e)\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n \n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n\n```\n\n- Bottleneck 类实现了一个 bottleneck 模块，该模块减少了通道的数量（降维），然后再次扩展它们。\n- **组件**：\n  - self.cv1：一个 1×1 卷积，用于减少通道数。\n  - self.cv2：一个 3×3 卷积，用于将通道数增加回原始通道数。\n  - self.add：一个布尔值，指示是否添加快捷方式连接。\n- **Forward Pass**：输入 x 通过 cv1 和 cv2 传递。如果 self.add 为 True，则原始输入 x 将添加到输出（残差连接）。\n\n2. CSP （Cross Stage Partial） 模块：\n\n- **BottleneckCSP：**瓶颈模块的 CSP 版本。\n- **CSPBlock**：具有多个瓶颈层的更复杂的 CSP 模块。\n\n```\nclass BottleneckCSP(nn.Module):\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__()\n        c_ = int(c2 * e)\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = nn.Sequential(\n            *[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)]\n        )\n        self.cv3 = Conv(2 * c_, c2, 1)\n        self.add = c1 == c2\n \n    def forward(self, x):\n        y1 = self.cv2(self.cv1(x))\n        y2 = x if self.add else None\n        return self.cv3(torch.cat((y1, y2), 1)) if y2 is not None else self.cv3(y1)\n\n```\n\n- CSPBottleneck 模块将特征图分为两部分。一部分通过一系列瓶颈层，另一部分直接连接到输出，从而降低了计算成本并增强了梯度流。\n- **组件**：\n  - self.cv1：减少通道数。\n  - self.cv2：瓶颈层序列。\n  - self.cv3：合并功能并调整通道数。\n  - self.add：确定是否添加快捷方式连接。\n\n3. 其他模块：\n\n- **SPPF：**Spatial Pyramid Pooling Fast 模块，可在多个比例下执行池化。\n- **Concat**：沿指定维度连接多个 Tensor。\n\n```\nclass SPPF(nn.Module):\n    def __init__(self, c1, c2, k=5):\n        super().__init__()\n        c_ = c1 // 2\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n \n    def forward(self, x):\n        x = self.cv1(x)\n        y1 = self.m(x)\n        y2 = self.m(y1)\n        y3 = self.m(y2)\n        return self.cv2(torch.cat([x, y1, y2, y3], 1))\n```\n\n- SPPF 模块在不同比例下执行最大池化，并将结果连接起来以捕获多个空间比例的要素。\n- **组件**：\n  - self.cv1：减少通道数。\n  - self.cv2：调整拼接后的 Channel 数。\n  - self.m：最大池化层数。\n- **Forward Pass**：输入 x 通过 cv1，然后通过三个连续的最大池化层（y1、y2、y3）。结果被连接并通过 cv2 传递。\n\n##### **了解概念：**\n\n- **瓶颈层**：用于通过在昂贵的操作之前减少通道数并在之后增加通道数来降低计算复杂性。\n- **残差连接**：通过缓解梯度消失问题来帮助训练更深的网络。\n- **CSP 架构**：将特征图分为两部分;一部分发生转换，而另一部分保持不变，从而提高学习能力并减少计算。\n\n#### **conv.py**\n\n此文件包含各种卷积模块，包括标准卷积和专用卷积。\n\n##### **关键组件：**\n\n**标准卷积模块 （Conv）：**\n\n```\nclass Conv(nn.Module):\n    default_act = nn.SiLU()  # default activation\n \n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n \n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n```\n\n- 实现具有批量规范化和激活的标准卷积层。\n- **组件**：\n  - self.conv：卷积层。\n  - self.bn：批量规范化。\n  - self.act：激活函数（默认为 nn.SiLU（））的\n- **Forward Pass**：应用卷积，然后进行批量规范化和激活。\n\n**深度卷积 （DWConv）：**\n\n```\nclass DWConv(Conv):\n    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):\n        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n```\n\n- 执行深度卷积，其中每个输入通道单独卷积。\n- **组件**：\n  - 继承自 Conv。\n  - 将 groups 参数设置为 c1 和 c2 的最大公约数，从而有效地对每个通道的卷积进行分组。\n\n1. 其他卷积模块：\n   - **Conv2**：RepConv 的简化版本，用于模型压缩和加速。\n   - **GhostConv**：实现 GhostNet 的 ghost 模块，减少特性图中的冗余。\n   - **RepConv**：可重新参数化的卷积层，可以从训练模式转换为推理模式。\n\n##### **了解概念：**\n\n- **自动填充 （****autopad****）：**自动计算保持输出尺寸一致所需的填充。\n- **深度卷积和点卷积**：用于 MobileNet 架构，以减少计算，同时保持准确性。\n- **重新参数化**：RepConv 等技术通过合并层来实现高效的训练和更快的推理。\n\n#### **head.py**\n\n此文件实现了负责生成模型最终预测的 head 模块。\n\n##### **关键组件：**\n\n**检测头 （Detect）：**\n\n```\nclass Detect(nn.Module):\n    def __init__(self, nc=80, ch=()):\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.nl = len(ch)  # number of detection layers\n        self.reg_max = 16  # DFL channels\n        self.no = nc + self.reg_max * 4  # number of outputs per anchor\n        self.stride = torch.zeros(self.nl)  # strides computed during build\n \n        # Define layers\n        self.cv2 = nn.ModuleList(\n            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch\n        )\n        self.cv3 = nn.ModuleList(\n            nn.Sequential(\n                nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),\n                nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),\n                nn.Conv2d(c3, self.nc, 1),\n            )\n            for x in ch\n        )\n        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n```\n\n- Detect 类定义输出边界框坐标和类概率的检测头。\n- **组件**：\n  - self.cv2：用于边界框回归的卷积层。\n  - self.cv3：用于分类的卷积层。\n  - self.dfl：用于边界框细化的 Distribution Focal Loss 模块。\n- **Forward Pass**：处理输入特征映射并输出边界框和类的预测。\n\n**分割 （****Segment****）：**\n\n```\nclass Segment(Detect):\n    def __init__(self, nc=80, nm=32, npr=256, ch=()):\n        super().__init__(nc, ch)\n        self.nm = nm  # number of masks\n        self.npr = npr  # number of prototypes\n        self.proto = Proto(ch[0], self.npr, self.nm)  # protos\n \n        c4 = max(ch[0] // 4, self.nm)\n        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)\n\n```\n\n- 扩展 Detect 类以包含分段功能。\n- **组件**：\n  - self.proto：生成掩码原型。\n  - self.cv4：掩码系数的卷积层。\n- **Forward Pass**：输出边界框、类概率和掩码系数。\n\n**姿势估计头部 （Pose）：**\n\n```\nclass Pose(Detect):\n    def __init__(self, nc=80, kpt_shape=(17, 3), ch=()):\n        super().__init__(nc, ch)\n        self.kpt_shape = kpt_shape  # number of keypoints, number of dimensions\n        self.nk = kpt_shape[0] * kpt_shape[1]  # total number of keypoint outputs\n \n        c4 = max(ch[0] // 4, self.nk)\n        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch)\n\n```\n\n- 扩展了 Detect 类，用于人体姿势估计任务。\n- **组件**：\n  - self.kpt_shape：关键点的形状（关键点的数量、每个关键点的维度）。\n  - self.cv4：用于关键点回归的卷积层。\n- **Forward Pass**：输出边界框、类概率和关键点坐标。\n\n##### **了解概念：**\n\n- **模块化**：通过扩展 Detect 类，我们可以为不同的任务创建专门的 head，同时重用通用功能。\n- **无锚点检测**：现代对象检测器通常使用无锚点方法，直接预测边界框。\n- **关键点估计**：在姿势估计中，模型预测表示关节或地标的关键点。\n\n### 3. **nn/tasks.py** 文件\n\n```python\n# Ultralytics YOLO <img draggable=\"false\" role=\"img\" class=\"emoji\" alt=\"🚀\" src=\"https://s.w.org/images/core/emoji/15.0.3/svg/1f680.svg\">, AGPL-3.0 license\n \nimport contextlib\nimport pickle\nimport types\nfrom copy import deepcopy\nfrom pathlib import Path\n \nimport torch\nimport torch.nn as nn\n \n# Other imports...\n \nclass BaseModel(nn.Module):\n    \"\"\"The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.\"\"\"\n \n    def forward(self, x, *args, **kwargs):\n        \"\"\"Handles both training and inference, returns predictions or loss.\"\"\"\n        if isinstance(x, dict):\n            return self.loss(x, *args, **kwargs)  # Training: return loss\n        return self.predict(x, *args, **kwargs)  # Inference: return predictions\n \n    def predict(self, x, profile=False, visualize=False, augment=False, embed=None):\n        \"\"\"Run a forward pass through the network for inference.\"\"\"\n        if augment:\n            return self._predict_augment(x)\n        return self._predict_once(x, profile, visualize, embed)\n     \n    def fuse(self, verbose=True):\n        \"\"\"Fuses Conv and BatchNorm layers for efficiency during inference.\"\"\"\n        for m in self.model.modules():\n            if isinstance(m, (Conv, Conv2, DWConv)) and hasattr(m, \"bn\"):\n                m.conv = fuse_conv_and_bn(m.conv, m.bn)\n                delattr(m, \"bn\")\n                m.forward = m.forward_fuse  # Use the fused forward\n        return self\n     \n    # More BaseModel methods...\n \nclass DetectionModel(BaseModel):\n    \"\"\"YOLOv8 detection model.\"\"\"\n \n    def __init__(self, cfg=\"yolov8n.yaml\", ch=3, nc=None, verbose=True):\n        \"\"\"Initialize the YOLOv8 detection model with config and parameters.\"\"\"\n        super().__init__()\n        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)\n        self.names = {i: f\"{i}\" for i in range(self.yaml[\"nc\"])}  # Class names\n        self.inplace = self.yaml.get(\"inplace\", True)\n \n        # Initialize strides\n        m = self.model[-1]  # Detect() layer\n        if isinstance(m, Detect):\n            s = 256  # Max stride\n            m.stride = torch.tensor([s / x.shape[-2] for x in self._predict_once(torch.zeros(1, ch, s, s))])\n            self.stride = m.stride\n            m.bias_init()  # Initialize biases\n \n    # More DetectionModel methods...\n \nclass SegmentationModel(DetectionModel):\n    \"\"\"YOLOv8 segmentation model.\"\"\"\n \n    def __init__(self, cfg=\"yolov8n-seg.yaml\", ch=3, nc=None, verbose=True):\n        \"\"\"Initialize YOLOv8 segmentation model with given config and parameters.\"\"\"\n        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\n \n    def init_criterion(self):\n        \"\"\"Initialize the loss criterion for the SegmentationModel.\"\"\"\n        return v8SegmentationLoss(self)\n \nclass PoseModel(DetectionModel):\n    \"\"\"YOLOv8 pose model.\"\"\"\n \n    def __init__(self, cfg=\"yolov8n-pose.yaml\", ch=3, nc=None, data_kpt_shape=(None, None), verbose=True):\n        \"\"\"Initialize YOLOv8 Pose model.\"\"\"\n        if not isinstance(cfg, dict):\n            cfg = yaml_model_load(cfg)\n        if list(data_kpt_shape) != list(cfg[\"kpt_shape\"]):\n            cfg[\"kpt_shape\"] = data_kpt_shape\n        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\n \n    def init_criterion(self):\n        \"\"\"Initialize the loss criterion for the PoseModel.\"\"\"\n        return v8PoseLoss(self)\n \nclass ClassificationModel(BaseModel):\n    \"\"\"YOLOv8 classification model.\"\"\"\n \n    def __init__(self, cfg=\"yolov8n-cls.yaml\", ch=3, nc=None, verbose=True):\n        \"\"\"Initialize the YOLOv8 classification model.\"\"\"\n        super().__init__()\n        self._from_yaml(cfg, ch, nc, verbose)\n \n    def _from_yaml(self, cfg, ch, nc, verbose):\n        \"\"\"Set YOLOv8 model configurations and define the model architecture.\"\"\"\n        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)\n        self.names = {i: f\"{i}\" for i in range(self.yaml[\"nc\"])}\n        self.info()\n \n    def reshape_outputs(model, nc):\n        \"\"\"Update a classification model to match the class count (nc).\"\"\"\n        name, m = list((model.model if hasattr(model, \"model\") else model).named_children())[-1]\n        if isinstance(m, nn.Linear):\n            if m.out_features != nc:\n                setattr(model, name, nn.Linear(m.in_features, nc))\n \n    def init_criterion(self):\n        \"\"\"Initialize the loss criterion for the ClassificationModel.\"\"\"\n        return v8ClassificationLoss()\n \nclass Ensemble(nn.ModuleList):\n    \"\"\"Ensemble of models.\"\"\"\n \n    def __init__(self):\n        \"\"\"Initialize an ensemble of models.\"\"\"\n        super().__init__()\n \n    def forward(self, x, augment=False, profile=False, visualize=False):\n        \"\"\"Generate the ensemble’s final layer by combining outputs from each model.\"\"\"\n        y = [module(x, augment, profile, visualize)[0] for module in self]\n        return torch.cat(y, 2), None  # Concatenate outputs along the third dimension\n \n# Functions ------------------------------------------------------------------------------------------------------------\n \ndef parse_model(d, ch, verbose=True):\n    \"\"\"Parse a YOLO model.yaml dictionary into a PyTorch model.\"\"\"\n    import ast\n \n    max_channels = float(\"inf\")\n    nc, act, scales = (d.get(x) for x in (\"nc\", \"activation\", \"scales\"))\n    depth, width, kpt_shape = (d.get(x, 1.0) for x in (\"depth_multiple\", \"width_multiple\", \"kpt_shape\"))\n \n    # Model scaling\n    if scales:\n        scale = d.get(\"scale\")\n        if not scale:networ\n            scale = tuple(scales.keys())[0]\n            LOGGER.warning(f\"WARNING <img draggable=\"false\" role=\"img\" class=\"emoji\" alt=\"⚠️\" src=\"https://s.w.org/images/core/emoji/15.0.3/svg/26a0.svg\"> no model scale passed. Assuming scale='{scale}'.\")\n        depth, width, max_channels = scales[scale]\n \n    if act:\n        Conv.default_act = eval(act)  # redefine default activation\n        if verbose:\n            LOGGER.info(f\"Activation: {act}\")\n \n    # Logging and parsing layers\n    if verbose:\n        LOGGER.info(f\"\\n{'':>3}{'from':>20}{'n':>3}{'params':>10}  {'module':<45}{'arguments':<30}\")\n    ch = [ch]\n    layers, save, c2 = [], [], ch[-1]\n \n    for i, (f, n, m, args) in enumerate(d[\"backbone\"] + d[\"head\"]):  # from, number, module, args\n        m = globals()[m] if m in globals() else getattr(nn, m[3:], m)  # get module\n        for j, a in enumerate(args):\n            if isinstance(a, str):\n                with contextlib.suppress(ValueError):\n                    args[j] = ast.literal_eval(a) if a in locals() else a\n \n        n = max(round(n * depth), 1) if n > 1 else n  # depth gain\n        if m in {Conv, Bottleneck, C2f, C3k2, ...}:  # Module list\n            c1, c2 = ch[f], args[0]\n            c2 = make_divisible(min(c2, max_channels) * width, 8)\n            args = [c1, c2, *args[1:]]\n            if m in {C2f, C3k2, ...}:  # Repeated layers\n                args.insert(2, n)\n                n = 1\n        elif m in {Concat, Detect, ...}:  # Head layers\n            args.append([ch[x] for x in f])\n        # Append layers\n        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)\n        layers.append(m_)\n \n        ch.append(c2)\n        save.extend([x % i for x in ([f] if isinstance(f, int) else f) if x != -1])\n \n        if verbose:\n            LOGGER.info(f\"{i:>3}{str(f):>20}{n:>3}{sum(x.numel() for x in m_.parameters()):10.0f}  {str(m):<45}{str(args):<30}\")\n \n    return nn.Sequential(*layers), sorted(save)\n \ndef yaml_model_load(path):\n    \"\"\"Load a YOLO model from a YAML file.\"\"\"\n    path = Path(path)\n    unified_path = path.with_name(path.stem.replace(\"yolov8\", \"yolov\"))\n    yaml_file = check_yaml(str(unified_path), hard=False) or check_yaml(path)\n    d = yaml_load(yaml_file)\n    d[\"scale\"] = guess_model_scale(path)\n    d[\"yaml_file\"] = str(path)\n    return d\n \n# More utility functions...\n \ndef guess_model_scale(model_path):\n    \"\"\"Extract the scale from the YAML file.\"\"\"\n    import re\n    return re.search(r\"yolov\\d+([nslmx])\", Path(model_path).stem).group(1)\n \ndef attempt_load_weights(weights, device=None, inplace=True, fuse=False):\n    \"\"\"Loads weights for a model or an ensemble of models.\"\"\"\n    ensemble = Ensemble()\n    for w in weights if isinstance(weights, list) else [weights]:\n        ckpt, _ = torch_safe_load(w)\n        model = (ckpt.get(\"ema\") or ckpt[\"model\"]).to(device).float()\n        model = model.fuse().eval() if fuse and hasattr(model, \"fuse\") else model.eval()\n        ensemble.append(model)\n \n    return ensemble if len(ensemble) > 1 else ensemble[-1]\n \n```\n\n此 tasks.py 脚本是代码管道的核心部分;它仍然使用 YOLOv8 方法和逻辑;我们只需要将 YOLO11 模型解析到其中。此脚本专为各种计算机视觉任务而设计，例如对象检测、分割、分类、姿势估计、OBB 等。它定义了用于训练、推理和模型管理的基础模型、特定于任务的模型和效用函数。\n\n#### **关键组件：**\n\n- **Imports：**该脚本从 Ultralytics 导入 PyTorch （torch）、神经网络层 （torch.nn） 和实用函数等基本模块。一些关键导入包括：\n  - 对 **C3k2**、**C2PSA**、**C3**、**SPPF、****Concat** 等架构模块进行建模。\n  - 损失函数，如 **v8DetectionLoss**、**v8SegmentationLoss**、**v8ClassificationLoss**、**v8OBBLoss**。\n  - 各种实用程序函数，如 model_info、**fuse_conv_and_bn**、**scale_img** **time_sync**，以帮助进行模型处理、分析和评估。\n\n#### **模型基类：**\n\n1. BaseModel 类：\n   - BaseModel 用作 Ultralytics YOLO 系列中所有模型的基类。\n   - 实现如下基本方法：\n     - **forward（）：**根据输入数据处理训练和推理。\n     - **predict（）：**处理前向传递以进行推理。\n     - **fuse（）：**融合 Conv2d 和 BatchNorm2d 层以提高效率。\n     - **info（）：**提供详细的模型信息。\n   - 此类旨在通过特定于任务的模型（例如检测、分割和分类）进行扩展。\n2. **DetectionModel** **类：**\n   - 扩展 BaseModel，专门用于对象检测任务。\n   - 加载模型配置，初始化检测头（如 Detect 模块）并设置模型步幅。\n   - 它支持使用 YOLOv8 等架构的检测任务，并可以通过 **_predict_augment（）** 执行增强推理。\n\n#### **特定于任务的模型：**\n\n1. **SegmentationModel 的 SegmentationModel** **中：**\n   - 专门用于分割任务（如 YOLOv8 分割）的 DetectionModel 的子类。\n   - 初始化特定于分割的损失函数 （v8SegmentationLoss）。\n2. **PoseModel 的 PoseModel** **中：**\n   - 通过初始化具有关键点检测 （**kpt_shape**） 特定配置的模型来处理姿态估计任务。\n   - 使用 v8PoseLoss 进行特定于姿势的损失计算。\n3. **分类型号****：**\n   - 专为使用 YOLOv8 分类架构的图像分类任务而设计。\n   - 初始化和管理特定于分类的损失 （**v8ClassificationLoss**）。\n   - 它还支持重塑用于分类任务的预训练 TorchVision 模型。\n4. **OBB型号****：**\n   - 用于定向边界框 （OBB） 检测任务。\n   - 实现特定的损失函数 （**v8OBBLoss**） 来处理旋转的边界框。\n5. **世界模型****：**\n   - 此模型处理图像字幕和基于文本的识别等任务。\n   - 利用 CLIP 模型中的文本特征执行基于文本的视觉识别任务。\n   - 包括对文本嵌入 （**txt_feats**） 的特殊处理，用于字幕和世界相关任务。\n\n#### **集成模型：**\n\n1. **集成****：**\n   - 一个简单的 ensemble 类，它将多个模型合并为一个模型。\n   - 允许对不同模型的输出进行平均或串联，以提高整体性能。\n   - 对于组合多个模型的输出提供更好的预测的任务非常有用。\n\n#### **实用功能：**\n\n1. 模型加载和管理：\n   - **attempt_load_weights（）、****attempt_load_one_weight（）**：用于加载模型、管理集成模型以及处理加载预训练权重时的兼容性问题的函数。\n   - 这些功能可确保以适当的步幅、层和配置正确加载模型。\n2. 临时模块重定向：\n   - **temporary_modules（）**：一个上下文管理器，用于临时重定向模块路径，确保在模块位置更改时向后兼容。\n   - 有助于保持与旧型号版本的兼容性。\n3. **Pickle**安全处理：\n   - SafeUnpickler：一个自定义的解封器，可以安全地加载模型检查点，确保未知类被安全的占位符（SafeClass）替换，以避免在加载过程中发生崩溃。\n\n#### **模型解析：**\n\n1. **parse_model（）** **中：**\n   - 此函数将 YAML 文件中的模型配置解析为 PyTorch 模型。\n   - 它处理主干和头架构，解释每个层类型（如 Conv、SPPF、Detect），并构建最终模型。\n   - 支持各种架构，包括 C3k2、C2PSA 等 YOLO11 组件。\n2. YAML 模型加载：\n   - **yaml_model_load（）**）：从 YAML 文件加载模型配置，检测模型比例（例如 n、s、m、l、x）并相应地调整参数。\n   - **guess_model_scale（）、****guess_model_task（）**：用于根据 YAML 文件结构推断模型规模和任务的辅助函数。","source":"_posts/computer-vision/CV007-YOLO11详解.md","raw":"---\ntitle: 'YOLO11 详解'\ndate: 2024-12-7 18:30:00\ncategories:\n  - computer-vision\ntags:\n  - yolo\n  - 目标检测\n\n---\n\n\n\n2024 年是 YOLO 模型的一年。在 2023 年发布 Ultralytics YOLOv8 之后， YOLOv9 和 YOLOv10也在2024年发布了。但等等，这还不是结束！Ultralytics YOLO11 终于来了，在激动人心的 YOLO Vision 2024 （YV24） 活动中亮相。\n\n![](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/feature.gif)\n\nYOLO11 系列是 YOLO 系列中最先进的 （SOTA）、最轻、最高效的型号，性能优于其前代产品。它由 Ultralytics 创建，该组织发布了 YOLOv8，这是迄今为止最稳定和使用最广泛的 YOLO 变体。现在，YOLO11 将继续 YOLO 系列的传统。在本文中，我们将探讨：\n\n- **什么是 YOLO11？**\n- **YOLO11 能做什么？**\n- **YOLO11 比其他 YOLO 变体更高效吗？**\n- **YOLO11 架构有哪些改进？**\n- **YOLO11 的代码pipeline是如何工作的？**\n- **YOLO11 的基准测试**\n- **YOLO11 快速回顾**\n\n# 什么是YOLO11\n\n![](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-1.png)\n\nYOLO11 是 Ultralytics 的 YOLO 系列的最新版本。YOLO11 配备了超轻量级型号，比以前的 YOLO 更快、更高效。YOLO11 能够执行更广泛的计算机视觉任务。Ultralytics 根据大小发布了 5 个 YOLO11 模型，并在**所有任务中发布了 25 个模型**：\n\n- **YOLO11n** – Nano 适用于小型和轻量级任务。\n- **YOLO11s** – Nano 的小升级，具有一些额外的准确性。\n- **YOLO11m** – 通用型。\n- **YOLO11l** – 大，精度更高，计算量更高。\n- **YOLO11x** – 超大尺寸，可实现最大精度和性能。\n\n![](https://learnopencv.com/wp-content/uploads/2024/10/yolo11-model-table.png)\n\nYOLO11 构建在 Ultralytics YOLOv8 代码库之上，并进行了一些架构修改。它还集成了以前 YOLO（如 YOLOv9 和 YOLOv10）的新功能（改进这些功能）以提高性能。我们将在博客文章的后面部分探讨架构和代码库中的新变化。\n\n# YOLO11的应用\n\nYOLO 以其对象检测模型而闻名。但是，YOLO11 可以执行多个计算机视觉任务，例如 YOLOv8。它包括：\n\n- **对象检测**\n- **实例分段**\n- **图像分类**\n- **姿势估计**\n- **定向目标检测 （OBB）**\n\n让我们来探索所有这些。\n\n### 对象检测\n\n![yolo11-对象检测](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-object-detection.gif)\n\nYOLO11 通过将输入图像传递到 CNN 以提取特征来执行对象检测。然后，网络预测这些网格中对象的边界框和类概率。为了处理多尺度检测，使用图层来确保检测到各种大小的物体。然后使用非极大值抑制 （NMS） 来优化这些预测，以过滤掉重复或低置信度的框，从而获得更准确的对象检测。YOLO11 在 MS-COCO 数据集上进行对象检测训练，其中包括 80 个预训练类。\n\n### 实例分割\n\n![ ](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-instance-segmentation-1-1733102326734-1.png)\n\n除了检测对象之外，YOLO11 还通过添加掩码预测分支扩展到实例分割。这些模型在 MS-COCO 数据集上进行训练，其中包括 80 个预训练类。此分支为每个检测到的对象生成像素级分割掩码，使模型能够区分重叠的对象并提供其形状的精确轮廓。head 中的蒙版分支处理特征映射并输出对象蒙版，从而在识别和区分图像中的对象时实现像素级精度。\n\n### 姿势估计\n\n![YOLO11 姿势](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-pose-estimation.gif)\n\nYOLO11 通过检测和预测物体上的关键点（例如人体的关节）来执行姿态估计。关键点连接起来形成骨架结构，该结构表示姿势。这些模型在 COCO 上进行训练，其中包括一个预先训练的类“person”。\n\n在头部添加姿态估计层，并训练网络预测关键点的坐标。后处理步骤将点连接起来以形成骨架结构，从而实现实时姿势识别。\n\n### 图像分类\n\n![YOLO11 图像分类](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-image-classification.gif)\n\n对于图像分类，YOLO11 使用其深度神经网络从输入图像中提取高级特征，并将其分配给多个预定义类别之一。这些模型在 ImageNet 上进行训练，其中包括 1000 个预训练类。该网络通过多层卷积和池化处理图像，在增强基本特征的同时减少空间维度。网络顶部的分类头输出预测的类，使其适用于需要识别图像整体类别的任务。\n\n### 定向目标检测 （OBB）\n\n![YOLO11-OBB](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-detection-1.gif)\n\nYOLO11 通过整合 OBB 扩展了常规对象检测，使模型能够检测和分类旋转或不规则方向的物体。这对于航空影像分析等应用程序特别有用。这些模型在 DOTAv1 上进行训练，其中包括 15 个预训练类。\n\n![YOLO11-OBB](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-logic-1-1024x615.png)\n\nOBB 模型不仅输出边界框坐标，还输出旋转角度 （θ） 或四个角点。这些坐标用于创建与对象方向对齐的边界框，从而提高旋转对象的检测准确性。\n\n# YOLO11 架构和 YOLO11 中的新增功能\n\nYOLO11 架构是对 YOLOv8 架构的升级，具有一些新的集成和参数调整。在我们继续主要部分之前，您可以查看我们关于 [**YOLOv8**](https://learnopencv.com/ultralytics-yolov8/) 的详细文章以大致了解架构。现在，如果你看一下 YOLO11 的配置文件：\n\n```yaml\n# Parameters\nnc: 80 # number of classes\nscales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\n  # [depth, width, max_channels]\n  n: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs\n  s: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\n  m: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs\n  l: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs\n  x: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs\n \n# YOLO11n backbone\nbackbone:\n  # [from, repeats, module, args]\n  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n  - [-1, 2, C3k2, [256, False, 0.25]]\n  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n  - [-1, 2, C3k2, [512, False, 0.25]]\n  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n  - [-1, 2, C3k2, [512, True]]\n  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n  - [-1, 2, C3k2, [1024, True]]\n  - [-1, 1, SPPF, [1024, 5]] # 9\n  - [-1, 2, C2PSA, [1024]] # 10\n \n# YOLO11n head\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n  - [-1, 2, C3k2, [512, False]] # 13\n \n  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n  - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\n \n  - [-1, 1, Conv, [256, 3, 2]]\n  - [[-1, 13], 1, Concat, [1]] # cat head P4\n  - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\n \n  - [-1, 1, Conv, [512, 3, 2]]\n  - [[-1, 10], 1, Concat, [1]] # cat head P5\n  - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\n \n  - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\n```\n\n架构级别的变化：\n\n### **1. 骨干**\n\n主干是模型的一部分，用于从多个比例的输入图像中提取特征。它通常涉及堆叠卷积层和块以创建不同分辨率的特征图。\n\n**卷积层：**YOLO11 具有类似的结构，带有初始卷积层来对图像进行下采样：\n\n```\n- [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n- [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n```\n\n- **C3k2 区块：**YOLO11 引入了 **C3k2 块，而不是 C2f**，它在计算方面效率更高。此块是 **CSP 瓶颈**的自定义实现，它使用两个卷积，而不是一个大型卷积（如 YOLOv8 中所示）。\n\n  - **CSP （Cross Stage Partial）：**CSP 网络拆分特征图并通过瓶颈层处理一部分，同时将另一部分与瓶颈的输出合并。这减少了计算负载并改善了特征表示。\n\n  ```\n  - [-1, 2, C3k2, [256, False, 0.25]]\n  ```\n\n  - C3k2 块还使用较小的内核大小（由 k2 表示），使其更快，同时保持性能。\n\n  **SPPF 和 C2PSA：**YOLO11 保留了 SPPF 块，但在 SPPF 之后添加了一个新的 **C2PSA** 块：\n\n```\n- [-1, 1, SPPF, [1024, 5]]\n- [-1, 2, C2PSA, [1024]\n```\n\n- **C2PSA （Cross Stage Partial with Spatial Attention）** 模块增强了特征图中的空间注意力，从而提高了模型对图像重要部分的关注。这使模型能够通过在空间上池化特征来更有效地关注特定的感兴趣区域。\n\n### **2. neck**\n\nneck 负责聚合来自不同分辨率的特征，并将它们传递给头部进行预测。它通常涉及来自不同级别的特征图的上采样和连接。\n\n**C3k2 区块：**YOLO11 用 **C3k2** 块替换了颈部的 C2f 块。如前所述，C3k2 是一个更快、更高效的区块。例如，在上采样和串联后，YOLO11 中的 neck 如下所示：\n\n```\n\t\n- [-1, 2, C3k2, [512, False]] # P4/16-medium\n```\n\n- 此更改提高了要素聚合过程的速度和性能。\n- **注意力机制：**YOLO11 通过 **C2PSA** 更侧重于空间注意力，这有助于模型专注于图像中的关键区域，以便更好地检测。这在 YOLOv8 中是缺失的，这使得 YOLO11 在检测较小或被遮挡的对象时可能更准确。\n\n------\n\n### **3. head**\n\nhead 是模型中负责生成最终预测的部分。在对象检测中，这通常意味着生成边界框并对这些框内的对象进行分类。\n\n**C3k2 区块：**与颈部类似，YOLO11 取代了头部的 **C2f** 块。\n\n```\n\t\n- [-1, 2, C3k2, [512, False]] # P4/16-medium\n\n```\n\n**检测层：**最终的 Detect 层与 YOLOv8 中的层相同：\n\n```\n- [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\n\n```\n\n使用 C3k2 块使模型在推理方面更快，在参数方面更高效。\n那么，让我们看看新块（层）在代码中的样子：\n\n------\n\n那么，让我们看看新块（层）在代码中的样子：\n\n1. **C3k2 区块（从** **blocks.py** 开始**）：**\n   - **C3k2** 是 **CSP 瓶颈**的更快、更高效的变体。它使用两个卷积而不是一个大型卷积，从而加快了特征提取速度。\n\n```\nclass C3k2(C2f):\n    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        self.m = nn.ModuleList(\n            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)\n        )\n```\n\n2. **C3k 块（从** **blocks.py** 开始**）**：\n\n- **C3k** 是一个更灵活的瓶颈模块，允许自定义内核大小。这对于提取图像中更详细的特征非常有用。\n\n```\nclass C3k(C3):\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)  # hidden channels\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n```\n\n3. **C2PSA 块（从** **blocks.py** 年起**）：**\n\n- **C2PSA** （Cross Stage Partial with Spatial Attention） 增强了模型的空间注意力能力。此模块增加了对特征图的关注，帮助模型专注于图像的重要区域。\n\n```\nclass C2PSA(nn.Module):\n    def __init__(self, c1, c2, e=0.5):\n        super().__init__()\n        c_ = int(c2 * e)\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)\n     \n    def forward(self, x):\n        return self.cv3(torch.cat((self.cv1(x), self.cv2(x)), 1))\n```\n\n# YOLO11 pipeline\n\n在 [**ultralytics**](https://github.com/ultralytics/ultralytics) GitHub 仓库中，我们将主要关注：\n\n1. **nn/modules/** 中的模块\n   - **block.py**\n   - **conv.py**\n   - **head.py**\n   - **transformer.py**\n   - **utils.py**\n2. **nn/tasks.py** 文件\n\n### 1. 代码库概述\n\n代码库被构建为多个模块，这些模块定义了 YOLO11 模型中使用的各种神经网络组件。这些组件在 nn/modules/ 目录中被组织到不同的文件中：\n\n- **block.py**：定义模型中使用的各种构建块（模块），例如瓶颈、CSP 模块和注意力机制。\n- **conv.py**：包含卷积模块，包括标准卷积、深度卷积和其他变体。\n- **head.py**：实现负责生成最终预测（例如，边界框、类概率）的模型头。\n- **transformer.py**：包括基于 transformer 的模块，用于注意力机制和高级特征提取。\n- **utils.py**：提供跨模块使用的实用程序函数和帮助程序类。\n\nnn/tasks.py 文件定义了不同的特定于任务的模型（例如，检测、分割、分类），这些模型将这些模块组合成完整的架构。\n\n### 2. nn/modules/ 中的模块\n\n如前所述，YOLO11 构建在 YOLOv8 代码库之上。因此，我们将主要关注更新的脚本：**block.py**、**conv.py** 和 **head.py** 在这里。\n\n#### **block.py**\n\n此文件定义 YOLO11 模型中使用的各种构建块。这些块是构成神经网络层的基本组件。\n\n##### **关键组件：**\n\n1. 瓶颈模块：\n   - **Bottleneck**：具有可选快捷方式连接的标准瓶颈模块。\n   - **Res**：使用一系列卷积和身份快捷方式的残差块。\n\n```\nclass Bottleneck(nn.Module):\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):\n        super().__init__()\n        c_ = int(c2 * e)\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n \n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n\n```\n\n- Bottleneck 类实现了一个 bottleneck 模块，该模块减少了通道的数量（降维），然后再次扩展它们。\n- **组件**：\n  - self.cv1：一个 1×1 卷积，用于减少通道数。\n  - self.cv2：一个 3×3 卷积，用于将通道数增加回原始通道数。\n  - self.add：一个布尔值，指示是否添加快捷方式连接。\n- **Forward Pass**：输入 x 通过 cv1 和 cv2 传递。如果 self.add 为 True，则原始输入 x 将添加到输出（残差连接）。\n\n2. CSP （Cross Stage Partial） 模块：\n\n- **BottleneckCSP：**瓶颈模块的 CSP 版本。\n- **CSPBlock**：具有多个瓶颈层的更复杂的 CSP 模块。\n\n```\nclass BottleneckCSP(nn.Module):\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        super().__init__()\n        c_ = int(c2 * e)\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = nn.Sequential(\n            *[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)]\n        )\n        self.cv3 = Conv(2 * c_, c2, 1)\n        self.add = c1 == c2\n \n    def forward(self, x):\n        y1 = self.cv2(self.cv1(x))\n        y2 = x if self.add else None\n        return self.cv3(torch.cat((y1, y2), 1)) if y2 is not None else self.cv3(y1)\n\n```\n\n- CSPBottleneck 模块将特征图分为两部分。一部分通过一系列瓶颈层，另一部分直接连接到输出，从而降低了计算成本并增强了梯度流。\n- **组件**：\n  - self.cv1：减少通道数。\n  - self.cv2：瓶颈层序列。\n  - self.cv3：合并功能并调整通道数。\n  - self.add：确定是否添加快捷方式连接。\n\n3. 其他模块：\n\n- **SPPF：**Spatial Pyramid Pooling Fast 模块，可在多个比例下执行池化。\n- **Concat**：沿指定维度连接多个 Tensor。\n\n```\nclass SPPF(nn.Module):\n    def __init__(self, c1, c2, k=5):\n        super().__init__()\n        c_ = c1 // 2\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n \n    def forward(self, x):\n        x = self.cv1(x)\n        y1 = self.m(x)\n        y2 = self.m(y1)\n        y3 = self.m(y2)\n        return self.cv2(torch.cat([x, y1, y2, y3], 1))\n```\n\n- SPPF 模块在不同比例下执行最大池化，并将结果连接起来以捕获多个空间比例的要素。\n- **组件**：\n  - self.cv1：减少通道数。\n  - self.cv2：调整拼接后的 Channel 数。\n  - self.m：最大池化层数。\n- **Forward Pass**：输入 x 通过 cv1，然后通过三个连续的最大池化层（y1、y2、y3）。结果被连接并通过 cv2 传递。\n\n##### **了解概念：**\n\n- **瓶颈层**：用于通过在昂贵的操作之前减少通道数并在之后增加通道数来降低计算复杂性。\n- **残差连接**：通过缓解梯度消失问题来帮助训练更深的网络。\n- **CSP 架构**：将特征图分为两部分;一部分发生转换，而另一部分保持不变，从而提高学习能力并减少计算。\n\n#### **conv.py**\n\n此文件包含各种卷积模块，包括标准卷积和专用卷积。\n\n##### **关键组件：**\n\n**标准卷积模块 （Conv）：**\n\n```\nclass Conv(nn.Module):\n    default_act = nn.SiLU()  # default activation\n \n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n \n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n```\n\n- 实现具有批量规范化和激活的标准卷积层。\n- **组件**：\n  - self.conv：卷积层。\n  - self.bn：批量规范化。\n  - self.act：激活函数（默认为 nn.SiLU（））的\n- **Forward Pass**：应用卷积，然后进行批量规范化和激活。\n\n**深度卷积 （DWConv）：**\n\n```\nclass DWConv(Conv):\n    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):\n        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n```\n\n- 执行深度卷积，其中每个输入通道单独卷积。\n- **组件**：\n  - 继承自 Conv。\n  - 将 groups 参数设置为 c1 和 c2 的最大公约数，从而有效地对每个通道的卷积进行分组。\n\n1. 其他卷积模块：\n   - **Conv2**：RepConv 的简化版本，用于模型压缩和加速。\n   - **GhostConv**：实现 GhostNet 的 ghost 模块，减少特性图中的冗余。\n   - **RepConv**：可重新参数化的卷积层，可以从训练模式转换为推理模式。\n\n##### **了解概念：**\n\n- **自动填充 （****autopad****）：**自动计算保持输出尺寸一致所需的填充。\n- **深度卷积和点卷积**：用于 MobileNet 架构，以减少计算，同时保持准确性。\n- **重新参数化**：RepConv 等技术通过合并层来实现高效的训练和更快的推理。\n\n#### **head.py**\n\n此文件实现了负责生成模型最终预测的 head 模块。\n\n##### **关键组件：**\n\n**检测头 （Detect）：**\n\n```\nclass Detect(nn.Module):\n    def __init__(self, nc=80, ch=()):\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.nl = len(ch)  # number of detection layers\n        self.reg_max = 16  # DFL channels\n        self.no = nc + self.reg_max * 4  # number of outputs per anchor\n        self.stride = torch.zeros(self.nl)  # strides computed during build\n \n        # Define layers\n        self.cv2 = nn.ModuleList(\n            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch\n        )\n        self.cv3 = nn.ModuleList(\n            nn.Sequential(\n                nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),\n                nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),\n                nn.Conv2d(c3, self.nc, 1),\n            )\n            for x in ch\n        )\n        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n```\n\n- Detect 类定义输出边界框坐标和类概率的检测头。\n- **组件**：\n  - self.cv2：用于边界框回归的卷积层。\n  - self.cv3：用于分类的卷积层。\n  - self.dfl：用于边界框细化的 Distribution Focal Loss 模块。\n- **Forward Pass**：处理输入特征映射并输出边界框和类的预测。\n\n**分割 （****Segment****）：**\n\n```\nclass Segment(Detect):\n    def __init__(self, nc=80, nm=32, npr=256, ch=()):\n        super().__init__(nc, ch)\n        self.nm = nm  # number of masks\n        self.npr = npr  # number of prototypes\n        self.proto = Proto(ch[0], self.npr, self.nm)  # protos\n \n        c4 = max(ch[0] // 4, self.nm)\n        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)\n\n```\n\n- 扩展 Detect 类以包含分段功能。\n- **组件**：\n  - self.proto：生成掩码原型。\n  - self.cv4：掩码系数的卷积层。\n- **Forward Pass**：输出边界框、类概率和掩码系数。\n\n**姿势估计头部 （Pose）：**\n\n```\nclass Pose(Detect):\n    def __init__(self, nc=80, kpt_shape=(17, 3), ch=()):\n        super().__init__(nc, ch)\n        self.kpt_shape = kpt_shape  # number of keypoints, number of dimensions\n        self.nk = kpt_shape[0] * kpt_shape[1]  # total number of keypoint outputs\n \n        c4 = max(ch[0] // 4, self.nk)\n        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch)\n\n```\n\n- 扩展了 Detect 类，用于人体姿势估计任务。\n- **组件**：\n  - self.kpt_shape：关键点的形状（关键点的数量、每个关键点的维度）。\n  - self.cv4：用于关键点回归的卷积层。\n- **Forward Pass**：输出边界框、类概率和关键点坐标。\n\n##### **了解概念：**\n\n- **模块化**：通过扩展 Detect 类，我们可以为不同的任务创建专门的 head，同时重用通用功能。\n- **无锚点检测**：现代对象检测器通常使用无锚点方法，直接预测边界框。\n- **关键点估计**：在姿势估计中，模型预测表示关节或地标的关键点。\n\n### 3. **nn/tasks.py** 文件\n\n```python\n# Ultralytics YOLO <img draggable=\"false\" role=\"img\" class=\"emoji\" alt=\"🚀\" src=\"https://s.w.org/images/core/emoji/15.0.3/svg/1f680.svg\">, AGPL-3.0 license\n \nimport contextlib\nimport pickle\nimport types\nfrom copy import deepcopy\nfrom pathlib import Path\n \nimport torch\nimport torch.nn as nn\n \n# Other imports...\n \nclass BaseModel(nn.Module):\n    \"\"\"The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.\"\"\"\n \n    def forward(self, x, *args, **kwargs):\n        \"\"\"Handles both training and inference, returns predictions or loss.\"\"\"\n        if isinstance(x, dict):\n            return self.loss(x, *args, **kwargs)  # Training: return loss\n        return self.predict(x, *args, **kwargs)  # Inference: return predictions\n \n    def predict(self, x, profile=False, visualize=False, augment=False, embed=None):\n        \"\"\"Run a forward pass through the network for inference.\"\"\"\n        if augment:\n            return self._predict_augment(x)\n        return self._predict_once(x, profile, visualize, embed)\n     \n    def fuse(self, verbose=True):\n        \"\"\"Fuses Conv and BatchNorm layers for efficiency during inference.\"\"\"\n        for m in self.model.modules():\n            if isinstance(m, (Conv, Conv2, DWConv)) and hasattr(m, \"bn\"):\n                m.conv = fuse_conv_and_bn(m.conv, m.bn)\n                delattr(m, \"bn\")\n                m.forward = m.forward_fuse  # Use the fused forward\n        return self\n     \n    # More BaseModel methods...\n \nclass DetectionModel(BaseModel):\n    \"\"\"YOLOv8 detection model.\"\"\"\n \n    def __init__(self, cfg=\"yolov8n.yaml\", ch=3, nc=None, verbose=True):\n        \"\"\"Initialize the YOLOv8 detection model with config and parameters.\"\"\"\n        super().__init__()\n        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)\n        self.names = {i: f\"{i}\" for i in range(self.yaml[\"nc\"])}  # Class names\n        self.inplace = self.yaml.get(\"inplace\", True)\n \n        # Initialize strides\n        m = self.model[-1]  # Detect() layer\n        if isinstance(m, Detect):\n            s = 256  # Max stride\n            m.stride = torch.tensor([s / x.shape[-2] for x in self._predict_once(torch.zeros(1, ch, s, s))])\n            self.stride = m.stride\n            m.bias_init()  # Initialize biases\n \n    # More DetectionModel methods...\n \nclass SegmentationModel(DetectionModel):\n    \"\"\"YOLOv8 segmentation model.\"\"\"\n \n    def __init__(self, cfg=\"yolov8n-seg.yaml\", ch=3, nc=None, verbose=True):\n        \"\"\"Initialize YOLOv8 segmentation model with given config and parameters.\"\"\"\n        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\n \n    def init_criterion(self):\n        \"\"\"Initialize the loss criterion for the SegmentationModel.\"\"\"\n        return v8SegmentationLoss(self)\n \nclass PoseModel(DetectionModel):\n    \"\"\"YOLOv8 pose model.\"\"\"\n \n    def __init__(self, cfg=\"yolov8n-pose.yaml\", ch=3, nc=None, data_kpt_shape=(None, None), verbose=True):\n        \"\"\"Initialize YOLOv8 Pose model.\"\"\"\n        if not isinstance(cfg, dict):\n            cfg = yaml_model_load(cfg)\n        if list(data_kpt_shape) != list(cfg[\"kpt_shape\"]):\n            cfg[\"kpt_shape\"] = data_kpt_shape\n        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\n \n    def init_criterion(self):\n        \"\"\"Initialize the loss criterion for the PoseModel.\"\"\"\n        return v8PoseLoss(self)\n \nclass ClassificationModel(BaseModel):\n    \"\"\"YOLOv8 classification model.\"\"\"\n \n    def __init__(self, cfg=\"yolov8n-cls.yaml\", ch=3, nc=None, verbose=True):\n        \"\"\"Initialize the YOLOv8 classification model.\"\"\"\n        super().__init__()\n        self._from_yaml(cfg, ch, nc, verbose)\n \n    def _from_yaml(self, cfg, ch, nc, verbose):\n        \"\"\"Set YOLOv8 model configurations and define the model architecture.\"\"\"\n        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)\n        self.names = {i: f\"{i}\" for i in range(self.yaml[\"nc\"])}\n        self.info()\n \n    def reshape_outputs(model, nc):\n        \"\"\"Update a classification model to match the class count (nc).\"\"\"\n        name, m = list((model.model if hasattr(model, \"model\") else model).named_children())[-1]\n        if isinstance(m, nn.Linear):\n            if m.out_features != nc:\n                setattr(model, name, nn.Linear(m.in_features, nc))\n \n    def init_criterion(self):\n        \"\"\"Initialize the loss criterion for the ClassificationModel.\"\"\"\n        return v8ClassificationLoss()\n \nclass Ensemble(nn.ModuleList):\n    \"\"\"Ensemble of models.\"\"\"\n \n    def __init__(self):\n        \"\"\"Initialize an ensemble of models.\"\"\"\n        super().__init__()\n \n    def forward(self, x, augment=False, profile=False, visualize=False):\n        \"\"\"Generate the ensemble’s final layer by combining outputs from each model.\"\"\"\n        y = [module(x, augment, profile, visualize)[0] for module in self]\n        return torch.cat(y, 2), None  # Concatenate outputs along the third dimension\n \n# Functions ------------------------------------------------------------------------------------------------------------\n \ndef parse_model(d, ch, verbose=True):\n    \"\"\"Parse a YOLO model.yaml dictionary into a PyTorch model.\"\"\"\n    import ast\n \n    max_channels = float(\"inf\")\n    nc, act, scales = (d.get(x) for x in (\"nc\", \"activation\", \"scales\"))\n    depth, width, kpt_shape = (d.get(x, 1.0) for x in (\"depth_multiple\", \"width_multiple\", \"kpt_shape\"))\n \n    # Model scaling\n    if scales:\n        scale = d.get(\"scale\")\n        if not scale:networ\n            scale = tuple(scales.keys())[0]\n            LOGGER.warning(f\"WARNING <img draggable=\"false\" role=\"img\" class=\"emoji\" alt=\"⚠️\" src=\"https://s.w.org/images/core/emoji/15.0.3/svg/26a0.svg\"> no model scale passed. Assuming scale='{scale}'.\")\n        depth, width, max_channels = scales[scale]\n \n    if act:\n        Conv.default_act = eval(act)  # redefine default activation\n        if verbose:\n            LOGGER.info(f\"Activation: {act}\")\n \n    # Logging and parsing layers\n    if verbose:\n        LOGGER.info(f\"\\n{'':>3}{'from':>20}{'n':>3}{'params':>10}  {'module':<45}{'arguments':<30}\")\n    ch = [ch]\n    layers, save, c2 = [], [], ch[-1]\n \n    for i, (f, n, m, args) in enumerate(d[\"backbone\"] + d[\"head\"]):  # from, number, module, args\n        m = globals()[m] if m in globals() else getattr(nn, m[3:], m)  # get module\n        for j, a in enumerate(args):\n            if isinstance(a, str):\n                with contextlib.suppress(ValueError):\n                    args[j] = ast.literal_eval(a) if a in locals() else a\n \n        n = max(round(n * depth), 1) if n > 1 else n  # depth gain\n        if m in {Conv, Bottleneck, C2f, C3k2, ...}:  # Module list\n            c1, c2 = ch[f], args[0]\n            c2 = make_divisible(min(c2, max_channels) * width, 8)\n            args = [c1, c2, *args[1:]]\n            if m in {C2f, C3k2, ...}:  # Repeated layers\n                args.insert(2, n)\n                n = 1\n        elif m in {Concat, Detect, ...}:  # Head layers\n            args.append([ch[x] for x in f])\n        # Append layers\n        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)\n        layers.append(m_)\n \n        ch.append(c2)\n        save.extend([x % i for x in ([f] if isinstance(f, int) else f) if x != -1])\n \n        if verbose:\n            LOGGER.info(f\"{i:>3}{str(f):>20}{n:>3}{sum(x.numel() for x in m_.parameters()):10.0f}  {str(m):<45}{str(args):<30}\")\n \n    return nn.Sequential(*layers), sorted(save)\n \ndef yaml_model_load(path):\n    \"\"\"Load a YOLO model from a YAML file.\"\"\"\n    path = Path(path)\n    unified_path = path.with_name(path.stem.replace(\"yolov8\", \"yolov\"))\n    yaml_file = check_yaml(str(unified_path), hard=False) or check_yaml(path)\n    d = yaml_load(yaml_file)\n    d[\"scale\"] = guess_model_scale(path)\n    d[\"yaml_file\"] = str(path)\n    return d\n \n# More utility functions...\n \ndef guess_model_scale(model_path):\n    \"\"\"Extract the scale from the YAML file.\"\"\"\n    import re\n    return re.search(r\"yolov\\d+([nslmx])\", Path(model_path).stem).group(1)\n \ndef attempt_load_weights(weights, device=None, inplace=True, fuse=False):\n    \"\"\"Loads weights for a model or an ensemble of models.\"\"\"\n    ensemble = Ensemble()\n    for w in weights if isinstance(weights, list) else [weights]:\n        ckpt, _ = torch_safe_load(w)\n        model = (ckpt.get(\"ema\") or ckpt[\"model\"]).to(device).float()\n        model = model.fuse().eval() if fuse and hasattr(model, \"fuse\") else model.eval()\n        ensemble.append(model)\n \n    return ensemble if len(ensemble) > 1 else ensemble[-1]\n \n```\n\n此 tasks.py 脚本是代码管道的核心部分;它仍然使用 YOLOv8 方法和逻辑;我们只需要将 YOLO11 模型解析到其中。此脚本专为各种计算机视觉任务而设计，例如对象检测、分割、分类、姿势估计、OBB 等。它定义了用于训练、推理和模型管理的基础模型、特定于任务的模型和效用函数。\n\n#### **关键组件：**\n\n- **Imports：**该脚本从 Ultralytics 导入 PyTorch （torch）、神经网络层 （torch.nn） 和实用函数等基本模块。一些关键导入包括：\n  - 对 **C3k2**、**C2PSA**、**C3**、**SPPF、****Concat** 等架构模块进行建模。\n  - 损失函数，如 **v8DetectionLoss**、**v8SegmentationLoss**、**v8ClassificationLoss**、**v8OBBLoss**。\n  - 各种实用程序函数，如 model_info、**fuse_conv_and_bn**、**scale_img** **time_sync**，以帮助进行模型处理、分析和评估。\n\n#### **模型基类：**\n\n1. BaseModel 类：\n   - BaseModel 用作 Ultralytics YOLO 系列中所有模型的基类。\n   - 实现如下基本方法：\n     - **forward（）：**根据输入数据处理训练和推理。\n     - **predict（）：**处理前向传递以进行推理。\n     - **fuse（）：**融合 Conv2d 和 BatchNorm2d 层以提高效率。\n     - **info（）：**提供详细的模型信息。\n   - 此类旨在通过特定于任务的模型（例如检测、分割和分类）进行扩展。\n2. **DetectionModel** **类：**\n   - 扩展 BaseModel，专门用于对象检测任务。\n   - 加载模型配置，初始化检测头（如 Detect 模块）并设置模型步幅。\n   - 它支持使用 YOLOv8 等架构的检测任务，并可以通过 **_predict_augment（）** 执行增强推理。\n\n#### **特定于任务的模型：**\n\n1. **SegmentationModel 的 SegmentationModel** **中：**\n   - 专门用于分割任务（如 YOLOv8 分割）的 DetectionModel 的子类。\n   - 初始化特定于分割的损失函数 （v8SegmentationLoss）。\n2. **PoseModel 的 PoseModel** **中：**\n   - 通过初始化具有关键点检测 （**kpt_shape**） 特定配置的模型来处理姿态估计任务。\n   - 使用 v8PoseLoss 进行特定于姿势的损失计算。\n3. **分类型号****：**\n   - 专为使用 YOLOv8 分类架构的图像分类任务而设计。\n   - 初始化和管理特定于分类的损失 （**v8ClassificationLoss**）。\n   - 它还支持重塑用于分类任务的预训练 TorchVision 模型。\n4. **OBB型号****：**\n   - 用于定向边界框 （OBB） 检测任务。\n   - 实现特定的损失函数 （**v8OBBLoss**） 来处理旋转的边界框。\n5. **世界模型****：**\n   - 此模型处理图像字幕和基于文本的识别等任务。\n   - 利用 CLIP 模型中的文本特征执行基于文本的视觉识别任务。\n   - 包括对文本嵌入 （**txt_feats**） 的特殊处理，用于字幕和世界相关任务。\n\n#### **集成模型：**\n\n1. **集成****：**\n   - 一个简单的 ensemble 类，它将多个模型合并为一个模型。\n   - 允许对不同模型的输出进行平均或串联，以提高整体性能。\n   - 对于组合多个模型的输出提供更好的预测的任务非常有用。\n\n#### **实用功能：**\n\n1. 模型加载和管理：\n   - **attempt_load_weights（）、****attempt_load_one_weight（）**：用于加载模型、管理集成模型以及处理加载预训练权重时的兼容性问题的函数。\n   - 这些功能可确保以适当的步幅、层和配置正确加载模型。\n2. 临时模块重定向：\n   - **temporary_modules（）**：一个上下文管理器，用于临时重定向模块路径，确保在模块位置更改时向后兼容。\n   - 有助于保持与旧型号版本的兼容性。\n3. **Pickle**安全处理：\n   - SafeUnpickler：一个自定义的解封器，可以安全地加载模型检查点，确保未知类被安全的占位符（SafeClass）替换，以避免在加载过程中发生崩溃。\n\n#### **模型解析：**\n\n1. **parse_model（）** **中：**\n   - 此函数将 YAML 文件中的模型配置解析为 PyTorch 模型。\n   - 它处理主干和头架构，解释每个层类型（如 Conv、SPPF、Detect），并构建最终模型。\n   - 支持各种架构，包括 C3k2、C2PSA 等 YOLO11 组件。\n2. YAML 模型加载：\n   - **yaml_model_load（）**）：从 YAML 文件加载模型配置，检测模型比例（例如 n、s、m、l、x）并相应地调整参数。\n   - **guess_model_scale（）、****guess_model_task（）**：用于根据 YAML 文件结构推断模型规模和任务的辅助函数。","slug":"computer-vision/CV007-YOLO11详解","published":1,"updated":"2024-12-07T11:41:57.743Z","comments":1,"layout":"post","photos":[],"_id":"cm4e4hinj0001t4hieixca7te","content":"<p>2024 年是 YOLO 模型的一年。在 2023 年发布 Ultralytics YOLOv8 之后， YOLOv9 和 YOLOv10也在2024年发布了。但等等，这还不是结束！Ultralytics YOLO11 终于来了，在激动人心的 YOLO Vision 2024 （YV24） 活动中亮相。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/feature.gif\"></p>\n<p>YOLO11 系列是 YOLO 系列中最先进的 （SOTA）、最轻、最高效的型号，性能优于其前代产品。它由 Ultralytics 创建，该组织发布了 YOLOv8，这是迄今为止最稳定和使用最广泛的 YOLO 变体。现在，YOLO11 将继续 YOLO 系列的传统。在本文中，我们将探讨：</p>\n<ul>\n<li><strong>什么是 YOLO11？</strong></li>\n<li><strong>YOLO11 能做什么？</strong></li>\n<li><strong>YOLO11 比其他 YOLO 变体更高效吗？</strong></li>\n<li><strong>YOLO11 架构有哪些改进？</strong></li>\n<li><strong>YOLO11 的代码pipeline是如何工作的？</strong></li>\n<li><strong>YOLO11 的基准测试</strong></li>\n<li><strong>YOLO11 快速回顾</strong></li>\n</ul>\n<h1 id=\"什么是YOLO11\"><a href=\"#什么是YOLO11\" class=\"headerlink\" title=\"什么是YOLO11\"></a>什么是YOLO11</h1><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-1.png\"></p>\n<p>YOLO11 是 Ultralytics 的 YOLO 系列的最新版本。YOLO11 配备了超轻量级型号，比以前的 YOLO 更快、更高效。YOLO11 能够执行更广泛的计算机视觉任务。Ultralytics 根据大小发布了 5 个 YOLO11 模型，并在<strong>所有任务中发布了 25 个模型</strong>：</p>\n<ul>\n<li><strong>YOLO11n</strong> – Nano 适用于小型和轻量级任务。</li>\n<li><strong>YOLO11s</strong> – Nano 的小升级，具有一些额外的准确性。</li>\n<li><strong>YOLO11m</strong> – 通用型。</li>\n<li><strong>YOLO11l</strong> – 大，精度更高，计算量更高。</li>\n<li><strong>YOLO11x</strong> – 超大尺寸，可实现最大精度和性能。</li>\n</ul>\n<p><img src=\"https://learnopencv.com/wp-content/uploads/2024/10/yolo11-model-table.png\"></p>\n<p>YOLO11 构建在 Ultralytics YOLOv8 代码库之上，并进行了一些架构修改。它还集成了以前 YOLO（如 YOLOv9 和 YOLOv10）的新功能（改进这些功能）以提高性能。我们将在博客文章的后面部分探讨架构和代码库中的新变化。</p>\n<h1 id=\"YOLO11的应用\"><a href=\"#YOLO11的应用\" class=\"headerlink\" title=\"YOLO11的应用\"></a>YOLO11的应用</h1><p>YOLO 以其对象检测模型而闻名。但是，YOLO11 可以执行多个计算机视觉任务，例如 YOLOv8。它包括：</p>\n<ul>\n<li><strong>对象检测</strong></li>\n<li><strong>实例分段</strong></li>\n<li><strong>图像分类</strong></li>\n<li><strong>姿势估计</strong></li>\n<li><strong>定向目标检测 （OBB）</strong></li>\n</ul>\n<p>让我们来探索所有这些。</p>\n<h3 id=\"对象检测\"><a href=\"#对象检测\" class=\"headerlink\" title=\"对象检测\"></a>对象检测</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-object-detection.gif\" alt=\"yolo11-对象检测\"></p>\n<p>YOLO11 通过将输入图像传递到 CNN 以提取特征来执行对象检测。然后，网络预测这些网格中对象的边界框和类概率。为了处理多尺度检测，使用图层来确保检测到各种大小的物体。然后使用非极大值抑制 （NMS） 来优化这些预测，以过滤掉重复或低置信度的框，从而获得更准确的对象检测。YOLO11 在 MS-COCO 数据集上进行对象检测训练，其中包括 80 个预训练类。</p>\n<h3 id=\"实例分割\"><a href=\"#实例分割\" class=\"headerlink\" title=\"实例分割\"></a>实例分割</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-instance-segmentation-1-1733102326734-1.png\" alt=\" \"></p>\n<p>除了检测对象之外，YOLO11 还通过添加掩码预测分支扩展到实例分割。这些模型在 MS-COCO 数据集上进行训练，其中包括 80 个预训练类。此分支为每个检测到的对象生成像素级分割掩码，使模型能够区分重叠的对象并提供其形状的精确轮廓。head 中的蒙版分支处理特征映射并输出对象蒙版，从而在识别和区分图像中的对象时实现像素级精度。</p>\n<h3 id=\"姿势估计\"><a href=\"#姿势估计\" class=\"headerlink\" title=\"姿势估计\"></a>姿势估计</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-pose-estimation.gif\" alt=\"YOLO11 姿势\"></p>\n<p>YOLO11 通过检测和预测物体上的关键点（例如人体的关节）来执行姿态估计。关键点连接起来形成骨架结构，该结构表示姿势。这些模型在 COCO 上进行训练，其中包括一个预先训练的类“person”。</p>\n<p>在头部添加姿态估计层，并训练网络预测关键点的坐标。后处理步骤将点连接起来以形成骨架结构，从而实现实时姿势识别。</p>\n<h3 id=\"图像分类\"><a href=\"#图像分类\" class=\"headerlink\" title=\"图像分类\"></a>图像分类</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-image-classification.gif\" alt=\"YOLO11 图像分类\"></p>\n<p>对于图像分类，YOLO11 使用其深度神经网络从输入图像中提取高级特征，并将其分配给多个预定义类别之一。这些模型在 ImageNet 上进行训练，其中包括 1000 个预训练类。该网络通过多层卷积和池化处理图像，在增强基本特征的同时减少空间维度。网络顶部的分类头输出预测的类，使其适用于需要识别图像整体类别的任务。</p>\n<h3 id=\"定向目标检测-（OBB）\"><a href=\"#定向目标检测-（OBB）\" class=\"headerlink\" title=\"定向目标检测 （OBB）\"></a>定向目标检测 （OBB）</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-detection-1.gif\" alt=\"YOLO11-OBB\"></p>\n<p>YOLO11 通过整合 OBB 扩展了常规对象检测，使模型能够检测和分类旋转或不规则方向的物体。这对于航空影像分析等应用程序特别有用。这些模型在 DOTAv1 上进行训练，其中包括 15 个预训练类。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-logic-1-1024x615.png\" alt=\"YOLO11-OBB\"></p>\n<p>OBB 模型不仅输出边界框坐标，还输出旋转角度 （θ） 或四个角点。这些坐标用于创建与对象方向对齐的边界框，从而提高旋转对象的检测准确性。</p>\n<h1 id=\"YOLO11-架构和-YOLO11-中的新增功能\"><a href=\"#YOLO11-架构和-YOLO11-中的新增功能\" class=\"headerlink\" title=\"YOLO11 架构和 YOLO11 中的新增功能\"></a>YOLO11 架构和 YOLO11 中的新增功能</h1><p>YOLO11 架构是对 YOLOv8 架构的升级，具有一些新的集成和参数调整。在我们继续主要部分之前，您可以查看我们关于 <a href=\"https://learnopencv.com/ultralytics-yolov8/\"><strong>YOLOv8</strong></a> 的详细文章以大致了解架构。现在，如果你看一下 YOLO11 的配置文件：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Parameters</span></span><br><span class=\"line\"><span class=\"attr\">nc:</span> <span class=\"number\">80</span> <span class=\"comment\"># number of classes</span></span><br><span class=\"line\"><span class=\"attr\">scales:</span> <span class=\"comment\"># model compound scaling constants, i.e. &#x27;model=yolo11n.yaml&#x27; will call yolo11.yaml with scale &#x27;n&#x27;</span></span><br><span class=\"line\">  <span class=\"comment\"># [depth, width, max_channels]</span></span><br><span class=\"line\">  <span class=\"attr\">n:</span> [<span class=\"number\">0.50</span>, <span class=\"number\">0.25</span>, <span class=\"number\">1024</span>] <span class=\"comment\"># summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs</span></span><br><span class=\"line\">  <span class=\"attr\">s:</span> [<span class=\"number\">0.50</span>, <span class=\"number\">0.50</span>, <span class=\"number\">1024</span>] <span class=\"comment\"># summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs</span></span><br><span class=\"line\">  <span class=\"attr\">m:</span> [<span class=\"number\">0.50</span>, <span class=\"number\">1.00</span>, <span class=\"number\">512</span>] <span class=\"comment\"># summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs</span></span><br><span class=\"line\">  <span class=\"attr\">l:</span> [<span class=\"number\">1.00</span>, <span class=\"number\">1.00</span>, <span class=\"number\">512</span>] <span class=\"comment\"># summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs</span></span><br><span class=\"line\">  <span class=\"attr\">x:</span> [<span class=\"number\">1.00</span>, <span class=\"number\">1.50</span>, <span class=\"number\">512</span>] <span class=\"comment\"># summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># YOLO11n backbone</span></span><br><span class=\"line\"><span class=\"attr\">backbone:</span></span><br><span class=\"line\">  <span class=\"comment\"># [from, repeats, module, args]</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">64</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 0-P1/2</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">128</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 1-P2/4</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">256</span>, <span class=\"literal\">False</span>, <span class=\"number\">0.25</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">256</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 3-P3/8</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">512</span>, <span class=\"literal\">False</span>, <span class=\"number\">0.25</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">512</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 5-P4/16</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">512</span>, <span class=\"literal\">True</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">1024</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 7-P5/32</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">1024</span>, <span class=\"literal\">True</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">SPPF</span>, [<span class=\"number\">1024</span>, <span class=\"number\">5</span>]] <span class=\"comment\"># 9</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C2PSA</span>, [<span class=\"number\">1024</span>]] <span class=\"comment\"># 10</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># YOLO11n head</span></span><br><span class=\"line\"><span class=\"attr\">head:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">nn.Upsample</span>, [<span class=\"string\">None</span>, <span class=\"number\">2</span>, <span class=\"string\">&quot;nearest&quot;</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">-1</span>, <span class=\"number\">6</span>], <span class=\"number\">1</span>, <span class=\"string\">Concat</span>, [<span class=\"number\">1</span>]] <span class=\"comment\"># cat backbone P4</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">512</span>, <span class=\"literal\">False</span>]] <span class=\"comment\"># 13</span></span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">nn.Upsample</span>, [<span class=\"string\">None</span>, <span class=\"number\">2</span>, <span class=\"string\">&quot;nearest&quot;</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">-1</span>, <span class=\"number\">4</span>], <span class=\"number\">1</span>, <span class=\"string\">Concat</span>, [<span class=\"number\">1</span>]] <span class=\"comment\"># cat backbone P3</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">256</span>, <span class=\"literal\">False</span>]] <span class=\"comment\"># 16 (P3/8-small)</span></span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">256</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">-1</span>, <span class=\"number\">13</span>], <span class=\"number\">1</span>, <span class=\"string\">Concat</span>, [<span class=\"number\">1</span>]] <span class=\"comment\"># cat head P4</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">512</span>, <span class=\"literal\">False</span>]] <span class=\"comment\"># 19 (P4/16-medium)</span></span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">512</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">-1</span>, <span class=\"number\">10</span>], <span class=\"number\">1</span>, <span class=\"string\">Concat</span>, [<span class=\"number\">1</span>]] <span class=\"comment\"># cat head P5</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">1024</span>, <span class=\"literal\">True</span>]] <span class=\"comment\"># 22 (P5/32-large)</span></span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">16</span>, <span class=\"number\">19</span>, <span class=\"number\">22</span>], <span class=\"number\">1</span>, <span class=\"string\">Detect</span>, [<span class=\"string\">nc</span>]] <span class=\"comment\"># Detect(P3, P4, P5)</span></span><br></pre></td></tr></table></figure>\n\n<p>架构级别的变化：</p>\n<h3 id=\"1-骨干\"><a href=\"#1-骨干\" class=\"headerlink\" title=\"1. 骨干\"></a><strong>1. 骨干</strong></h3><p>主干是模型的一部分，用于从多个比例的输入图像中提取特征。它通常涉及堆叠卷积层和块以创建不同分辨率的特征图。</p>\n<p><strong>卷积层：</strong>YOLO11 具有类似的结构，带有初始卷积层来对图像进行下采样：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- [-1, 1, Conv, [64, 3, 2]] # 0-P1/2</span><br><span class=\"line\">- [-1, 1, Conv, [128, 3, 2]] # 1-P2/4</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><strong>C3k2 区块：</strong>YOLO11 引入了 <strong>C3k2 块，而不是 C2f</strong>，它在计算方面效率更高。此块是 <strong>CSP 瓶颈</strong>的自定义实现，它使用两个卷积，而不是一个大型卷积（如 YOLOv8 中所示）。</p>\n<ul>\n<li><strong>CSP （Cross Stage Partial）：</strong>CSP 网络拆分特征图并通过瓶颈层处理一部分，同时将另一部分与瓶颈的输出合并。这减少了计算负载并改善了特征表示。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- [-1, 2, C3k2, [256, False, 0.25]]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>C3k2 块还使用较小的内核大小（由 k2 表示），使其更快，同时保持性能。</li>\n</ul>\n<p><strong>SPPF 和 C2PSA：</strong>YOLO11 保留了 SPPF 块，但在 SPPF 之后添加了一个新的 <strong>C2PSA</strong> 块：</p>\n</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- [-1, 1, SPPF, [1024, 5]]</span><br><span class=\"line\">- [-1, 2, C2PSA, [1024]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><strong>C2PSA （Cross Stage Partial with Spatial Attention）</strong> 模块增强了特征图中的空间注意力，从而提高了模型对图像重要部分的关注。这使模型能够通过在空间上池化特征来更有效地关注特定的感兴趣区域。</li>\n</ul>\n<h3 id=\"2-neck\"><a href=\"#2-neck\" class=\"headerlink\" title=\"2. neck\"></a><strong>2. neck</strong></h3><p>neck 负责聚合来自不同分辨率的特征，并将它们传递给头部进行预测。它通常涉及来自不同级别的特征图的上采样和连接。</p>\n<p><strong>C3k2 区块：</strong>YOLO11 用 <strong>C3k2</strong> 块替换了颈部的 C2f 块。如前所述，C3k2 是一个更快、更高效的区块。例如，在上采样和串联后，YOLO11 中的 neck 如下所示：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t</span><br><span class=\"line\">- [-1, 2, C3k2, [512, False]] # P4/16-medium</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>此更改提高了要素聚合过程的速度和性能。</li>\n<li><strong>注意力机制：</strong>YOLO11 通过 <strong>C2PSA</strong> 更侧重于空间注意力，这有助于模型专注于图像中的关键区域，以便更好地检测。这在 YOLOv8 中是缺失的，这使得 YOLO11 在检测较小或被遮挡的对象时可能更准确。</li>\n</ul>\n<hr>\n<h3 id=\"3-head\"><a href=\"#3-head\" class=\"headerlink\" title=\"3. head\"></a><strong>3. head</strong></h3><p>head 是模型中负责生成最终预测的部分。在对象检测中，这通常意味着生成边界框并对这些框内的对象进行分类。</p>\n<p><strong>C3k2 区块：</strong>与颈部类似，YOLO11 取代了头部的 <strong>C2f</strong> 块。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t</span><br><span class=\"line\">- [-1, 2, C3k2, [512, False]] # P4/16-medium</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p><strong>检测层：</strong>最终的 Detect 层与 YOLOv8 中的层相同：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>使用 C3k2 块使模型在推理方面更快，在参数方面更高效。<br>那么，让我们看看新块（层）在代码中的样子：</p>\n<hr>\n<p>那么，让我们看看新块（层）在代码中的样子：</p>\n<ol>\n<li><strong>C3k2 区块（从</strong> <strong>blocks.py</strong> 开始<strong>）：</strong><ul>\n<li><strong>C3k2</strong> 是 <strong>CSP 瓶颈</strong>的更快、更高效的变体。它使用两个卷积而不是一个大型卷积，从而加快了特征提取速度。</li>\n</ul>\n</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class C3k2(C2f):</span><br><span class=\"line\">    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):</span><br><span class=\"line\">        super().__init__(c1, c2, n, shortcut, g, e)</span><br><span class=\"line\">        self.m = nn.ModuleList(</span><br><span class=\"line\">            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)</span><br><span class=\"line\">        )</span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li><strong>C3k 块（从</strong> <strong>blocks.py</strong> 开始<strong>）</strong>：</li>\n</ol>\n<ul>\n<li><strong>C3k</strong> 是一个更灵活的瓶颈模块，允许自定义内核大小。这对于提取图像中更详细的特征非常有用。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class C3k(C3):</span><br><span class=\"line\">    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):</span><br><span class=\"line\">        super().__init__(c1, c2, n, shortcut, g, e)</span><br><span class=\"line\">        c_ = int(c2 * e)  # hidden channels</span><br><span class=\"line\">        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))</span><br></pre></td></tr></table></figure>\n\n<ol start=\"3\">\n<li><strong>C2PSA 块（从</strong> <strong>blocks.py</strong> 年起<strong>）：</strong></li>\n</ol>\n<ul>\n<li><strong>C2PSA</strong> （Cross Stage Partial with Spatial Attention） 增强了模型的空间注意力能力。此模块增加了对特征图的关注，帮助模型专注于图像的重要区域。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class C2PSA(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, e=0.5):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        c_ = int(c2 * e)</span><br><span class=\"line\">        self.cv1 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv2 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv3 = Conv(2 * c_, c2, 1)</span><br><span class=\"line\">     </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        return self.cv3(torch.cat((self.cv1(x), self.cv2(x)), 1))</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"YOLO11-pipeline\"><a href=\"#YOLO11-pipeline\" class=\"headerlink\" title=\"YOLO11 pipeline\"></a>YOLO11 pipeline</h1><p>在 <a href=\"https://github.com/ultralytics/ultralytics\"><strong>ultralytics</strong></a> GitHub 仓库中，我们将主要关注：</p>\n<ol>\n<li><strong>nn&#x2F;modules&#x2F;</strong> 中的模块<ul>\n<li><strong>block.py</strong></li>\n<li><strong>conv.py</strong></li>\n<li><strong>head.py</strong></li>\n<li><strong>transformer.py</strong></li>\n<li><strong>utils.py</strong></li>\n</ul>\n</li>\n<li><strong>nn&#x2F;tasks.py</strong> 文件</li>\n</ol>\n<h3 id=\"1-代码库概述\"><a href=\"#1-代码库概述\" class=\"headerlink\" title=\"1. 代码库概述\"></a>1. 代码库概述</h3><p>代码库被构建为多个模块，这些模块定义了 YOLO11 模型中使用的各种神经网络组件。这些组件在 nn&#x2F;modules&#x2F; 目录中被组织到不同的文件中：</p>\n<ul>\n<li><strong>block.py</strong>：定义模型中使用的各种构建块（模块），例如瓶颈、CSP 模块和注意力机制。</li>\n<li><strong>conv.py</strong>：包含卷积模块，包括标准卷积、深度卷积和其他变体。</li>\n<li><strong>head.py</strong>：实现负责生成最终预测（例如，边界框、类概率）的模型头。</li>\n<li><strong>transformer.py</strong>：包括基于 transformer 的模块，用于注意力机制和高级特征提取。</li>\n<li><strong>utils.py</strong>：提供跨模块使用的实用程序函数和帮助程序类。</li>\n</ul>\n<p>nn&#x2F;tasks.py 文件定义了不同的特定于任务的模型（例如，检测、分割、分类），这些模型将这些模块组合成完整的架构。</p>\n<h3 id=\"2-nn-modules-中的模块\"><a href=\"#2-nn-modules-中的模块\" class=\"headerlink\" title=\"2. nn&#x2F;modules&#x2F; 中的模块\"></a>2. nn&#x2F;modules&#x2F; 中的模块</h3><p>如前所述，YOLO11 构建在 YOLOv8 代码库之上。因此，我们将主要关注更新的脚本：<strong>block.py</strong>、<strong>conv.py</strong> 和 <strong>head.py</strong> 在这里。</p>\n<h4 id=\"block-py\"><a href=\"#block-py\" class=\"headerlink\" title=\"block.py\"></a><strong>block.py</strong></h4><p>此文件定义 YOLO11 模型中使用的各种构建块。这些块是构成神经网络层的基本组件。</p>\n<h5 id=\"关键组件：\"><a href=\"#关键组件：\" class=\"headerlink\" title=\"关键组件：\"></a><strong>关键组件：</strong></h5><ol>\n<li>瓶颈模块：<ul>\n<li><strong>Bottleneck</strong>：具有可选快捷方式连接的标准瓶颈模块。</li>\n<li><strong>Res</strong>：使用一系列卷积和身份快捷方式的残差块。</li>\n</ul>\n</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Bottleneck(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        c_ = int(c2 * e)</span><br><span class=\"line\">        self.cv1 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv2 = Conv(c_, c2, 3, 1, g=g)</span><br><span class=\"line\">        self.add = shortcut and c1 == c2</span><br><span class=\"line\"> </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>Bottleneck 类实现了一个 bottleneck 模块，该模块减少了通道的数量（降维），然后再次扩展它们。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.cv1：一个 1×1 卷积，用于减少通道数。</li>\n<li>self.cv2：一个 3×3 卷积，用于将通道数增加回原始通道数。</li>\n<li>self.add：一个布尔值，指示是否添加快捷方式连接。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：输入 x 通过 cv1 和 cv2 传递。如果 self.add 为 True，则原始输入 x 将添加到输出（残差连接）。</li>\n</ul>\n<ol start=\"2\">\n<li>CSP （Cross Stage Partial） 模块：</li>\n</ol>\n<ul>\n<li><strong>BottleneckCSP：</strong>瓶颈模块的 CSP 版本。</li>\n<li><strong>CSPBlock</strong>：具有多个瓶颈层的更复杂的 CSP 模块。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class BottleneckCSP(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        c_ = int(c2 * e)</span><br><span class=\"line\">        self.cv1 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv2 = nn.Sequential(</span><br><span class=\"line\">            *[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)]</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.cv3 = Conv(2 * c_, c2, 1)</span><br><span class=\"line\">        self.add = c1 == c2</span><br><span class=\"line\"> </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        y1 = self.cv2(self.cv1(x))</span><br><span class=\"line\">        y2 = x if self.add else None</span><br><span class=\"line\">        return self.cv3(torch.cat((y1, y2), 1)) if y2 is not None else self.cv3(y1)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>CSPBottleneck 模块将特征图分为两部分。一部分通过一系列瓶颈层，另一部分直接连接到输出，从而降低了计算成本并增强了梯度流。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.cv1：减少通道数。</li>\n<li>self.cv2：瓶颈层序列。</li>\n<li>self.cv3：合并功能并调整通道数。</li>\n<li>self.add：确定是否添加快捷方式连接。</li>\n</ul>\n</li>\n</ul>\n<ol start=\"3\">\n<li>其他模块：</li>\n</ol>\n<ul>\n<li><strong>SPPF：</strong>Spatial Pyramid Pooling Fast 模块，可在多个比例下执行池化。</li>\n<li><strong>Concat</strong>：沿指定维度连接多个 Tensor。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class SPPF(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, k=5):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        c_ = c1 // 2</span><br><span class=\"line\">        self.cv1 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv2 = Conv(c_ * 4, c2, 1, 1)</span><br><span class=\"line\">        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)</span><br><span class=\"line\"> </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        x = self.cv1(x)</span><br><span class=\"line\">        y1 = self.m(x)</span><br><span class=\"line\">        y2 = self.m(y1)</span><br><span class=\"line\">        y3 = self.m(y2)</span><br><span class=\"line\">        return self.cv2(torch.cat([x, y1, y2, y3], 1))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>SPPF 模块在不同比例下执行最大池化，并将结果连接起来以捕获多个空间比例的要素。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.cv1：减少通道数。</li>\n<li>self.cv2：调整拼接后的 Channel 数。</li>\n<li>self.m：最大池化层数。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：输入 x 通过 cv1，然后通过三个连续的最大池化层（y1、y2、y3）。结果被连接并通过 cv2 传递。</li>\n</ul>\n<h5 id=\"了解概念：\"><a href=\"#了解概念：\" class=\"headerlink\" title=\"了解概念：\"></a><strong>了解概念：</strong></h5><ul>\n<li><strong>瓶颈层</strong>：用于通过在昂贵的操作之前减少通道数并在之后增加通道数来降低计算复杂性。</li>\n<li><strong>残差连接</strong>：通过缓解梯度消失问题来帮助训练更深的网络。</li>\n<li><strong>CSP 架构</strong>：将特征图分为两部分;一部分发生转换，而另一部分保持不变，从而提高学习能力并减少计算。</li>\n</ul>\n<h4 id=\"conv-py\"><a href=\"#conv-py\" class=\"headerlink\" title=\"conv.py\"></a><strong>conv.py</strong></h4><p>此文件包含各种卷积模块，包括标准卷积和专用卷积。</p>\n<h5 id=\"关键组件：-1\"><a href=\"#关键组件：-1\" class=\"headerlink\" title=\"关键组件：\"></a><strong>关键组件：</strong></h5><p><strong>标准卷积模块 （Conv）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Conv(nn.Module):</span><br><span class=\"line\">    default_act = nn.SiLU()  # default activation</span><br><span class=\"line\"> </span><br><span class=\"line\">    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)</span><br><span class=\"line\">        self.bn = nn.BatchNorm2d(c2)</span><br><span class=\"line\">        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()</span><br><span class=\"line\"> </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        return self.act(self.bn(self.conv(x)))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>实现具有批量规范化和激活的标准卷积层。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.conv：卷积层。</li>\n<li>self.bn：批量规范化。</li>\n<li>self.act：激活函数（默认为 nn.SiLU（））的</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：应用卷积，然后进行批量规范化和激活。</li>\n</ul>\n<p><strong>深度卷积 （DWConv）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class DWConv(Conv):</span><br><span class=\"line\">    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):</span><br><span class=\"line\">        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>执行深度卷积，其中每个输入通道单独卷积。</li>\n<li><strong>组件</strong>：<ul>\n<li>继承自 Conv。</li>\n<li>将 groups 参数设置为 c1 和 c2 的最大公约数，从而有效地对每个通道的卷积进行分组。</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>其他卷积模块：<ul>\n<li><strong>Conv2</strong>：RepConv 的简化版本，用于模型压缩和加速。</li>\n<li><strong>GhostConv</strong>：实现 GhostNet 的 ghost 模块，减少特性图中的冗余。</li>\n<li><strong>RepConv</strong>：可重新参数化的卷积层，可以从训练模式转换为推理模式。</li>\n</ul>\n</li>\n</ol>\n<h5 id=\"了解概念：-1\"><a href=\"#了解概念：-1\" class=\"headerlink\" title=\"了解概念：\"></a><strong>了解概念：</strong></h5><ul>\n<li><strong>自动填充 （<strong><strong>autopad</strong></strong>）：</strong>自动计算保持输出尺寸一致所需的填充。</li>\n<li><strong>深度卷积和点卷积</strong>：用于 MobileNet 架构，以减少计算，同时保持准确性。</li>\n<li><strong>重新参数化</strong>：RepConv 等技术通过合并层来实现高效的训练和更快的推理。</li>\n</ul>\n<h4 id=\"head-py\"><a href=\"#head-py\" class=\"headerlink\" title=\"head.py\"></a><strong>head.py</strong></h4><p>此文件实现了负责生成模型最终预测的 head 模块。</p>\n<h5 id=\"关键组件：-2\"><a href=\"#关键组件：-2\" class=\"headerlink\" title=\"关键组件：\"></a><strong>关键组件：</strong></h5><p><strong>检测头 （Detect）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Detect(nn.Module):</span><br><span class=\"line\">    def __init__(self, nc=80, ch=()):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.nc = nc  # number of classes</span><br><span class=\"line\">        self.nl = len(ch)  # number of detection layers</span><br><span class=\"line\">        self.reg_max = 16  # DFL channels</span><br><span class=\"line\">        self.no = nc + self.reg_max * 4  # number of outputs per anchor</span><br><span class=\"line\">        self.stride = torch.zeros(self.nl)  # strides computed during build</span><br><span class=\"line\"> </span><br><span class=\"line\">        # Define layers</span><br><span class=\"line\">        self.cv2 = nn.ModuleList(</span><br><span class=\"line\">            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.cv3 = nn.ModuleList(</span><br><span class=\"line\">            nn.Sequential(</span><br><span class=\"line\">                nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),</span><br><span class=\"line\">                nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),</span><br><span class=\"line\">                nn.Conv2d(c3, self.nc, 1),</span><br><span class=\"line\">            )</span><br><span class=\"line\">            for x in ch</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.dfl = DFL(self.reg_max) if self.reg_max &gt; 1 else nn.Identity()</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>Detect 类定义输出边界框坐标和类概率的检测头。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.cv2：用于边界框回归的卷积层。</li>\n<li>self.cv3：用于分类的卷积层。</li>\n<li>self.dfl：用于边界框细化的 Distribution Focal Loss 模块。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：处理输入特征映射并输出边界框和类的预测。</li>\n</ul>\n<p><strong>分割 （<strong><strong>Segment</strong></strong>）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Segment(Detect):</span><br><span class=\"line\">    def __init__(self, nc=80, nm=32, npr=256, ch=()):</span><br><span class=\"line\">        super().__init__(nc, ch)</span><br><span class=\"line\">        self.nm = nm  # number of masks</span><br><span class=\"line\">        self.npr = npr  # number of prototypes</span><br><span class=\"line\">        self.proto = Proto(ch[0], self.npr, self.nm)  # protos</span><br><span class=\"line\"> </span><br><span class=\"line\">        c4 = max(ch[0] // 4, self.nm)</span><br><span class=\"line\">        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>扩展 Detect 类以包含分段功能。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.proto：生成掩码原型。</li>\n<li>self.cv4：掩码系数的卷积层。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：输出边界框、类概率和掩码系数。</li>\n</ul>\n<p><strong>姿势估计头部 （Pose）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Pose(Detect):</span><br><span class=\"line\">    def __init__(self, nc=80, kpt_shape=(17, 3), ch=()):</span><br><span class=\"line\">        super().__init__(nc, ch)</span><br><span class=\"line\">        self.kpt_shape = kpt_shape  # number of keypoints, number of dimensions</span><br><span class=\"line\">        self.nk = kpt_shape[0] * kpt_shape[1]  # total number of keypoint outputs</span><br><span class=\"line\"> </span><br><span class=\"line\">        c4 = max(ch[0] // 4, self.nk)</span><br><span class=\"line\">        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>扩展了 Detect 类，用于人体姿势估计任务。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.kpt_shape：关键点的形状（关键点的数量、每个关键点的维度）。</li>\n<li>self.cv4：用于关键点回归的卷积层。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：输出边界框、类概率和关键点坐标。</li>\n</ul>\n<h5 id=\"了解概念：-2\"><a href=\"#了解概念：-2\" class=\"headerlink\" title=\"了解概念：\"></a><strong>了解概念：</strong></h5><ul>\n<li><strong>模块化</strong>：通过扩展 Detect 类，我们可以为不同的任务创建专门的 head，同时重用通用功能。</li>\n<li><strong>无锚点检测</strong>：现代对象检测器通常使用无锚点方法，直接预测边界框。</li>\n<li><strong>关键点估计</strong>：在姿势估计中，模型预测表示关节或地标的关键点。</li>\n</ul>\n<h3 id=\"3-nn-tasks-py-文件\"><a href=\"#3-nn-tasks-py-文件\" class=\"headerlink\" title=\"3. nn&#x2F;tasks.py 文件\"></a>3. <strong>nn&#x2F;tasks.py</strong> 文件</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Ultralytics YOLO &lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;🚀&quot; src=&quot;https://s.w.org/images/core/emoji/15.0.3/svg/1f680.svg&quot;&gt;, AGPL-3.0 license</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> contextlib</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">import</span> types</span><br><span class=\"line\"><span class=\"keyword\">from</span> copy <span class=\"keyword\">import</span> deepcopy</span><br><span class=\"line\"><span class=\"keyword\">from</span> pathlib <span class=\"keyword\">import</span> Path</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># Other imports...</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BaseModel</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, *args, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Handles both training and inference, returns predictions or loss.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(x, <span class=\"built_in\">dict</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.loss(x, *args, **kwargs)  <span class=\"comment\"># Training: return loss</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.predict(x, *args, **kwargs)  <span class=\"comment\"># Inference: return predictions</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x, profile=<span class=\"literal\">False</span>, visualize=<span class=\"literal\">False</span>, augment=<span class=\"literal\">False</span>, embed=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Run a forward pass through the network for inference.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> augment:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>._predict_augment(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>._predict_once(x, profile, visualize, embed)</span><br><span class=\"line\">     </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fuse</span>(<span class=\"params\">self, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Fuses Conv and BatchNorm layers for efficiency during inference.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.model.modules():</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, (Conv, Conv2, DWConv)) <span class=\"keyword\">and</span> <span class=\"built_in\">hasattr</span>(m, <span class=\"string\">&quot;bn&quot;</span>):</span><br><span class=\"line\">                m.conv = fuse_conv_and_bn(m.conv, m.bn)</span><br><span class=\"line\">                <span class=\"built_in\">delattr</span>(m, <span class=\"string\">&quot;bn&quot;</span>)</span><br><span class=\"line\">                m.forward = m.forward_fuse  <span class=\"comment\"># Use the fused forward</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span></span><br><span class=\"line\">     </span><br><span class=\"line\">    <span class=\"comment\"># More BaseModel methods...</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DetectionModel</span>(<span class=\"title class_ inherited__\">BaseModel</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;YOLOv8 detection model.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, cfg=<span class=\"string\">&quot;yolov8n.yaml&quot;</span>, ch=<span class=\"number\">3</span>, nc=<span class=\"literal\">None</span>, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the YOLOv8 detection model with config and parameters.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.yaml = cfg <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(cfg, <span class=\"built_in\">dict</span>) <span class=\"keyword\">else</span> yaml_model_load(cfg)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.model, <span class=\"variable language_\">self</span>.save = parse_model(deepcopy(<span class=\"variable language_\">self</span>.yaml), ch=ch, verbose=verbose)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.names = &#123;i: <span class=\"string\">f&quot;<span class=\"subst\">&#123;i&#125;</span>&quot;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"variable language_\">self</span>.yaml[<span class=\"string\">&quot;nc&quot;</span>])&#125;  <span class=\"comment\"># Class names</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.inplace = <span class=\"variable language_\">self</span>.yaml.get(<span class=\"string\">&quot;inplace&quot;</span>, <span class=\"literal\">True</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\"># Initialize strides</span></span><br><span class=\"line\">        m = <span class=\"variable language_\">self</span>.model[-<span class=\"number\">1</span>]  <span class=\"comment\"># Detect() layer</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, Detect):</span><br><span class=\"line\">            s = <span class=\"number\">256</span>  <span class=\"comment\"># Max stride</span></span><br><span class=\"line\">            m.stride = torch.tensor([s / x.shape[-<span class=\"number\">2</span>] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>._predict_once(torch.zeros(<span class=\"number\">1</span>, ch, s, s))])</span><br><span class=\"line\">            <span class=\"variable language_\">self</span>.stride = m.stride</span><br><span class=\"line\">            m.bias_init()  <span class=\"comment\"># Initialize biases</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># More DetectionModel methods...</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SegmentationModel</span>(<span class=\"title class_ inherited__\">DetectionModel</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;YOLOv8 segmentation model.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, cfg=<span class=\"string\">&quot;yolov8n-seg.yaml&quot;</span>, ch=<span class=\"number\">3</span>, nc=<span class=\"literal\">None</span>, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize YOLOv8 segmentation model with given config and parameters.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_criterion</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the loss criterion for the SegmentationModel.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> v8SegmentationLoss(<span class=\"variable language_\">self</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PoseModel</span>(<span class=\"title class_ inherited__\">DetectionModel</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;YOLOv8 pose model.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, cfg=<span class=\"string\">&quot;yolov8n-pose.yaml&quot;</span>, ch=<span class=\"number\">3</span>, nc=<span class=\"literal\">None</span>, data_kpt_shape=(<span class=\"params\"><span class=\"literal\">None</span>, <span class=\"literal\">None</span></span>), verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize YOLOv8 Pose model.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(cfg, <span class=\"built_in\">dict</span>):</span><br><span class=\"line\">            cfg = yaml_model_load(cfg)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">list</span>(data_kpt_shape) != <span class=\"built_in\">list</span>(cfg[<span class=\"string\">&quot;kpt_shape&quot;</span>]):</span><br><span class=\"line\">            cfg[<span class=\"string\">&quot;kpt_shape&quot;</span>] = data_kpt_shape</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_criterion</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the loss criterion for the PoseModel.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> v8PoseLoss(<span class=\"variable language_\">self</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ClassificationModel</span>(<span class=\"title class_ inherited__\">BaseModel</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;YOLOv8 classification model.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, cfg=<span class=\"string\">&quot;yolov8n-cls.yaml&quot;</span>, ch=<span class=\"number\">3</span>, nc=<span class=\"literal\">None</span>, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the YOLOv8 classification model.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>._from_yaml(cfg, ch, nc, verbose)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_from_yaml</span>(<span class=\"params\">self, cfg, ch, nc, verbose</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Set YOLOv8 model configurations and define the model architecture.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.yaml = cfg <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(cfg, <span class=\"built_in\">dict</span>) <span class=\"keyword\">else</span> yaml_model_load(cfg)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.model, <span class=\"variable language_\">self</span>.save = parse_model(deepcopy(<span class=\"variable language_\">self</span>.yaml), ch=ch, verbose=verbose)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.names = &#123;i: <span class=\"string\">f&quot;<span class=\"subst\">&#123;i&#125;</span>&quot;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"variable language_\">self</span>.yaml[<span class=\"string\">&quot;nc&quot;</span>])&#125;</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.info()</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reshape_outputs</span>(<span class=\"params\">model, nc</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Update a classification model to match the class count (nc).&quot;&quot;&quot;</span></span><br><span class=\"line\">        name, m = <span class=\"built_in\">list</span>((model.model <span class=\"keyword\">if</span> <span class=\"built_in\">hasattr</span>(model, <span class=\"string\">&quot;model&quot;</span>) <span class=\"keyword\">else</span> model).named_children())[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.Linear):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> m.out_features != nc:</span><br><span class=\"line\">                <span class=\"built_in\">setattr</span>(model, name, nn.Linear(m.in_features, nc))</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_criterion</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the loss criterion for the ClassificationModel.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> v8ClassificationLoss()</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Ensemble</span>(nn.ModuleList):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Ensemble of models.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize an ensemble of models.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, augment=<span class=\"literal\">False</span>, profile=<span class=\"literal\">False</span>, visualize=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Generate the ensemble’s final layer by combining outputs from each model.&quot;&quot;&quot;</span></span><br><span class=\"line\">        y = [module(x, augment, profile, visualize)[<span class=\"number\">0</span>] <span class=\"keyword\">for</span> module <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.cat(y, <span class=\"number\">2</span>), <span class=\"literal\">None</span>  <span class=\"comment\"># Concatenate outputs along the third dimension</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># Functions ------------------------------------------------------------------------------------------------------------</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">parse_model</span>(<span class=\"params\">d, ch, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Parse a YOLO model.yaml dictionary into a PyTorch model.&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> ast</span><br><span class=\"line\"> </span><br><span class=\"line\">    max_channels = <span class=\"built_in\">float</span>(<span class=\"string\">&quot;inf&quot;</span>)</span><br><span class=\"line\">    nc, act, scales = (d.get(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> (<span class=\"string\">&quot;nc&quot;</span>, <span class=\"string\">&quot;activation&quot;</span>, <span class=\"string\">&quot;scales&quot;</span>))</span><br><span class=\"line\">    depth, width, kpt_shape = (d.get(x, <span class=\"number\">1.0</span>) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> (<span class=\"string\">&quot;depth_multiple&quot;</span>, <span class=\"string\">&quot;width_multiple&quot;</span>, <span class=\"string\">&quot;kpt_shape&quot;</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Model scaling</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> scales:</span><br><span class=\"line\">        scale = d.get(<span class=\"string\">&quot;scale&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> scale:networ</span><br><span class=\"line\">            scale = <span class=\"built_in\">tuple</span>(scales.keys())[<span class=\"number\">0</span>]</span><br><span class=\"line\">            LOGGER.warning(<span class=\"string\">f&quot;WARNING &lt;img draggable=&quot;</span>false<span class=\"string\">&quot; role=&quot;</span>img<span class=\"string\">&quot; class=&quot;</span>emoji<span class=\"string\">&quot; alt=&quot;</span>⚠️<span class=\"string\">&quot; src=&quot;</span>https://s.w.org/images/core/emoji/<span class=\"number\">15.0</span><span class=\"number\">.3</span>/svg/26a0.svg<span class=\"string\">&quot;&gt; no model scale passed. Assuming scale=&#x27;&#123;scale&#125;&#x27;.&quot;</span>)</span><br><span class=\"line\">        depth, width, max_channels = scales[scale]</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">if</span> act:</span><br><span class=\"line\">        Conv.default_act = <span class=\"built_in\">eval</span>(act)  <span class=\"comment\"># redefine default activation</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> verbose:</span><br><span class=\"line\">            LOGGER.info(<span class=\"string\">f&quot;Activation: <span class=\"subst\">&#123;act&#125;</span>&quot;</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Logging and parsing layers</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> verbose:</span><br><span class=\"line\">        LOGGER.info(<span class=\"string\">f&quot;\\n<span class=\"subst\">&#123;<span class=\"string\">&#x27;&#x27;</span>:&gt;<span class=\"number\">3</span>&#125;</span><span class=\"subst\">&#123;<span class=\"string\">&#x27;from&#x27;</span>:&gt;<span class=\"number\">20</span>&#125;</span><span class=\"subst\">&#123;<span class=\"string\">&#x27;n&#x27;</span>:&gt;<span class=\"number\">3</span>&#125;</span><span class=\"subst\">&#123;<span class=\"string\">&#x27;params&#x27;</span>:&gt;<span class=\"number\">10</span>&#125;</span>  <span class=\"subst\">&#123;<span class=\"string\">&#x27;module&#x27;</span>:&lt;<span class=\"number\">45</span>&#125;</span><span class=\"subst\">&#123;<span class=\"string\">&#x27;arguments&#x27;</span>:&lt;<span class=\"number\">30</span>&#125;</span>&quot;</span>)</span><br><span class=\"line\">    ch = [ch]</span><br><span class=\"line\">    layers, save, c2 = [], [], ch[-<span class=\"number\">1</span>]</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (f, n, m, args) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(d[<span class=\"string\">&quot;backbone&quot;</span>] + d[<span class=\"string\">&quot;head&quot;</span>]):  <span class=\"comment\"># from, number, module, args</span></span><br><span class=\"line\">        m = <span class=\"built_in\">globals</span>()[m] <span class=\"keyword\">if</span> m <span class=\"keyword\">in</span> <span class=\"built_in\">globals</span>() <span class=\"keyword\">else</span> <span class=\"built_in\">getattr</span>(nn, m[<span class=\"number\">3</span>:], m)  <span class=\"comment\"># get module</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j, a <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(args):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(a, <span class=\"built_in\">str</span>):</span><br><span class=\"line\">                <span class=\"keyword\">with</span> contextlib.suppress(ValueError):</span><br><span class=\"line\">                    args[j] = ast.literal_eval(a) <span class=\"keyword\">if</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">locals</span>() <span class=\"keyword\">else</span> a</span><br><span class=\"line\"> </span><br><span class=\"line\">        n = <span class=\"built_in\">max</span>(<span class=\"built_in\">round</span>(n * depth), <span class=\"number\">1</span>) <span class=\"keyword\">if</span> n &gt; <span class=\"number\">1</span> <span class=\"keyword\">else</span> n  <span class=\"comment\"># depth gain</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> m <span class=\"keyword\">in</span> &#123;Conv, Bottleneck, C2f, C3k2, ...&#125;:  <span class=\"comment\"># Module list</span></span><br><span class=\"line\">            c1, c2 = ch[f], args[<span class=\"number\">0</span>]</span><br><span class=\"line\">            c2 = make_divisible(<span class=\"built_in\">min</span>(c2, max_channels) * width, <span class=\"number\">8</span>)</span><br><span class=\"line\">            args = [c1, c2, *args[<span class=\"number\">1</span>:]]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> m <span class=\"keyword\">in</span> &#123;C2f, C3k2, ...&#125;:  <span class=\"comment\"># Repeated layers</span></span><br><span class=\"line\">                args.insert(<span class=\"number\">2</span>, n)</span><br><span class=\"line\">                n = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> m <span class=\"keyword\">in</span> &#123;Concat, Detect, ...&#125;:  <span class=\"comment\"># Head layers</span></span><br><span class=\"line\">            args.append([ch[x] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> f])</span><br><span class=\"line\">        <span class=\"comment\"># Append layers</span></span><br><span class=\"line\">        m_ = nn.Sequential(*(m(*args) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n))) <span class=\"keyword\">if</span> n &gt; <span class=\"number\">1</span> <span class=\"keyword\">else</span> m(*args)</span><br><span class=\"line\">        layers.append(m_)</span><br><span class=\"line\"> </span><br><span class=\"line\">        ch.append(c2)</span><br><span class=\"line\">        save.extend([x % i <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> ([f] <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(f, <span class=\"built_in\">int</span>) <span class=\"keyword\">else</span> f) <span class=\"keyword\">if</span> x != -<span class=\"number\">1</span>])</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"keyword\">if</span> verbose:</span><br><span class=\"line\">            LOGGER.info(<span class=\"string\">f&quot;<span class=\"subst\">&#123;i:&gt;<span class=\"number\">3</span>&#125;</span><span class=\"subst\">&#123;<span class=\"built_in\">str</span>(f):&gt;<span class=\"number\">20</span>&#125;</span><span class=\"subst\">&#123;n:&gt;<span class=\"number\">3</span>&#125;</span><span class=\"subst\">&#123;<span class=\"built_in\">sum</span>(x.numel() <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> m_.parameters()):<span class=\"number\">10.0</span>f&#125;</span>  <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(m):&lt;<span class=\"number\">45</span>&#125;</span><span class=\"subst\">&#123;<span class=\"built_in\">str</span>(args):&lt;<span class=\"number\">30</span>&#125;</span>&quot;</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(*layers), <span class=\"built_in\">sorted</span>(save)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">yaml_model_load</span>(<span class=\"params\">path</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Load a YOLO model from a YAML file.&quot;&quot;&quot;</span></span><br><span class=\"line\">    path = Path(path)</span><br><span class=\"line\">    unified_path = path.with_name(path.stem.replace(<span class=\"string\">&quot;yolov8&quot;</span>, <span class=\"string\">&quot;yolov&quot;</span>))</span><br><span class=\"line\">    yaml_file = check_yaml(<span class=\"built_in\">str</span>(unified_path), hard=<span class=\"literal\">False</span>) <span class=\"keyword\">or</span> check_yaml(path)</span><br><span class=\"line\">    d = yaml_load(yaml_file)</span><br><span class=\"line\">    d[<span class=\"string\">&quot;scale&quot;</span>] = guess_model_scale(path)</span><br><span class=\"line\">    d[<span class=\"string\">&quot;yaml_file&quot;</span>] = <span class=\"built_in\">str</span>(path)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> d</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># More utility functions...</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">guess_model_scale</span>(<span class=\"params\">model_path</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Extract the scale from the YAML file.&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> re</span><br><span class=\"line\">    <span class=\"keyword\">return</span> re.search(<span class=\"string\">r&quot;yolov\\d+([nslmx])&quot;</span>, Path(model_path).stem).group(<span class=\"number\">1</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attempt_load_weights</span>(<span class=\"params\">weights, device=<span class=\"literal\">None</span>, inplace=<span class=\"literal\">True</span>, fuse=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Loads weights for a model or an ensemble of models.&quot;&quot;&quot;</span></span><br><span class=\"line\">    ensemble = Ensemble()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> w <span class=\"keyword\">in</span> weights <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(weights, <span class=\"built_in\">list</span>) <span class=\"keyword\">else</span> [weights]:</span><br><span class=\"line\">        ckpt, _ = torch_safe_load(w)</span><br><span class=\"line\">        model = (ckpt.get(<span class=\"string\">&quot;ema&quot;</span>) <span class=\"keyword\">or</span> ckpt[<span class=\"string\">&quot;model&quot;</span>]).to(device).<span class=\"built_in\">float</span>()</span><br><span class=\"line\">        model = model.fuse().<span class=\"built_in\">eval</span>() <span class=\"keyword\">if</span> fuse <span class=\"keyword\">and</span> <span class=\"built_in\">hasattr</span>(model, <span class=\"string\">&quot;fuse&quot;</span>) <span class=\"keyword\">else</span> model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">        ensemble.append(model)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> ensemble <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(ensemble) &gt; <span class=\"number\">1</span> <span class=\"keyword\">else</span> ensemble[-<span class=\"number\">1</span>]</span><br><span class=\"line\"> </span><br></pre></td></tr></table></figure>\n\n<p>此 tasks.py 脚本是代码管道的核心部分;它仍然使用 YOLOv8 方法和逻辑;我们只需要将 YOLO11 模型解析到其中。此脚本专为各种计算机视觉任务而设计，例如对象检测、分割、分类、姿势估计、OBB 等。它定义了用于训练、推理和模型管理的基础模型、特定于任务的模型和效用函数。</p>\n<h4 id=\"关键组件：-3\"><a href=\"#关键组件：-3\" class=\"headerlink\" title=\"关键组件：\"></a><strong>关键组件：</strong></h4><ul>\n<li><strong>Imports：</strong>该脚本从 Ultralytics 导入 PyTorch （torch）、神经网络层 （torch.nn） 和实用函数等基本模块。一些关键导入包括：<ul>\n<li>对 <strong>C3k2</strong>、<strong>C2PSA</strong>、<strong>C3</strong>、<strong>SPPF、****Concat</strong> 等架构模块进行建模。</li>\n<li>损失函数，如 <strong>v8DetectionLoss</strong>、<strong>v8SegmentationLoss</strong>、<strong>v8ClassificationLoss</strong>、<strong>v8OBBLoss</strong>。</li>\n<li>各种实用程序函数，如 model_info、<strong>fuse_conv_and_bn</strong>、<strong>scale_img</strong> <strong>time_sync</strong>，以帮助进行模型处理、分析和评估。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"模型基类：\"><a href=\"#模型基类：\" class=\"headerlink\" title=\"模型基类：\"></a><strong>模型基类：</strong></h4><ol>\n<li>BaseModel 类：<ul>\n<li>BaseModel 用作 Ultralytics YOLO 系列中所有模型的基类。</li>\n<li>实现如下基本方法：<ul>\n<li><strong>forward（）：</strong>根据输入数据处理训练和推理。</li>\n<li><strong>predict（）：</strong>处理前向传递以进行推理。</li>\n<li><strong>fuse（）：</strong>融合 Conv2d 和 BatchNorm2d 层以提高效率。</li>\n<li><strong>info（）：</strong>提供详细的模型信息。</li>\n</ul>\n</li>\n<li>此类旨在通过特定于任务的模型（例如检测、分割和分类）进行扩展。</li>\n</ul>\n</li>\n<li><strong>DetectionModel</strong> <strong>类：</strong><ul>\n<li>扩展 BaseModel，专门用于对象检测任务。</li>\n<li>加载模型配置，初始化检测头（如 Detect 模块）并设置模型步幅。</li>\n<li>它支持使用 YOLOv8 等架构的检测任务，并可以通过 <strong>_predict_augment（）</strong> 执行增强推理。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"特定于任务的模型：\"><a href=\"#特定于任务的模型：\" class=\"headerlink\" title=\"特定于任务的模型：\"></a><strong>特定于任务的模型：</strong></h4><ol>\n<li><strong>SegmentationModel 的 SegmentationModel</strong> <strong>中：</strong><ul>\n<li>专门用于分割任务（如 YOLOv8 分割）的 DetectionModel 的子类。</li>\n<li>初始化特定于分割的损失函数 （v8SegmentationLoss）。</li>\n</ul>\n</li>\n<li><strong>PoseModel 的 PoseModel</strong> <strong>中：</strong><ul>\n<li>通过初始化具有关键点检测 （<strong>kpt_shape</strong>） 特定配置的模型来处理姿态估计任务。</li>\n<li>使用 v8PoseLoss 进行特定于姿势的损失计算。</li>\n</ul>\n</li>\n<li><strong>分类型号****：</strong><ul>\n<li>专为使用 YOLOv8 分类架构的图像分类任务而设计。</li>\n<li>初始化和管理特定于分类的损失 （<strong>v8ClassificationLoss</strong>）。</li>\n<li>它还支持重塑用于分类任务的预训练 TorchVision 模型。</li>\n</ul>\n</li>\n<li><strong>OBB型号****：</strong><ul>\n<li>用于定向边界框 （OBB） 检测任务。</li>\n<li>实现特定的损失函数 （<strong>v8OBBLoss</strong>） 来处理旋转的边界框。</li>\n</ul>\n</li>\n<li><strong>世界模型****：</strong><ul>\n<li>此模型处理图像字幕和基于文本的识别等任务。</li>\n<li>利用 CLIP 模型中的文本特征执行基于文本的视觉识别任务。</li>\n<li>包括对文本嵌入 （<strong>txt_feats</strong>） 的特殊处理，用于字幕和世界相关任务。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"集成模型：\"><a href=\"#集成模型：\" class=\"headerlink\" title=\"集成模型：\"></a><strong>集成模型：</strong></h4><ol>\n<li><strong>集成****：</strong><ul>\n<li>一个简单的 ensemble 类，它将多个模型合并为一个模型。</li>\n<li>允许对不同模型的输出进行平均或串联，以提高整体性能。</li>\n<li>对于组合多个模型的输出提供更好的预测的任务非常有用。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"实用功能：\"><a href=\"#实用功能：\" class=\"headerlink\" title=\"实用功能：\"></a><strong>实用功能：</strong></h4><ol>\n<li>模型加载和管理：<ul>\n<li><strong>attempt_load_weights（）、****attempt_load_one_weight（）</strong>：用于加载模型、管理集成模型以及处理加载预训练权重时的兼容性问题的函数。</li>\n<li>这些功能可确保以适当的步幅、层和配置正确加载模型。</li>\n</ul>\n</li>\n<li>临时模块重定向：<ul>\n<li><strong>temporary_modules（）</strong>：一个上下文管理器，用于临时重定向模块路径，确保在模块位置更改时向后兼容。</li>\n<li>有助于保持与旧型号版本的兼容性。</li>\n</ul>\n</li>\n<li><strong>Pickle</strong>安全处理：<ul>\n<li>SafeUnpickler：一个自定义的解封器，可以安全地加载模型检查点，确保未知类被安全的占位符（SafeClass）替换，以避免在加载过程中发生崩溃。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"模型解析：\"><a href=\"#模型解析：\" class=\"headerlink\" title=\"模型解析：\"></a><strong>模型解析：</strong></h4><ol>\n<li><strong>parse_model（）</strong> <strong>中：</strong><ul>\n<li>此函数将 YAML 文件中的模型配置解析为 PyTorch 模型。</li>\n<li>它处理主干和头架构，解释每个层类型（如 Conv、SPPF、Detect），并构建最终模型。</li>\n<li>支持各种架构，包括 C3k2、C2PSA 等 YOLO11 组件。</li>\n</ul>\n</li>\n<li>YAML 模型加载：<ul>\n<li><strong>yaml_model_load（）</strong>）：从 YAML 文件加载模型配置，检测模型比例（例如 n、s、m、l、x）并相应地调整参数。</li>\n<li><strong>guess_model_scale（）、****guess_model_task（）</strong>：用于根据 YAML 文件结构推断模型规模和任务的辅助函数。</li>\n</ul>\n</li>\n</ol>\n","excerpt":"","more":"<p>2024 年是 YOLO 模型的一年。在 2023 年发布 Ultralytics YOLOv8 之后， YOLOv9 和 YOLOv10也在2024年发布了。但等等，这还不是结束！Ultralytics YOLO11 终于来了，在激动人心的 YOLO Vision 2024 （YV24） 活动中亮相。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/feature.gif\"></p>\n<p>YOLO11 系列是 YOLO 系列中最先进的 （SOTA）、最轻、最高效的型号，性能优于其前代产品。它由 Ultralytics 创建，该组织发布了 YOLOv8，这是迄今为止最稳定和使用最广泛的 YOLO 变体。现在，YOLO11 将继续 YOLO 系列的传统。在本文中，我们将探讨：</p>\n<ul>\n<li><strong>什么是 YOLO11？</strong></li>\n<li><strong>YOLO11 能做什么？</strong></li>\n<li><strong>YOLO11 比其他 YOLO 变体更高效吗？</strong></li>\n<li><strong>YOLO11 架构有哪些改进？</strong></li>\n<li><strong>YOLO11 的代码pipeline是如何工作的？</strong></li>\n<li><strong>YOLO11 的基准测试</strong></li>\n<li><strong>YOLO11 快速回顾</strong></li>\n</ul>\n<h1 id=\"什么是YOLO11\"><a href=\"#什么是YOLO11\" class=\"headerlink\" title=\"什么是YOLO11\"></a>什么是YOLO11</h1><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-1.png\"></p>\n<p>YOLO11 是 Ultralytics 的 YOLO 系列的最新版本。YOLO11 配备了超轻量级型号，比以前的 YOLO 更快、更高效。YOLO11 能够执行更广泛的计算机视觉任务。Ultralytics 根据大小发布了 5 个 YOLO11 模型，并在<strong>所有任务中发布了 25 个模型</strong>：</p>\n<ul>\n<li><strong>YOLO11n</strong> – Nano 适用于小型和轻量级任务。</li>\n<li><strong>YOLO11s</strong> – Nano 的小升级，具有一些额外的准确性。</li>\n<li><strong>YOLO11m</strong> – 通用型。</li>\n<li><strong>YOLO11l</strong> – 大，精度更高，计算量更高。</li>\n<li><strong>YOLO11x</strong> – 超大尺寸，可实现最大精度和性能。</li>\n</ul>\n<p><img src=\"https://learnopencv.com/wp-content/uploads/2024/10/yolo11-model-table.png\"></p>\n<p>YOLO11 构建在 Ultralytics YOLOv8 代码库之上，并进行了一些架构修改。它还集成了以前 YOLO（如 YOLOv9 和 YOLOv10）的新功能（改进这些功能）以提高性能。我们将在博客文章的后面部分探讨架构和代码库中的新变化。</p>\n<h1 id=\"YOLO11的应用\"><a href=\"#YOLO11的应用\" class=\"headerlink\" title=\"YOLO11的应用\"></a>YOLO11的应用</h1><p>YOLO 以其对象检测模型而闻名。但是，YOLO11 可以执行多个计算机视觉任务，例如 YOLOv8。它包括：</p>\n<ul>\n<li><strong>对象检测</strong></li>\n<li><strong>实例分段</strong></li>\n<li><strong>图像分类</strong></li>\n<li><strong>姿势估计</strong></li>\n<li><strong>定向目标检测 （OBB）</strong></li>\n</ul>\n<p>让我们来探索所有这些。</p>\n<h3 id=\"对象检测\"><a href=\"#对象检测\" class=\"headerlink\" title=\"对象检测\"></a>对象检测</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-object-detection.gif\" alt=\"yolo11-对象检测\"></p>\n<p>YOLO11 通过将输入图像传递到 CNN 以提取特征来执行对象检测。然后，网络预测这些网格中对象的边界框和类概率。为了处理多尺度检测，使用图层来确保检测到各种大小的物体。然后使用非极大值抑制 （NMS） 来优化这些预测，以过滤掉重复或低置信度的框，从而获得更准确的对象检测。YOLO11 在 MS-COCO 数据集上进行对象检测训练，其中包括 80 个预训练类。</p>\n<h3 id=\"实例分割\"><a href=\"#实例分割\" class=\"headerlink\" title=\"实例分割\"></a>实例分割</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-instance-segmentation-1-1733102326734-1.png\" alt=\" \"></p>\n<p>除了检测对象之外，YOLO11 还通过添加掩码预测分支扩展到实例分割。这些模型在 MS-COCO 数据集上进行训练，其中包括 80 个预训练类。此分支为每个检测到的对象生成像素级分割掩码，使模型能够区分重叠的对象并提供其形状的精确轮廓。head 中的蒙版分支处理特征映射并输出对象蒙版，从而在识别和区分图像中的对象时实现像素级精度。</p>\n<h3 id=\"姿势估计\"><a href=\"#姿势估计\" class=\"headerlink\" title=\"姿势估计\"></a>姿势估计</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-pose-estimation.gif\" alt=\"YOLO11 姿势\"></p>\n<p>YOLO11 通过检测和预测物体上的关键点（例如人体的关节）来执行姿态估计。关键点连接起来形成骨架结构，该结构表示姿势。这些模型在 COCO 上进行训练，其中包括一个预先训练的类“person”。</p>\n<p>在头部添加姿态估计层，并训练网络预测关键点的坐标。后处理步骤将点连接起来以形成骨架结构，从而实现实时姿势识别。</p>\n<h3 id=\"图像分类\"><a href=\"#图像分类\" class=\"headerlink\" title=\"图像分类\"></a>图像分类</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-image-classification.gif\" alt=\"YOLO11 图像分类\"></p>\n<p>对于图像分类，YOLO11 使用其深度神经网络从输入图像中提取高级特征，并将其分配给多个预定义类别之一。这些模型在 ImageNet 上进行训练，其中包括 1000 个预训练类。该网络通过多层卷积和池化处理图像，在增强基本特征的同时减少空间维度。网络顶部的分类头输出预测的类，使其适用于需要识别图像整体类别的任务。</p>\n<h3 id=\"定向目标检测-（OBB）\"><a href=\"#定向目标检测-（OBB）\" class=\"headerlink\" title=\"定向目标检测 （OBB）\"></a>定向目标检测 （OBB）</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-detection-1.gif\" alt=\"YOLO11-OBB\"></p>\n<p>YOLO11 通过整合 OBB 扩展了常规对象检测，使模型能够检测和分类旋转或不规则方向的物体。这对于航空影像分析等应用程序特别有用。这些模型在 DOTAv1 上进行训练，其中包括 15 个预训练类。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-logic-1-1024x615.png\" alt=\"YOLO11-OBB\"></p>\n<p>OBB 模型不仅输出边界框坐标，还输出旋转角度 （θ） 或四个角点。这些坐标用于创建与对象方向对齐的边界框，从而提高旋转对象的检测准确性。</p>\n<h1 id=\"YOLO11-架构和-YOLO11-中的新增功能\"><a href=\"#YOLO11-架构和-YOLO11-中的新增功能\" class=\"headerlink\" title=\"YOLO11 架构和 YOLO11 中的新增功能\"></a>YOLO11 架构和 YOLO11 中的新增功能</h1><p>YOLO11 架构是对 YOLOv8 架构的升级，具有一些新的集成和参数调整。在我们继续主要部分之前，您可以查看我们关于 <a href=\"https://learnopencv.com/ultralytics-yolov8/\"><strong>YOLOv8</strong></a> 的详细文章以大致了解架构。现在，如果你看一下 YOLO11 的配置文件：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Parameters</span></span><br><span class=\"line\"><span class=\"attr\">nc:</span> <span class=\"number\">80</span> <span class=\"comment\"># number of classes</span></span><br><span class=\"line\"><span class=\"attr\">scales:</span> <span class=\"comment\"># model compound scaling constants, i.e. &#x27;model=yolo11n.yaml&#x27; will call yolo11.yaml with scale &#x27;n&#x27;</span></span><br><span class=\"line\">  <span class=\"comment\"># [depth, width, max_channels]</span></span><br><span class=\"line\">  <span class=\"attr\">n:</span> [<span class=\"number\">0.50</span>, <span class=\"number\">0.25</span>, <span class=\"number\">1024</span>] <span class=\"comment\"># summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs</span></span><br><span class=\"line\">  <span class=\"attr\">s:</span> [<span class=\"number\">0.50</span>, <span class=\"number\">0.50</span>, <span class=\"number\">1024</span>] <span class=\"comment\"># summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs</span></span><br><span class=\"line\">  <span class=\"attr\">m:</span> [<span class=\"number\">0.50</span>, <span class=\"number\">1.00</span>, <span class=\"number\">512</span>] <span class=\"comment\"># summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs</span></span><br><span class=\"line\">  <span class=\"attr\">l:</span> [<span class=\"number\">1.00</span>, <span class=\"number\">1.00</span>, <span class=\"number\">512</span>] <span class=\"comment\"># summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs</span></span><br><span class=\"line\">  <span class=\"attr\">x:</span> [<span class=\"number\">1.00</span>, <span class=\"number\">1.50</span>, <span class=\"number\">512</span>] <span class=\"comment\"># summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># YOLO11n backbone</span></span><br><span class=\"line\"><span class=\"attr\">backbone:</span></span><br><span class=\"line\">  <span class=\"comment\"># [from, repeats, module, args]</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">64</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 0-P1/2</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">128</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 1-P2/4</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">256</span>, <span class=\"literal\">False</span>, <span class=\"number\">0.25</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">256</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 3-P3/8</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">512</span>, <span class=\"literal\">False</span>, <span class=\"number\">0.25</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">512</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 5-P4/16</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">512</span>, <span class=\"literal\">True</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">1024</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]] <span class=\"comment\"># 7-P5/32</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">1024</span>, <span class=\"literal\">True</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">SPPF</span>, [<span class=\"number\">1024</span>, <span class=\"number\">5</span>]] <span class=\"comment\"># 9</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C2PSA</span>, [<span class=\"number\">1024</span>]] <span class=\"comment\"># 10</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># YOLO11n head</span></span><br><span class=\"line\"><span class=\"attr\">head:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">nn.Upsample</span>, [<span class=\"string\">None</span>, <span class=\"number\">2</span>, <span class=\"string\">&quot;nearest&quot;</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">-1</span>, <span class=\"number\">6</span>], <span class=\"number\">1</span>, <span class=\"string\">Concat</span>, [<span class=\"number\">1</span>]] <span class=\"comment\"># cat backbone P4</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">512</span>, <span class=\"literal\">False</span>]] <span class=\"comment\"># 13</span></span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">nn.Upsample</span>, [<span class=\"string\">None</span>, <span class=\"number\">2</span>, <span class=\"string\">&quot;nearest&quot;</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">-1</span>, <span class=\"number\">4</span>], <span class=\"number\">1</span>, <span class=\"string\">Concat</span>, [<span class=\"number\">1</span>]] <span class=\"comment\"># cat backbone P3</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">256</span>, <span class=\"literal\">False</span>]] <span class=\"comment\"># 16 (P3/8-small)</span></span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">256</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">-1</span>, <span class=\"number\">13</span>], <span class=\"number\">1</span>, <span class=\"string\">Concat</span>, [<span class=\"number\">1</span>]] <span class=\"comment\"># cat head P4</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">512</span>, <span class=\"literal\">False</span>]] <span class=\"comment\"># 19 (P4/16-medium)</span></span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">1</span>, <span class=\"string\">Conv</span>, [<span class=\"number\">512</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]]</span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">-1</span>, <span class=\"number\">10</span>], <span class=\"number\">1</span>, <span class=\"string\">Concat</span>, [<span class=\"number\">1</span>]] <span class=\"comment\"># cat head P5</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> [<span class=\"number\">-1</span>, <span class=\"number\">2</span>, <span class=\"string\">C3k2</span>, [<span class=\"number\">1024</span>, <span class=\"literal\">True</span>]] <span class=\"comment\"># 22 (P5/32-large)</span></span><br><span class=\"line\"> </span><br><span class=\"line\">  <span class=\"bullet\">-</span> [[<span class=\"number\">16</span>, <span class=\"number\">19</span>, <span class=\"number\">22</span>], <span class=\"number\">1</span>, <span class=\"string\">Detect</span>, [<span class=\"string\">nc</span>]] <span class=\"comment\"># Detect(P3, P4, P5)</span></span><br></pre></td></tr></table></figure>\n\n<p>架构级别的变化：</p>\n<h3 id=\"1-骨干\"><a href=\"#1-骨干\" class=\"headerlink\" title=\"1. 骨干\"></a><strong>1. 骨干</strong></h3><p>主干是模型的一部分，用于从多个比例的输入图像中提取特征。它通常涉及堆叠卷积层和块以创建不同分辨率的特征图。</p>\n<p><strong>卷积层：</strong>YOLO11 具有类似的结构，带有初始卷积层来对图像进行下采样：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- [-1, 1, Conv, [64, 3, 2]] # 0-P1/2</span><br><span class=\"line\">- [-1, 1, Conv, [128, 3, 2]] # 1-P2/4</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><strong>C3k2 区块：</strong>YOLO11 引入了 <strong>C3k2 块，而不是 C2f</strong>，它在计算方面效率更高。此块是 <strong>CSP 瓶颈</strong>的自定义实现，它使用两个卷积，而不是一个大型卷积（如 YOLOv8 中所示）。</p>\n<ul>\n<li><strong>CSP （Cross Stage Partial）：</strong>CSP 网络拆分特征图并通过瓶颈层处理一部分，同时将另一部分与瓶颈的输出合并。这减少了计算负载并改善了特征表示。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- [-1, 2, C3k2, [256, False, 0.25]]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>C3k2 块还使用较小的内核大小（由 k2 表示），使其更快，同时保持性能。</li>\n</ul>\n<p><strong>SPPF 和 C2PSA：</strong>YOLO11 保留了 SPPF 块，但在 SPPF 之后添加了一个新的 <strong>C2PSA</strong> 块：</p>\n</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- [-1, 1, SPPF, [1024, 5]]</span><br><span class=\"line\">- [-1, 2, C2PSA, [1024]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><strong>C2PSA （Cross Stage Partial with Spatial Attention）</strong> 模块增强了特征图中的空间注意力，从而提高了模型对图像重要部分的关注。这使模型能够通过在空间上池化特征来更有效地关注特定的感兴趣区域。</li>\n</ul>\n<h3 id=\"2-neck\"><a href=\"#2-neck\" class=\"headerlink\" title=\"2. neck\"></a><strong>2. neck</strong></h3><p>neck 负责聚合来自不同分辨率的特征，并将它们传递给头部进行预测。它通常涉及来自不同级别的特征图的上采样和连接。</p>\n<p><strong>C3k2 区块：</strong>YOLO11 用 <strong>C3k2</strong> 块替换了颈部的 C2f 块。如前所述，C3k2 是一个更快、更高效的区块。例如，在上采样和串联后，YOLO11 中的 neck 如下所示：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t</span><br><span class=\"line\">- [-1, 2, C3k2, [512, False]] # P4/16-medium</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>此更改提高了要素聚合过程的速度和性能。</li>\n<li><strong>注意力机制：</strong>YOLO11 通过 <strong>C2PSA</strong> 更侧重于空间注意力，这有助于模型专注于图像中的关键区域，以便更好地检测。这在 YOLOv8 中是缺失的，这使得 YOLO11 在检测较小或被遮挡的对象时可能更准确。</li>\n</ul>\n<hr>\n<h3 id=\"3-head\"><a href=\"#3-head\" class=\"headerlink\" title=\"3. head\"></a><strong>3. head</strong></h3><p>head 是模型中负责生成最终预测的部分。在对象检测中，这通常意味着生成边界框并对这些框内的对象进行分类。</p>\n<p><strong>C3k2 区块：</strong>与颈部类似，YOLO11 取代了头部的 <strong>C2f</strong> 块。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t</span><br><span class=\"line\">- [-1, 2, C3k2, [512, False]] # P4/16-medium</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p><strong>检测层：</strong>最终的 Detect 层与 YOLOv8 中的层相同：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>使用 C3k2 块使模型在推理方面更快，在参数方面更高效。<br>那么，让我们看看新块（层）在代码中的样子：</p>\n<hr>\n<p>那么，让我们看看新块（层）在代码中的样子：</p>\n<ol>\n<li><strong>C3k2 区块（从</strong> <strong>blocks.py</strong> 开始<strong>）：</strong><ul>\n<li><strong>C3k2</strong> 是 <strong>CSP 瓶颈</strong>的更快、更高效的变体。它使用两个卷积而不是一个大型卷积，从而加快了特征提取速度。</li>\n</ul>\n</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class C3k2(C2f):</span><br><span class=\"line\">    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):</span><br><span class=\"line\">        super().__init__(c1, c2, n, shortcut, g, e)</span><br><span class=\"line\">        self.m = nn.ModuleList(</span><br><span class=\"line\">            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)</span><br><span class=\"line\">        )</span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li><strong>C3k 块（从</strong> <strong>blocks.py</strong> 开始<strong>）</strong>：</li>\n</ol>\n<ul>\n<li><strong>C3k</strong> 是一个更灵活的瓶颈模块，允许自定义内核大小。这对于提取图像中更详细的特征非常有用。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class C3k(C3):</span><br><span class=\"line\">    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):</span><br><span class=\"line\">        super().__init__(c1, c2, n, shortcut, g, e)</span><br><span class=\"line\">        c_ = int(c2 * e)  # hidden channels</span><br><span class=\"line\">        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))</span><br></pre></td></tr></table></figure>\n\n<ol start=\"3\">\n<li><strong>C2PSA 块（从</strong> <strong>blocks.py</strong> 年起<strong>）：</strong></li>\n</ol>\n<ul>\n<li><strong>C2PSA</strong> （Cross Stage Partial with Spatial Attention） 增强了模型的空间注意力能力。此模块增加了对特征图的关注，帮助模型专注于图像的重要区域。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class C2PSA(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, e=0.5):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        c_ = int(c2 * e)</span><br><span class=\"line\">        self.cv1 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv2 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv3 = Conv(2 * c_, c2, 1)</span><br><span class=\"line\">     </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        return self.cv3(torch.cat((self.cv1(x), self.cv2(x)), 1))</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"YOLO11-pipeline\"><a href=\"#YOLO11-pipeline\" class=\"headerlink\" title=\"YOLO11 pipeline\"></a>YOLO11 pipeline</h1><p>在 <a href=\"https://github.com/ultralytics/ultralytics\"><strong>ultralytics</strong></a> GitHub 仓库中，我们将主要关注：</p>\n<ol>\n<li><strong>nn&#x2F;modules&#x2F;</strong> 中的模块<ul>\n<li><strong>block.py</strong></li>\n<li><strong>conv.py</strong></li>\n<li><strong>head.py</strong></li>\n<li><strong>transformer.py</strong></li>\n<li><strong>utils.py</strong></li>\n</ul>\n</li>\n<li><strong>nn&#x2F;tasks.py</strong> 文件</li>\n</ol>\n<h3 id=\"1-代码库概述\"><a href=\"#1-代码库概述\" class=\"headerlink\" title=\"1. 代码库概述\"></a>1. 代码库概述</h3><p>代码库被构建为多个模块，这些模块定义了 YOLO11 模型中使用的各种神经网络组件。这些组件在 nn&#x2F;modules&#x2F; 目录中被组织到不同的文件中：</p>\n<ul>\n<li><strong>block.py</strong>：定义模型中使用的各种构建块（模块），例如瓶颈、CSP 模块和注意力机制。</li>\n<li><strong>conv.py</strong>：包含卷积模块，包括标准卷积、深度卷积和其他变体。</li>\n<li><strong>head.py</strong>：实现负责生成最终预测（例如，边界框、类概率）的模型头。</li>\n<li><strong>transformer.py</strong>：包括基于 transformer 的模块，用于注意力机制和高级特征提取。</li>\n<li><strong>utils.py</strong>：提供跨模块使用的实用程序函数和帮助程序类。</li>\n</ul>\n<p>nn&#x2F;tasks.py 文件定义了不同的特定于任务的模型（例如，检测、分割、分类），这些模型将这些模块组合成完整的架构。</p>\n<h3 id=\"2-nn-modules-中的模块\"><a href=\"#2-nn-modules-中的模块\" class=\"headerlink\" title=\"2. nn&#x2F;modules&#x2F; 中的模块\"></a>2. nn&#x2F;modules&#x2F; 中的模块</h3><p>如前所述，YOLO11 构建在 YOLOv8 代码库之上。因此，我们将主要关注更新的脚本：<strong>block.py</strong>、<strong>conv.py</strong> 和 <strong>head.py</strong> 在这里。</p>\n<h4 id=\"block-py\"><a href=\"#block-py\" class=\"headerlink\" title=\"block.py\"></a><strong>block.py</strong></h4><p>此文件定义 YOLO11 模型中使用的各种构建块。这些块是构成神经网络层的基本组件。</p>\n<h5 id=\"关键组件：\"><a href=\"#关键组件：\" class=\"headerlink\" title=\"关键组件：\"></a><strong>关键组件：</strong></h5><ol>\n<li>瓶颈模块：<ul>\n<li><strong>Bottleneck</strong>：具有可选快捷方式连接的标准瓶颈模块。</li>\n<li><strong>Res</strong>：使用一系列卷积和身份快捷方式的残差块。</li>\n</ul>\n</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Bottleneck(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        c_ = int(c2 * e)</span><br><span class=\"line\">        self.cv1 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv2 = Conv(c_, c2, 3, 1, g=g)</span><br><span class=\"line\">        self.add = shortcut and c1 == c2</span><br><span class=\"line\"> </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>Bottleneck 类实现了一个 bottleneck 模块，该模块减少了通道的数量（降维），然后再次扩展它们。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.cv1：一个 1×1 卷积，用于减少通道数。</li>\n<li>self.cv2：一个 3×3 卷积，用于将通道数增加回原始通道数。</li>\n<li>self.add：一个布尔值，指示是否添加快捷方式连接。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：输入 x 通过 cv1 和 cv2 传递。如果 self.add 为 True，则原始输入 x 将添加到输出（残差连接）。</li>\n</ul>\n<ol start=\"2\">\n<li>CSP （Cross Stage Partial） 模块：</li>\n</ol>\n<ul>\n<li><strong>BottleneckCSP：</strong>瓶颈模块的 CSP 版本。</li>\n<li><strong>CSPBlock</strong>：具有多个瓶颈层的更复杂的 CSP 模块。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class BottleneckCSP(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        c_ = int(c2 * e)</span><br><span class=\"line\">        self.cv1 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv2 = nn.Sequential(</span><br><span class=\"line\">            *[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)]</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.cv3 = Conv(2 * c_, c2, 1)</span><br><span class=\"line\">        self.add = c1 == c2</span><br><span class=\"line\"> </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        y1 = self.cv2(self.cv1(x))</span><br><span class=\"line\">        y2 = x if self.add else None</span><br><span class=\"line\">        return self.cv3(torch.cat((y1, y2), 1)) if y2 is not None else self.cv3(y1)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>CSPBottleneck 模块将特征图分为两部分。一部分通过一系列瓶颈层，另一部分直接连接到输出，从而降低了计算成本并增强了梯度流。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.cv1：减少通道数。</li>\n<li>self.cv2：瓶颈层序列。</li>\n<li>self.cv3：合并功能并调整通道数。</li>\n<li>self.add：确定是否添加快捷方式连接。</li>\n</ul>\n</li>\n</ul>\n<ol start=\"3\">\n<li>其他模块：</li>\n</ol>\n<ul>\n<li><strong>SPPF：</strong>Spatial Pyramid Pooling Fast 模块，可在多个比例下执行池化。</li>\n<li><strong>Concat</strong>：沿指定维度连接多个 Tensor。</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class SPPF(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, k=5):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        c_ = c1 // 2</span><br><span class=\"line\">        self.cv1 = Conv(c1, c_, 1, 1)</span><br><span class=\"line\">        self.cv2 = Conv(c_ * 4, c2, 1, 1)</span><br><span class=\"line\">        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)</span><br><span class=\"line\"> </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        x = self.cv1(x)</span><br><span class=\"line\">        y1 = self.m(x)</span><br><span class=\"line\">        y2 = self.m(y1)</span><br><span class=\"line\">        y3 = self.m(y2)</span><br><span class=\"line\">        return self.cv2(torch.cat([x, y1, y2, y3], 1))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>SPPF 模块在不同比例下执行最大池化，并将结果连接起来以捕获多个空间比例的要素。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.cv1：减少通道数。</li>\n<li>self.cv2：调整拼接后的 Channel 数。</li>\n<li>self.m：最大池化层数。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：输入 x 通过 cv1，然后通过三个连续的最大池化层（y1、y2、y3）。结果被连接并通过 cv2 传递。</li>\n</ul>\n<h5 id=\"了解概念：\"><a href=\"#了解概念：\" class=\"headerlink\" title=\"了解概念：\"></a><strong>了解概念：</strong></h5><ul>\n<li><strong>瓶颈层</strong>：用于通过在昂贵的操作之前减少通道数并在之后增加通道数来降低计算复杂性。</li>\n<li><strong>残差连接</strong>：通过缓解梯度消失问题来帮助训练更深的网络。</li>\n<li><strong>CSP 架构</strong>：将特征图分为两部分;一部分发生转换，而另一部分保持不变，从而提高学习能力并减少计算。</li>\n</ul>\n<h4 id=\"conv-py\"><a href=\"#conv-py\" class=\"headerlink\" title=\"conv.py\"></a><strong>conv.py</strong></h4><p>此文件包含各种卷积模块，包括标准卷积和专用卷积。</p>\n<h5 id=\"关键组件：-1\"><a href=\"#关键组件：-1\" class=\"headerlink\" title=\"关键组件：\"></a><strong>关键组件：</strong></h5><p><strong>标准卷积模块 （Conv）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Conv(nn.Module):</span><br><span class=\"line\">    default_act = nn.SiLU()  # default activation</span><br><span class=\"line\"> </span><br><span class=\"line\">    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)</span><br><span class=\"line\">        self.bn = nn.BatchNorm2d(c2)</span><br><span class=\"line\">        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()</span><br><span class=\"line\"> </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        return self.act(self.bn(self.conv(x)))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>实现具有批量规范化和激活的标准卷积层。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.conv：卷积层。</li>\n<li>self.bn：批量规范化。</li>\n<li>self.act：激活函数（默认为 nn.SiLU（））的</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：应用卷积，然后进行批量规范化和激活。</li>\n</ul>\n<p><strong>深度卷积 （DWConv）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class DWConv(Conv):</span><br><span class=\"line\">    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):</span><br><span class=\"line\">        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>执行深度卷积，其中每个输入通道单独卷积。</li>\n<li><strong>组件</strong>：<ul>\n<li>继承自 Conv。</li>\n<li>将 groups 参数设置为 c1 和 c2 的最大公约数，从而有效地对每个通道的卷积进行分组。</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>其他卷积模块：<ul>\n<li><strong>Conv2</strong>：RepConv 的简化版本，用于模型压缩和加速。</li>\n<li><strong>GhostConv</strong>：实现 GhostNet 的 ghost 模块，减少特性图中的冗余。</li>\n<li><strong>RepConv</strong>：可重新参数化的卷积层，可以从训练模式转换为推理模式。</li>\n</ul>\n</li>\n</ol>\n<h5 id=\"了解概念：-1\"><a href=\"#了解概念：-1\" class=\"headerlink\" title=\"了解概念：\"></a><strong>了解概念：</strong></h5><ul>\n<li><strong>自动填充 （<strong><strong>autopad</strong></strong>）：</strong>自动计算保持输出尺寸一致所需的填充。</li>\n<li><strong>深度卷积和点卷积</strong>：用于 MobileNet 架构，以减少计算，同时保持准确性。</li>\n<li><strong>重新参数化</strong>：RepConv 等技术通过合并层来实现高效的训练和更快的推理。</li>\n</ul>\n<h4 id=\"head-py\"><a href=\"#head-py\" class=\"headerlink\" title=\"head.py\"></a><strong>head.py</strong></h4><p>此文件实现了负责生成模型最终预测的 head 模块。</p>\n<h5 id=\"关键组件：-2\"><a href=\"#关键组件：-2\" class=\"headerlink\" title=\"关键组件：\"></a><strong>关键组件：</strong></h5><p><strong>检测头 （Detect）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Detect(nn.Module):</span><br><span class=\"line\">    def __init__(self, nc=80, ch=()):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.nc = nc  # number of classes</span><br><span class=\"line\">        self.nl = len(ch)  # number of detection layers</span><br><span class=\"line\">        self.reg_max = 16  # DFL channels</span><br><span class=\"line\">        self.no = nc + self.reg_max * 4  # number of outputs per anchor</span><br><span class=\"line\">        self.stride = torch.zeros(self.nl)  # strides computed during build</span><br><span class=\"line\"> </span><br><span class=\"line\">        # Define layers</span><br><span class=\"line\">        self.cv2 = nn.ModuleList(</span><br><span class=\"line\">            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.cv3 = nn.ModuleList(</span><br><span class=\"line\">            nn.Sequential(</span><br><span class=\"line\">                nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),</span><br><span class=\"line\">                nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),</span><br><span class=\"line\">                nn.Conv2d(c3, self.nc, 1),</span><br><span class=\"line\">            )</span><br><span class=\"line\">            for x in ch</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.dfl = DFL(self.reg_max) if self.reg_max &gt; 1 else nn.Identity()</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>Detect 类定义输出边界框坐标和类概率的检测头。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.cv2：用于边界框回归的卷积层。</li>\n<li>self.cv3：用于分类的卷积层。</li>\n<li>self.dfl：用于边界框细化的 Distribution Focal Loss 模块。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：处理输入特征映射并输出边界框和类的预测。</li>\n</ul>\n<p><strong>分割 （<strong><strong>Segment</strong></strong>）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Segment(Detect):</span><br><span class=\"line\">    def __init__(self, nc=80, nm=32, npr=256, ch=()):</span><br><span class=\"line\">        super().__init__(nc, ch)</span><br><span class=\"line\">        self.nm = nm  # number of masks</span><br><span class=\"line\">        self.npr = npr  # number of prototypes</span><br><span class=\"line\">        self.proto = Proto(ch[0], self.npr, self.nm)  # protos</span><br><span class=\"line\"> </span><br><span class=\"line\">        c4 = max(ch[0] // 4, self.nm)</span><br><span class=\"line\">        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>扩展 Detect 类以包含分段功能。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.proto：生成掩码原型。</li>\n<li>self.cv4：掩码系数的卷积层。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：输出边界框、类概率和掩码系数。</li>\n</ul>\n<p><strong>姿势估计头部 （Pose）：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Pose(Detect):</span><br><span class=\"line\">    def __init__(self, nc=80, kpt_shape=(17, 3), ch=()):</span><br><span class=\"line\">        super().__init__(nc, ch)</span><br><span class=\"line\">        self.kpt_shape = kpt_shape  # number of keypoints, number of dimensions</span><br><span class=\"line\">        self.nk = kpt_shape[0] * kpt_shape[1]  # total number of keypoint outputs</span><br><span class=\"line\"> </span><br><span class=\"line\">        c4 = max(ch[0] // 4, self.nk)</span><br><span class=\"line\">        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>扩展了 Detect 类，用于人体姿势估计任务。</li>\n<li><strong>组件</strong>：<ul>\n<li>self.kpt_shape：关键点的形状（关键点的数量、每个关键点的维度）。</li>\n<li>self.cv4：用于关键点回归的卷积层。</li>\n</ul>\n</li>\n<li><strong>Forward Pass</strong>：输出边界框、类概率和关键点坐标。</li>\n</ul>\n<h5 id=\"了解概念：-2\"><a href=\"#了解概念：-2\" class=\"headerlink\" title=\"了解概念：\"></a><strong>了解概念：</strong></h5><ul>\n<li><strong>模块化</strong>：通过扩展 Detect 类，我们可以为不同的任务创建专门的 head，同时重用通用功能。</li>\n<li><strong>无锚点检测</strong>：现代对象检测器通常使用无锚点方法，直接预测边界框。</li>\n<li><strong>关键点估计</strong>：在姿势估计中，模型预测表示关节或地标的关键点。</li>\n</ul>\n<h3 id=\"3-nn-tasks-py-文件\"><a href=\"#3-nn-tasks-py-文件\" class=\"headerlink\" title=\"3. nn&#x2F;tasks.py 文件\"></a>3. <strong>nn&#x2F;tasks.py</strong> 文件</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Ultralytics YOLO &lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;🚀&quot; src=&quot;https://s.w.org/images/core/emoji/15.0.3/svg/1f680.svg&quot;&gt;, AGPL-3.0 license</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> contextlib</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">import</span> types</span><br><span class=\"line\"><span class=\"keyword\">from</span> copy <span class=\"keyword\">import</span> deepcopy</span><br><span class=\"line\"><span class=\"keyword\">from</span> pathlib <span class=\"keyword\">import</span> Path</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># Other imports...</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BaseModel</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, *args, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Handles both training and inference, returns predictions or loss.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(x, <span class=\"built_in\">dict</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.loss(x, *args, **kwargs)  <span class=\"comment\"># Training: return loss</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.predict(x, *args, **kwargs)  <span class=\"comment\"># Inference: return predictions</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x, profile=<span class=\"literal\">False</span>, visualize=<span class=\"literal\">False</span>, augment=<span class=\"literal\">False</span>, embed=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Run a forward pass through the network for inference.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> augment:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>._predict_augment(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>._predict_once(x, profile, visualize, embed)</span><br><span class=\"line\">     </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fuse</span>(<span class=\"params\">self, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Fuses Conv and BatchNorm layers for efficiency during inference.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.model.modules():</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, (Conv, Conv2, DWConv)) <span class=\"keyword\">and</span> <span class=\"built_in\">hasattr</span>(m, <span class=\"string\">&quot;bn&quot;</span>):</span><br><span class=\"line\">                m.conv = fuse_conv_and_bn(m.conv, m.bn)</span><br><span class=\"line\">                <span class=\"built_in\">delattr</span>(m, <span class=\"string\">&quot;bn&quot;</span>)</span><br><span class=\"line\">                m.forward = m.forward_fuse  <span class=\"comment\"># Use the fused forward</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span></span><br><span class=\"line\">     </span><br><span class=\"line\">    <span class=\"comment\"># More BaseModel methods...</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DetectionModel</span>(<span class=\"title class_ inherited__\">BaseModel</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;YOLOv8 detection model.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, cfg=<span class=\"string\">&quot;yolov8n.yaml&quot;</span>, ch=<span class=\"number\">3</span>, nc=<span class=\"literal\">None</span>, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the YOLOv8 detection model with config and parameters.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.yaml = cfg <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(cfg, <span class=\"built_in\">dict</span>) <span class=\"keyword\">else</span> yaml_model_load(cfg)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.model, <span class=\"variable language_\">self</span>.save = parse_model(deepcopy(<span class=\"variable language_\">self</span>.yaml), ch=ch, verbose=verbose)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.names = &#123;i: <span class=\"string\">f&quot;<span class=\"subst\">&#123;i&#125;</span>&quot;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"variable language_\">self</span>.yaml[<span class=\"string\">&quot;nc&quot;</span>])&#125;  <span class=\"comment\"># Class names</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.inplace = <span class=\"variable language_\">self</span>.yaml.get(<span class=\"string\">&quot;inplace&quot;</span>, <span class=\"literal\">True</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"comment\"># Initialize strides</span></span><br><span class=\"line\">        m = <span class=\"variable language_\">self</span>.model[-<span class=\"number\">1</span>]  <span class=\"comment\"># Detect() layer</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, Detect):</span><br><span class=\"line\">            s = <span class=\"number\">256</span>  <span class=\"comment\"># Max stride</span></span><br><span class=\"line\">            m.stride = torch.tensor([s / x.shape[-<span class=\"number\">2</span>] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>._predict_once(torch.zeros(<span class=\"number\">1</span>, ch, s, s))])</span><br><span class=\"line\">            <span class=\"variable language_\">self</span>.stride = m.stride</span><br><span class=\"line\">            m.bias_init()  <span class=\"comment\"># Initialize biases</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># More DetectionModel methods...</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SegmentationModel</span>(<span class=\"title class_ inherited__\">DetectionModel</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;YOLOv8 segmentation model.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, cfg=<span class=\"string\">&quot;yolov8n-seg.yaml&quot;</span>, ch=<span class=\"number\">3</span>, nc=<span class=\"literal\">None</span>, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize YOLOv8 segmentation model with given config and parameters.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_criterion</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the loss criterion for the SegmentationModel.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> v8SegmentationLoss(<span class=\"variable language_\">self</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PoseModel</span>(<span class=\"title class_ inherited__\">DetectionModel</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;YOLOv8 pose model.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, cfg=<span class=\"string\">&quot;yolov8n-pose.yaml&quot;</span>, ch=<span class=\"number\">3</span>, nc=<span class=\"literal\">None</span>, data_kpt_shape=(<span class=\"params\"><span class=\"literal\">None</span>, <span class=\"literal\">None</span></span>), verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize YOLOv8 Pose model.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(cfg, <span class=\"built_in\">dict</span>):</span><br><span class=\"line\">            cfg = yaml_model_load(cfg)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">list</span>(data_kpt_shape) != <span class=\"built_in\">list</span>(cfg[<span class=\"string\">&quot;kpt_shape&quot;</span>]):</span><br><span class=\"line\">            cfg[<span class=\"string\">&quot;kpt_shape&quot;</span>] = data_kpt_shape</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_criterion</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the loss criterion for the PoseModel.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> v8PoseLoss(<span class=\"variable language_\">self</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ClassificationModel</span>(<span class=\"title class_ inherited__\">BaseModel</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;YOLOv8 classification model.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, cfg=<span class=\"string\">&quot;yolov8n-cls.yaml&quot;</span>, ch=<span class=\"number\">3</span>, nc=<span class=\"literal\">None</span>, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the YOLOv8 classification model.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>._from_yaml(cfg, ch, nc, verbose)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_from_yaml</span>(<span class=\"params\">self, cfg, ch, nc, verbose</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Set YOLOv8 model configurations and define the model architecture.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.yaml = cfg <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(cfg, <span class=\"built_in\">dict</span>) <span class=\"keyword\">else</span> yaml_model_load(cfg)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.model, <span class=\"variable language_\">self</span>.save = parse_model(deepcopy(<span class=\"variable language_\">self</span>.yaml), ch=ch, verbose=verbose)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.names = &#123;i: <span class=\"string\">f&quot;<span class=\"subst\">&#123;i&#125;</span>&quot;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"variable language_\">self</span>.yaml[<span class=\"string\">&quot;nc&quot;</span>])&#125;</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.info()</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reshape_outputs</span>(<span class=\"params\">model, nc</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Update a classification model to match the class count (nc).&quot;&quot;&quot;</span></span><br><span class=\"line\">        name, m = <span class=\"built_in\">list</span>((model.model <span class=\"keyword\">if</span> <span class=\"built_in\">hasattr</span>(model, <span class=\"string\">&quot;model&quot;</span>) <span class=\"keyword\">else</span> model).named_children())[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.Linear):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> m.out_features != nc:</span><br><span class=\"line\">                <span class=\"built_in\">setattr</span>(model, name, nn.Linear(m.in_features, nc))</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">init_criterion</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize the loss criterion for the ClassificationModel.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> v8ClassificationLoss()</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Ensemble</span>(nn.ModuleList):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Ensemble of models.&quot;&quot;&quot;</span></span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize an ensemble of models.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, augment=<span class=\"literal\">False</span>, profile=<span class=\"literal\">False</span>, visualize=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Generate the ensemble’s final layer by combining outputs from each model.&quot;&quot;&quot;</span></span><br><span class=\"line\">        y = [module(x, augment, profile, visualize)[<span class=\"number\">0</span>] <span class=\"keyword\">for</span> module <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.cat(y, <span class=\"number\">2</span>), <span class=\"literal\">None</span>  <span class=\"comment\"># Concatenate outputs along the third dimension</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># Functions ------------------------------------------------------------------------------------------------------------</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">parse_model</span>(<span class=\"params\">d, ch, verbose=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Parse a YOLO model.yaml dictionary into a PyTorch model.&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> ast</span><br><span class=\"line\"> </span><br><span class=\"line\">    max_channels = <span class=\"built_in\">float</span>(<span class=\"string\">&quot;inf&quot;</span>)</span><br><span class=\"line\">    nc, act, scales = (d.get(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> (<span class=\"string\">&quot;nc&quot;</span>, <span class=\"string\">&quot;activation&quot;</span>, <span class=\"string\">&quot;scales&quot;</span>))</span><br><span class=\"line\">    depth, width, kpt_shape = (d.get(x, <span class=\"number\">1.0</span>) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> (<span class=\"string\">&quot;depth_multiple&quot;</span>, <span class=\"string\">&quot;width_multiple&quot;</span>, <span class=\"string\">&quot;kpt_shape&quot;</span>))</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Model scaling</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> scales:</span><br><span class=\"line\">        scale = d.get(<span class=\"string\">&quot;scale&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> scale:networ</span><br><span class=\"line\">            scale = <span class=\"built_in\">tuple</span>(scales.keys())[<span class=\"number\">0</span>]</span><br><span class=\"line\">            LOGGER.warning(<span class=\"string\">f&quot;WARNING &lt;img draggable=&quot;</span>false<span class=\"string\">&quot; role=&quot;</span>img<span class=\"string\">&quot; class=&quot;</span>emoji<span class=\"string\">&quot; alt=&quot;</span>⚠️<span class=\"string\">&quot; src=&quot;</span>https://s.w.org/images/core/emoji/<span class=\"number\">15.0</span><span class=\"number\">.3</span>/svg/26a0.svg<span class=\"string\">&quot;&gt; no model scale passed. Assuming scale=&#x27;&#123;scale&#125;&#x27;.&quot;</span>)</span><br><span class=\"line\">        depth, width, max_channels = scales[scale]</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">if</span> act:</span><br><span class=\"line\">        Conv.default_act = <span class=\"built_in\">eval</span>(act)  <span class=\"comment\"># redefine default activation</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> verbose:</span><br><span class=\"line\">            LOGGER.info(<span class=\"string\">f&quot;Activation: <span class=\"subst\">&#123;act&#125;</span>&quot;</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"comment\"># Logging and parsing layers</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> verbose:</span><br><span class=\"line\">        LOGGER.info(<span class=\"string\">f&quot;\\n<span class=\"subst\">&#123;<span class=\"string\">&#x27;&#x27;</span>:&gt;<span class=\"number\">3</span>&#125;</span><span class=\"subst\">&#123;<span class=\"string\">&#x27;from&#x27;</span>:&gt;<span class=\"number\">20</span>&#125;</span><span class=\"subst\">&#123;<span class=\"string\">&#x27;n&#x27;</span>:&gt;<span class=\"number\">3</span>&#125;</span><span class=\"subst\">&#123;<span class=\"string\">&#x27;params&#x27;</span>:&gt;<span class=\"number\">10</span>&#125;</span>  <span class=\"subst\">&#123;<span class=\"string\">&#x27;module&#x27;</span>:&lt;<span class=\"number\">45</span>&#125;</span><span class=\"subst\">&#123;<span class=\"string\">&#x27;arguments&#x27;</span>:&lt;<span class=\"number\">30</span>&#125;</span>&quot;</span>)</span><br><span class=\"line\">    ch = [ch]</span><br><span class=\"line\">    layers, save, c2 = [], [], ch[-<span class=\"number\">1</span>]</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (f, n, m, args) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(d[<span class=\"string\">&quot;backbone&quot;</span>] + d[<span class=\"string\">&quot;head&quot;</span>]):  <span class=\"comment\"># from, number, module, args</span></span><br><span class=\"line\">        m = <span class=\"built_in\">globals</span>()[m] <span class=\"keyword\">if</span> m <span class=\"keyword\">in</span> <span class=\"built_in\">globals</span>() <span class=\"keyword\">else</span> <span class=\"built_in\">getattr</span>(nn, m[<span class=\"number\">3</span>:], m)  <span class=\"comment\"># get module</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j, a <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(args):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(a, <span class=\"built_in\">str</span>):</span><br><span class=\"line\">                <span class=\"keyword\">with</span> contextlib.suppress(ValueError):</span><br><span class=\"line\">                    args[j] = ast.literal_eval(a) <span class=\"keyword\">if</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">locals</span>() <span class=\"keyword\">else</span> a</span><br><span class=\"line\"> </span><br><span class=\"line\">        n = <span class=\"built_in\">max</span>(<span class=\"built_in\">round</span>(n * depth), <span class=\"number\">1</span>) <span class=\"keyword\">if</span> n &gt; <span class=\"number\">1</span> <span class=\"keyword\">else</span> n  <span class=\"comment\"># depth gain</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> m <span class=\"keyword\">in</span> &#123;Conv, Bottleneck, C2f, C3k2, ...&#125;:  <span class=\"comment\"># Module list</span></span><br><span class=\"line\">            c1, c2 = ch[f], args[<span class=\"number\">0</span>]</span><br><span class=\"line\">            c2 = make_divisible(<span class=\"built_in\">min</span>(c2, max_channels) * width, <span class=\"number\">8</span>)</span><br><span class=\"line\">            args = [c1, c2, *args[<span class=\"number\">1</span>:]]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> m <span class=\"keyword\">in</span> &#123;C2f, C3k2, ...&#125;:  <span class=\"comment\"># Repeated layers</span></span><br><span class=\"line\">                args.insert(<span class=\"number\">2</span>, n)</span><br><span class=\"line\">                n = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> m <span class=\"keyword\">in</span> &#123;Concat, Detect, ...&#125;:  <span class=\"comment\"># Head layers</span></span><br><span class=\"line\">            args.append([ch[x] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> f])</span><br><span class=\"line\">        <span class=\"comment\"># Append layers</span></span><br><span class=\"line\">        m_ = nn.Sequential(*(m(*args) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n))) <span class=\"keyword\">if</span> n &gt; <span class=\"number\">1</span> <span class=\"keyword\">else</span> m(*args)</span><br><span class=\"line\">        layers.append(m_)</span><br><span class=\"line\"> </span><br><span class=\"line\">        ch.append(c2)</span><br><span class=\"line\">        save.extend([x % i <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> ([f] <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(f, <span class=\"built_in\">int</span>) <span class=\"keyword\">else</span> f) <span class=\"keyword\">if</span> x != -<span class=\"number\">1</span>])</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"keyword\">if</span> verbose:</span><br><span class=\"line\">            LOGGER.info(<span class=\"string\">f&quot;<span class=\"subst\">&#123;i:&gt;<span class=\"number\">3</span>&#125;</span><span class=\"subst\">&#123;<span class=\"built_in\">str</span>(f):&gt;<span class=\"number\">20</span>&#125;</span><span class=\"subst\">&#123;n:&gt;<span class=\"number\">3</span>&#125;</span><span class=\"subst\">&#123;<span class=\"built_in\">sum</span>(x.numel() <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> m_.parameters()):<span class=\"number\">10.0</span>f&#125;</span>  <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(m):&lt;<span class=\"number\">45</span>&#125;</span><span class=\"subst\">&#123;<span class=\"built_in\">str</span>(args):&lt;<span class=\"number\">30</span>&#125;</span>&quot;</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(*layers), <span class=\"built_in\">sorted</span>(save)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">yaml_model_load</span>(<span class=\"params\">path</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Load a YOLO model from a YAML file.&quot;&quot;&quot;</span></span><br><span class=\"line\">    path = Path(path)</span><br><span class=\"line\">    unified_path = path.with_name(path.stem.replace(<span class=\"string\">&quot;yolov8&quot;</span>, <span class=\"string\">&quot;yolov&quot;</span>))</span><br><span class=\"line\">    yaml_file = check_yaml(<span class=\"built_in\">str</span>(unified_path), hard=<span class=\"literal\">False</span>) <span class=\"keyword\">or</span> check_yaml(path)</span><br><span class=\"line\">    d = yaml_load(yaml_file)</span><br><span class=\"line\">    d[<span class=\"string\">&quot;scale&quot;</span>] = guess_model_scale(path)</span><br><span class=\"line\">    d[<span class=\"string\">&quot;yaml_file&quot;</span>] = <span class=\"built_in\">str</span>(path)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> d</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># More utility functions...</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">guess_model_scale</span>(<span class=\"params\">model_path</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Extract the scale from the YAML file.&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> re</span><br><span class=\"line\">    <span class=\"keyword\">return</span> re.search(<span class=\"string\">r&quot;yolov\\d+([nslmx])&quot;</span>, Path(model_path).stem).group(<span class=\"number\">1</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attempt_load_weights</span>(<span class=\"params\">weights, device=<span class=\"literal\">None</span>, inplace=<span class=\"literal\">True</span>, fuse=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Loads weights for a model or an ensemble of models.&quot;&quot;&quot;</span></span><br><span class=\"line\">    ensemble = Ensemble()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> w <span class=\"keyword\">in</span> weights <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(weights, <span class=\"built_in\">list</span>) <span class=\"keyword\">else</span> [weights]:</span><br><span class=\"line\">        ckpt, _ = torch_safe_load(w)</span><br><span class=\"line\">        model = (ckpt.get(<span class=\"string\">&quot;ema&quot;</span>) <span class=\"keyword\">or</span> ckpt[<span class=\"string\">&quot;model&quot;</span>]).to(device).<span class=\"built_in\">float</span>()</span><br><span class=\"line\">        model = model.fuse().<span class=\"built_in\">eval</span>() <span class=\"keyword\">if</span> fuse <span class=\"keyword\">and</span> <span class=\"built_in\">hasattr</span>(model, <span class=\"string\">&quot;fuse&quot;</span>) <span class=\"keyword\">else</span> model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">        ensemble.append(model)</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> ensemble <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(ensemble) &gt; <span class=\"number\">1</span> <span class=\"keyword\">else</span> ensemble[-<span class=\"number\">1</span>]</span><br><span class=\"line\"> </span><br></pre></td></tr></table></figure>\n\n<p>此 tasks.py 脚本是代码管道的核心部分;它仍然使用 YOLOv8 方法和逻辑;我们只需要将 YOLO11 模型解析到其中。此脚本专为各种计算机视觉任务而设计，例如对象检测、分割、分类、姿势估计、OBB 等。它定义了用于训练、推理和模型管理的基础模型、特定于任务的模型和效用函数。</p>\n<h4 id=\"关键组件：-3\"><a href=\"#关键组件：-3\" class=\"headerlink\" title=\"关键组件：\"></a><strong>关键组件：</strong></h4><ul>\n<li><strong>Imports：</strong>该脚本从 Ultralytics 导入 PyTorch （torch）、神经网络层 （torch.nn） 和实用函数等基本模块。一些关键导入包括：<ul>\n<li>对 <strong>C3k2</strong>、<strong>C2PSA</strong>、<strong>C3</strong>、<strong>SPPF、****Concat</strong> 等架构模块进行建模。</li>\n<li>损失函数，如 <strong>v8DetectionLoss</strong>、<strong>v8SegmentationLoss</strong>、<strong>v8ClassificationLoss</strong>、<strong>v8OBBLoss</strong>。</li>\n<li>各种实用程序函数，如 model_info、<strong>fuse_conv_and_bn</strong>、<strong>scale_img</strong> <strong>time_sync</strong>，以帮助进行模型处理、分析和评估。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"模型基类：\"><a href=\"#模型基类：\" class=\"headerlink\" title=\"模型基类：\"></a><strong>模型基类：</strong></h4><ol>\n<li>BaseModel 类：<ul>\n<li>BaseModel 用作 Ultralytics YOLO 系列中所有模型的基类。</li>\n<li>实现如下基本方法：<ul>\n<li><strong>forward（）：</strong>根据输入数据处理训练和推理。</li>\n<li><strong>predict（）：</strong>处理前向传递以进行推理。</li>\n<li><strong>fuse（）：</strong>融合 Conv2d 和 BatchNorm2d 层以提高效率。</li>\n<li><strong>info（）：</strong>提供详细的模型信息。</li>\n</ul>\n</li>\n<li>此类旨在通过特定于任务的模型（例如检测、分割和分类）进行扩展。</li>\n</ul>\n</li>\n<li><strong>DetectionModel</strong> <strong>类：</strong><ul>\n<li>扩展 BaseModel，专门用于对象检测任务。</li>\n<li>加载模型配置，初始化检测头（如 Detect 模块）并设置模型步幅。</li>\n<li>它支持使用 YOLOv8 等架构的检测任务，并可以通过 <strong>_predict_augment（）</strong> 执行增强推理。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"特定于任务的模型：\"><a href=\"#特定于任务的模型：\" class=\"headerlink\" title=\"特定于任务的模型：\"></a><strong>特定于任务的模型：</strong></h4><ol>\n<li><strong>SegmentationModel 的 SegmentationModel</strong> <strong>中：</strong><ul>\n<li>专门用于分割任务（如 YOLOv8 分割）的 DetectionModel 的子类。</li>\n<li>初始化特定于分割的损失函数 （v8SegmentationLoss）。</li>\n</ul>\n</li>\n<li><strong>PoseModel 的 PoseModel</strong> <strong>中：</strong><ul>\n<li>通过初始化具有关键点检测 （<strong>kpt_shape</strong>） 特定配置的模型来处理姿态估计任务。</li>\n<li>使用 v8PoseLoss 进行特定于姿势的损失计算。</li>\n</ul>\n</li>\n<li><strong>分类型号****：</strong><ul>\n<li>专为使用 YOLOv8 分类架构的图像分类任务而设计。</li>\n<li>初始化和管理特定于分类的损失 （<strong>v8ClassificationLoss</strong>）。</li>\n<li>它还支持重塑用于分类任务的预训练 TorchVision 模型。</li>\n</ul>\n</li>\n<li><strong>OBB型号****：</strong><ul>\n<li>用于定向边界框 （OBB） 检测任务。</li>\n<li>实现特定的损失函数 （<strong>v8OBBLoss</strong>） 来处理旋转的边界框。</li>\n</ul>\n</li>\n<li><strong>世界模型****：</strong><ul>\n<li>此模型处理图像字幕和基于文本的识别等任务。</li>\n<li>利用 CLIP 模型中的文本特征执行基于文本的视觉识别任务。</li>\n<li>包括对文本嵌入 （<strong>txt_feats</strong>） 的特殊处理，用于字幕和世界相关任务。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"集成模型：\"><a href=\"#集成模型：\" class=\"headerlink\" title=\"集成模型：\"></a><strong>集成模型：</strong></h4><ol>\n<li><strong>集成****：</strong><ul>\n<li>一个简单的 ensemble 类，它将多个模型合并为一个模型。</li>\n<li>允许对不同模型的输出进行平均或串联，以提高整体性能。</li>\n<li>对于组合多个模型的输出提供更好的预测的任务非常有用。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"实用功能：\"><a href=\"#实用功能：\" class=\"headerlink\" title=\"实用功能：\"></a><strong>实用功能：</strong></h4><ol>\n<li>模型加载和管理：<ul>\n<li><strong>attempt_load_weights（）、****attempt_load_one_weight（）</strong>：用于加载模型、管理集成模型以及处理加载预训练权重时的兼容性问题的函数。</li>\n<li>这些功能可确保以适当的步幅、层和配置正确加载模型。</li>\n</ul>\n</li>\n<li>临时模块重定向：<ul>\n<li><strong>temporary_modules（）</strong>：一个上下文管理器，用于临时重定向模块路径，确保在模块位置更改时向后兼容。</li>\n<li>有助于保持与旧型号版本的兼容性。</li>\n</ul>\n</li>\n<li><strong>Pickle</strong>安全处理：<ul>\n<li>SafeUnpickler：一个自定义的解封器，可以安全地加载模型检查点，确保未知类被安全的占位符（SafeClass）替换，以避免在加载过程中发生崩溃。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"模型解析：\"><a href=\"#模型解析：\" class=\"headerlink\" title=\"模型解析：\"></a><strong>模型解析：</strong></h4><ol>\n<li><strong>parse_model（）</strong> <strong>中：</strong><ul>\n<li>此函数将 YAML 文件中的模型配置解析为 PyTorch 模型。</li>\n<li>它处理主干和头架构，解释每个层类型（如 Conv、SPPF、Detect），并构建最终模型。</li>\n<li>支持各种架构，包括 C3k2、C2PSA 等 YOLO11 组件。</li>\n</ul>\n</li>\n<li>YAML 模型加载：<ul>\n<li><strong>yaml_model_load（）</strong>）：从 YAML 文件加载模型配置，检测模型比例（例如 n、s、m、l、x）并相应地调整参数。</li>\n<li><strong>guess_model_scale（）、****guess_model_task（）</strong>：用于根据 YAML 文件结构推断模型规模和任务的辅助函数。</li>\n</ul>\n</li>\n</ol>\n"},{"title":"YOLO V10 详解","date":"2024-12-07T10:30:00.000Z","_content":"\n\n\n# 引言\n\n### 背景介绍\n\n实时物体检测一直是计算机视觉领域的研究热点，旨在低延迟下准确预测图像中物体的类别和位置。该技术广泛应用于自动驾驶、机器人导航和物体跟踪等实际应用中。近年来，基于卷积神经网络（CNN）的物体检测器因其高效的性能而受到广泛关注，其中YOLO系列因其出色的性能和效率平衡而脱颖而出。\n\n### 研究内容\n\n本文旨在解决YOLO系列在实际部署中依赖非极大值抑制（NMS）导致的推理延迟问题，并通过优化模型架构进一步提升其性能和效率。具体来说，本文提出了以下两个主要目标：\n\n1. 提出一种无需NMS训练的一致双分配策略，以实现高效的端到端检测。\n2. 提出一种全面的效率-准确性驱动的模型设计策略，从后处理和模型架构两方面提升YOLO的性能和效率。\n\n### 研究难点\n\nYOLO系列在实际应用中面临的主要挑战包括：\n\n1. **NMS依赖性**：传统的YOLO训练过程中采用一对多标签分配策略，导致推理过程中需要依赖NMS进行后处理，这不仅增加了推理延迟，还使得模型对NMS超参数敏感，难以实现最优的端到端部署。\n2. **模型架构设计**：尽管已有大量研究探索了不同的模型架构设计策略，但YOLO系列在各个组件的设计上仍存在计算冗余，限制了模型的性能和效率。\n\n### 相关工作\n\n本文回顾了现有的实时物体检测器和端到端物体检测器的相关研究：\n\n1. **传统YOLO系列**：YOLOv1、YOLOv2和YOLOv3是典型的三部分检测架构，包括主干、颈部和头部。后续的YOLOv4和YOLOv5引入了CSPNet设计，并结合数据增强策略和多种模型尺度。YOLOv6、YOLOv7和YOLOv8分别提出了BiC、E-ELAN和C2f等新的组件设计。\n2. **端到端物体检测器**：DETR系列通过引入Transformer架构和匈牙利损失实现了一对一匹配预测，消除了手工设计的组件和后处理。其他研究如Learnable NMS和关系网络也尝试通过不同的方法实现端到端检测。\n\n### 研究方法\n\n本文提出了一种无需NMS训练的一致双分配策略和全面的效率-准确性驱动的模型设计策略：\n\n1. **一致双分配策略**：通过引入双标签分配和一致的匹配度量，结合一对多和一对一标签分配的优势，实现高效的端到端检测。\n2. **效率-准确性驱动的模型设计策略**：从模型架构的各个组件入手，提出了轻量级分类头、空间-通道解耦下采样和排名引导的块设计等优化方法，减少计算冗余并提升模型性能。\n\n### 实验设计\n\n本文在COCO数据集上对提出的YOLOv10模型进行了广泛的实验验证，具体包括：\n\n1. **数据集**：使用COCO数据集进行训练和评估，采用标准的训练-验证-测试划分。\n2. **实验设置**：所有模型在8块NVIDIA 3090 GPU上进行训练，采用SGD优化器，并结合Mosaic、Mixup和复制粘贴等数据增强策略。\n3. **评估指标**：使用标准平均精度（AP）和不同IoU阈值下的AP值评估模型性能，并测量推理延迟以评估效率。\n\n### 结果与分析\n\n实验结果表明，YOLOv10在多个模型尺度上均取得了显著的性能和效率提升：\n\n1. **性能提升**：YOLOv10-S在相似的AP下比RT-DETR-R18快1.8倍，YOLOv10-B在相同性能下比YOLOv9-C减少了46%的延迟。\n2. **参数和计算量减少**：YOLOv10-S和YOLOv10-B分别比RT-DETR-R18和YOLOv9-C减少了2.8倍和25%的参数数量和FLOPs。\n3. **全面优势**：YOLOv10在多个模型尺度上均优于现有先进模型，展示了其在计算-准确性权衡上的优越性。\n\n### 总体结论\n\n本文通过提出一致双分配策略和全面的效率-准确性驱动的模型设计策略，成功提升了YOLO系列的性能和效率。实验结果验证了YOLOv10在多个模型尺度上的优越性，展示了其在实时物体检测领域的潜力。未来的工作将进一步探索减少小模型中一对多训练和无需NMS训练之间的性能差距的方法。\n\n# 研究方法\n\n### 无NMS训练的一致双重分配\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/5f60bbd9bccdfacc4ce2a53bce9011dc-image.png)\n\n- 双重标签分配。\n\n  为了实现无需NMS的训练，论文提出了一致的双重分配策略。该策略结合一对多和多对一的标签分配策略的优点。具体来说，双重分配策略包括一个多对一的头和一个一对一的头。在训练过程中，两个头共同优化，使得主干和颈部能够享受多对一分配提供的丰富监督信号。在推理过程中，丢弃多对一头，只使用一对一头来做出预测，从而实现无需NMS的高效端到端部署。\n\n- 一致的匹配度量。\n\n为了确保两个头之间的和谐监督，论文提出了一致的匹配度量。该度量公式如下：\n$$\nm(\\alpha,\\beta)=s\\cdot p^{\\alpha}\\cdot IoU(\\hat{b},b)^{\\beta}\n$$\n其中，*p* 是分类分数，$\\hat{b}$ 和 *b* 分别表示预测和实例的边界框，*s* 表示空间先验，指示预测的锚点是否在实例内，*α* 和 *β* 是两个重要的超参数，平衡语义预测任务和位置回归任务的影响。\n\n### 全局效率-准确性驱动的模型设计\n\n除了后处理，YOLOs的模型架构也对效率-准确性权衡提出了巨大挑战[50, 8, 29]。尽管以前的工作探索了各种设计策略，但仍然缺乏对YOLOs中各个组件的全面检查。因此，模型架构展现出不可忽视的计算冗余和受限能力，这阻碍了其实现高效率和性能的潜力。在这里，我们旨在从效率和准确性角度全面执行YOLOs的模型设计。\n\n效率驱动的模型设计。YOLO中的组件包括茎(stem)、下采样层、带有基本构建块的阶段以及头部。茎的计算成本较低，因此我们对其他三个部分采用效率驱动的模型设计。\n\n（1）轻量级分类头部。在YOLO中，分类和回归头部通常共享相同的架构。然而，它们在计算开销上表现出显著的差异。例如，在YOLOv8-S中，分类头部的FLOPs和参数数量分别为回归头部的2.5倍和2.4倍。然而，在分析分类错误和回归错误的影响后（见表6），我们发现回归头部对YOLOs的性能承担了更大的重要性。因此，我们可以减少分类头部的开销，而不用担心会大幅损害性能。因此，我们简单地采用轻量级的架构用于分类头部，它由两个深度可分离的卷积[25, 9]组成，核大小为3x3，然后是一个1x1卷积。\n\n（2）空间通道解耦的下采样。YOLO通常利用常规的3x3标准卷积，步长为2，同时实现空间下采样（从H x W到$\\frac{H}{2} \\times \\frac{W}{2}$）和通道变换（从C到2C）。这引入了不可忽视的计算成本，即O(29*H**W**C*2)，以及参数数量，即O(18$C^2$)。相反，我们提出将空间缩减和通道增加操作解耦，以实现更高效的缩减。具体来说，我们首先利用逐点卷积来调节通道维度，然后利用深度卷积来进行空间缩减。这将计算成本降低到$O(2HWC^2+ \\frac{9}{2}HWC)$，参数数量减少到$O(2C^2+18C)$。同时，在缩减过程中最大化信息保留，从而在延迟减少的同时具有竞争力。\n\n（3）基于rank引导的模块设计：YOLOs通常在所有阶段使用相同的基本构建块，例如YOLOv8中的瓶颈块。为了彻底检查YOLOs的这种同质设计，我们利用内在秩来分析每个阶段的冗余。具体来说，我们计算每个阶段中最后一个基本块的最后一个卷积的数值秩，这计算了大于阈值的奇异值的数量。图3.(a)展示了YOLOv8的结果，表明深层阶段和大型模型更容易表现出更多的冗余。这一观察表明，简单地为所有阶段应用相同的块设计对于最佳的容量-效率权衡是次优的。为了解决这个问题，我们提出了一种基于秩的块设计方案，旨在通过紧凑的架构设计降低被证明是冗余的阶段复杂度。我们首先提出了一个紧凑的倒置块（CIB）结构，它采用廉价的深度可分离卷积进行空间混合，以及成本效益高的点对点卷积进行通道混合，如图3.(b)所示。它可以作为高效的基本构建块，例如嵌入在ELAN结构中（图3.(b)）。然后，我们提倡一种基于秩的块分配策略，以实现最佳效率，同时保持有竞争力的容量。具体来说，给定一个模型，我们根据其内在秩按升序对所有阶段进行排序。我们进一步检查用CIB替换领先阶段的基本块的性能变化。如果与给定模型相比没有性能下降，我们就继续替换下一个阶段，否则就停止该过程。因此，我们可以在不同阶段和模型规模上实现自适应的紧凑块设计，实现更高的效率而不损害性能。\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/0af9b20597598897c15c90a02d5c1093-1733232140019-3.png)\n\n### 准确性驱动的模型设计。\n\n我们进一步探索大核卷积和自注意力在准确性驱动设计中的应用，旨在以最小的成本提升性能。\n\n（1）大核卷积。采用大核深度卷积是一种有效的方法来扩大感受野并增强模型的能力[10, 40, 39]。然而，简单地在所有阶段都利用它们可能会引入浅层特征中的污染，同时也会在高分辨率阶段引入显著的I/O开销和延迟[8]。因此，我们建议在深度阶段利用CIB中的大核深度卷积。具体来说，我们增加了CIB中第二个3x3深度卷积的核大小为7x7，随后[39]。此外，我们采用结构重参数化技术[11,10,59]来引入另一个3x3深度卷积分支，以缓解优化问题而不增加推理开销。此外，随着模型规模的增加，其感受野自然扩大，使用大核卷积的好处逐渐减弱。因此，我们只在小型模型规模上采用大核卷积。\n\n(2) 部分自注意力(PSA)。自注意力[58]由于其显著的全球建模能力[38, 14, 76]而被广泛应用于各种视觉任务。然而，它表现出高计算复杂性和内存占用。为了解决这个问题，鉴于普遍存在的注意力头重用[69]，我们提出了一个高效的局部自注意力(PSA)模块设计，如图3.(c)所示。具体来说，在1x1卷积之后，我们将通道中的特征均匀划分为两部分。我们只将一部分输入到由多头自注意力模块(MHSA)和前馈网络(FFN)组成的NPSA块中。然后将两部分通过1x1卷积连接并融合。此外，我们遵循[22]的方法，将查询和键的维度分配给MHSA中的值的一半，并用BatchNorm[27]替换LayerNorm[1]以进行快速推理。此外，PSA仅在分辨率最低的第4阶段之后放置，避免了过多的开销。\n\n# 实验\n\n### 实现细节\n\n我们选择YOLOv8[21]作为我们的基线模型，因为它在延迟准确性和各种模型规模的可用性方面表现良好。我们采用了一致的NMS-free训练双重分配，并基于此进行了全体的效率-准确性驱动的模型设计，从而带来了我们的YOLOv10模型。YOLOv10具有与YOLOv8相同的变体，即N/S/M/L/X。此外，我们通过简单增加YOLOv10-M的宽度尺度因子，推导出了一个新的变体YOLOv10-B。我们在相同的全局从零开始设置[21, 65, 62]下，在COCO[35]上验证了所提出的检测器。此外，所有模型的延迟都在T4 GPU和TensorRT FP16上进行测试，遵循[78]的方法。\n\n### 与最先进技术的比较\n\n如表1所示，我们的YOLOv10在各种模型规模上实现了最先进的性能和端到端的延迟。我们首先将YOLOv10与我们基线模型，即YOLOv8进行比较。在N/S/M/L/X五种变体中，我们的YOLOv10实现了1.2%/1.4%/0.5%/0.3%/0.5%的AP改进，参数减少了28%/ 36%/ 41%/ 44%/ 57%，计算减少了23%/ 24%/ 25%/ 27%/ 38%，延迟减少了70%/65%/50%/41%/37%。与其他YOLO相比，\n\n![image-20241203212603338](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212603338.png)\n\nYOLOv10在准确性和计算成本之间也展现了卓越的权衡。具体来说，对于轻量级和小型的模型，YOLOv10-N/S的性能超过了YOLOv6-3.0-N/S，分别提高了1.5 AP和2.0 AP，参数减少了51%，计算量减少了41%。对于中等规模的模型，与YOLOv9-C/ YOLO-MS相比，YOLOv10-B/M在相同或更好的性能下，延迟降低了46%/62%。对于大型模型，与Gold-YOLO-L相比，我们的YOLOv10-L在参数减少了68%，延迟降低了32%，并且AP显著提高了1.4%。此外，与RT-DETR相比，YOLOv10获得了显著的性能和延迟提升。值得注意的是，在相似的性能下，YOLOv10-S/X的推理速度比RT-DETR-R18/R101快了1.8倍和1.3倍。这些结果充分展示了YOLOv10作为实时端到端检测器的优越性。\n\n我们还使用原始的一对多训练方法将YOLOv10与其他YOLO进行了比较。在这种情况下，我们考虑了模型前向过程（Latencyf）的性能和延迟[62, 21, 60]。如表1所示，YOLOv10在不同模型规模上也展现了最先进的表现和效率，这表明了我们架构设计的有效性。\n\n###  模型分析\n\n![image-20241203212826090](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212826090.png)\n\n![image-20241203213021438](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213021438.png)\n\n![image-20241203213123173](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213123173.png)\n\n![image-20241203213200887](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213200887.png)\n\n1. **消融研究**：基于YOLOv10-S和YOLOv10-M的消融研究表明，无NMS训练结合一致的双标签分配显著降低了YOLOv10-S的端到端延迟，同时保持了44.3%的AP竞争力。此外，效率驱动的设计减少了参数数量和计算量，并显著降低了延迟。\n\n2. **双标签分配**：双标签分配为无NMS的YOLO提供了丰富的监督信息，并在推理时实现了高效性。一致匹配度量的引入进一步缩小了两个分支之间的监督差距，提高了性能。\n\n3. **效率驱动模型设计**：效率驱动模型设计通过轻量级分类头、空间-通道解耦下采样和紧凑倒置块（CIB）等组件，有效减少了参数数量、FLOPs和延迟，同时保持了竞争性的性能。\n\n4. **准确性驱动模型设计**：准确性驱动模型设计通过大核卷积和部分自注意力（PSA）模块，在不显著增加延迟的情况下提高了性能。\n\n5. **大核卷积**：大核卷积的使用扩大了感受野并增强了模型能力，但在小模型中效果更佳。\n\n6. **部分自注意力模块**：PSA模块通过减少自注意力头中的冗余来缓解优化问题，从而在不牺牲高效率的情况下提升了模型性能。\n\n# YOLOv10代码\n\n### C2fUIB介绍\n\n**C2fUIB只是用CIB结构替换了YOLOv8中 C2f的Bottleneck结构**\n\n**实现代码ultralytics/nn/modules/block.py**\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/8ed70c479c7530fe3d36f4f44fbbf2d8.png)\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/a3198a08d3b755ec2e1e85cb8979c2ee.png)\n\n```python\nclass CIB(nn.Module):\n    \"\"\"Standard bottleneck.\"\"\"\n\n    def __init__(self, c1, c2, shortcut=True, e=0.5, lk=False):\n        \"\"\"Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and\n        expansion.\n        \"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = nn.Sequential(\n            Conv(c1, c1, 3, g=c1),\n            Conv(c1, 2 * c_, 1),\n            Conv(2 * c_, 2 * c_, 3, g=2 * c_) if not lk else RepVGGDW(2 * c_),\n            Conv(2 * c_, c2, 1),\n            Conv(c2, c2, 3, g=c2),\n        )\n\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        \"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\n        return x + self.cv1(x) if self.add else self.cv1(x)\n\nclass C2fCIB(C2f):\n    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n\n    def __init__(self, c1, c2, n=1, shortcut=False, lk=False, g=1, e=0.5):\n        \"\"\"Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,\n        expansion.\n        \"\"\"\n        super().__init__(c1, c2, n, shortcut, g, e)\n        self.m = nn.ModuleList(CIB(self.c, self.c, shortcut, e=1.0, lk=lk) for _ in range(n))\n```\n\n### PSA介绍\n\n具体来说，我们在1×1卷积后将特征均匀地分为两部分。我们只将一部分输入到由多头自注意力模块（MHSA）和前馈网络（FFN）组成的NPSA块中。然后，两部分通过1×1卷积连接并融合。此外，遵循将查询和键的维度分配为值的一半，并用BatchNorm替换LayerNorm以实现快速推理。\n\n**实现代码ultralytics/nn/modules/block.py**\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/82d0ed0834ac712354683efcb0108bc6.png)\n\n```python\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8,\n                 attn_ratio=0.5):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.key_dim = int(self.head_dim * attn_ratio)\n        self.scale = self.key_dim ** -0.5\n        nh_kd = nh_kd = self.key_dim * num_heads\n        h = dim + nh_kd * 2\n        self.qkv = Conv(dim, h, 1, act=False)\n        self.proj = Conv(dim, dim, 1, act=False)\n        self.pe = Conv(dim, dim, 3, 1, g=dim, act=False)\n\n    def forward(self, x):\n        B, _, H, W = x.shape\n        N = H * W\n        qkv = self.qkv(x)\n        q, k, v = qkv.view(B, self.num_heads, -1, N).split([self.key_dim, self.key_dim, self.head_dim], dim=2)\n\n        attn = (\n            (q.transpose(-2, -1) @ k) * self.scale\n        )\n        attn = attn.softmax(dim=-1)\n        x = (v @ attn.transpose(-2, -1)).view(B, -1, H, W) + self.pe(v.reshape(B, -1, H, W))\n        x = self.proj(x)\n        return x\n\nclass PSA(nn.Module):\n\n    def __init__(self, c1, c2, e=0.5):\n        super().__init__()\n        assert(c1 == c2)\n        self.c = int(c1 * e)\n        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n        self.cv2 = Conv(2 * self.c, c1, 1)\n        \n        self.attn = Attention(self.c, attn_ratio=0.5, num_heads=self.c // 64)\n        self.ffn = nn.Sequential(\n            Conv(self.c, self.c*2, 1),\n            Conv(self.c*2, self.c, 1, act=False)\n        )\n        \n    def forward(self, x):\n        a, b = self.cv1(x).split((self.c, self.c), dim=1)\n        b = b + self.attn(b)\n        b = b + self.ffn(b)\n        return self.cv2(torch.cat((a, b), 1))\n```\n\n### SCDown\n\nOLOs通常利用常规的3×3标准卷积，步长为2，同时实现空间下采样（从H×W到H/2×W/2）和通道变换（从C到2C）。这引入了不可忽视的计算成本$O(9HWC^2)$和参数数量O$(18C^2)$。相反，我们提议将空间缩减和通道增加操作解耦，以实现更高效的下采样。具体来说，我们首先利用点对点卷积来调整通道维度，然后利用深度可分离卷积进行空间下采样。这将计算成本降低到O(2HWC^2 + 9HWC)，并将参数数量减少到O(2C^2 + 18C)。同时，它最大限度地保留了下采样过程中的信息，从而在减少延迟的同时保持了有竞争力的性能。\n\n**实现代码ultralytics/nn/modules/block.py**\n\n```\nclass SCDown(nn.Module):\n    def __init__(self, c1, c2, k, s):\n        super().__init__()\n        self.cv1 = Conv(c1, c2, 1, 1)\n        self.cv2 = Conv(c2, c2, k=k, s=s, g=c2, act=False)\n\n    def forward(self, x):\n        return self.cv2(self.cv1(x))\n```\n\n\n\n参考：[YOLOv10真正实时端到端目标检测（原理介绍+代码详见+结构框图）-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2426044)\n\n文章合集：https://github.com/chongzicbo/ReadWriteThink\n\n","source":"_posts/computer-vision/CV010-YOLO V10详解.md","raw":"---\ntitle: 'YOLO V10 详解'\ndate: 2024-12-7 18:30:00\ncategories:\n  - computer-vision\ntags:\n  - yolo\n  - 目标检测\n---\n\n\n\n# 引言\n\n### 背景介绍\n\n实时物体检测一直是计算机视觉领域的研究热点，旨在低延迟下准确预测图像中物体的类别和位置。该技术广泛应用于自动驾驶、机器人导航和物体跟踪等实际应用中。近年来，基于卷积神经网络（CNN）的物体检测器因其高效的性能而受到广泛关注，其中YOLO系列因其出色的性能和效率平衡而脱颖而出。\n\n### 研究内容\n\n本文旨在解决YOLO系列在实际部署中依赖非极大值抑制（NMS）导致的推理延迟问题，并通过优化模型架构进一步提升其性能和效率。具体来说，本文提出了以下两个主要目标：\n\n1. 提出一种无需NMS训练的一致双分配策略，以实现高效的端到端检测。\n2. 提出一种全面的效率-准确性驱动的模型设计策略，从后处理和模型架构两方面提升YOLO的性能和效率。\n\n### 研究难点\n\nYOLO系列在实际应用中面临的主要挑战包括：\n\n1. **NMS依赖性**：传统的YOLO训练过程中采用一对多标签分配策略，导致推理过程中需要依赖NMS进行后处理，这不仅增加了推理延迟，还使得模型对NMS超参数敏感，难以实现最优的端到端部署。\n2. **模型架构设计**：尽管已有大量研究探索了不同的模型架构设计策略，但YOLO系列在各个组件的设计上仍存在计算冗余，限制了模型的性能和效率。\n\n### 相关工作\n\n本文回顾了现有的实时物体检测器和端到端物体检测器的相关研究：\n\n1. **传统YOLO系列**：YOLOv1、YOLOv2和YOLOv3是典型的三部分检测架构，包括主干、颈部和头部。后续的YOLOv4和YOLOv5引入了CSPNet设计，并结合数据增强策略和多种模型尺度。YOLOv6、YOLOv7和YOLOv8分别提出了BiC、E-ELAN和C2f等新的组件设计。\n2. **端到端物体检测器**：DETR系列通过引入Transformer架构和匈牙利损失实现了一对一匹配预测，消除了手工设计的组件和后处理。其他研究如Learnable NMS和关系网络也尝试通过不同的方法实现端到端检测。\n\n### 研究方法\n\n本文提出了一种无需NMS训练的一致双分配策略和全面的效率-准确性驱动的模型设计策略：\n\n1. **一致双分配策略**：通过引入双标签分配和一致的匹配度量，结合一对多和一对一标签分配的优势，实现高效的端到端检测。\n2. **效率-准确性驱动的模型设计策略**：从模型架构的各个组件入手，提出了轻量级分类头、空间-通道解耦下采样和排名引导的块设计等优化方法，减少计算冗余并提升模型性能。\n\n### 实验设计\n\n本文在COCO数据集上对提出的YOLOv10模型进行了广泛的实验验证，具体包括：\n\n1. **数据集**：使用COCO数据集进行训练和评估，采用标准的训练-验证-测试划分。\n2. **实验设置**：所有模型在8块NVIDIA 3090 GPU上进行训练，采用SGD优化器，并结合Mosaic、Mixup和复制粘贴等数据增强策略。\n3. **评估指标**：使用标准平均精度（AP）和不同IoU阈值下的AP值评估模型性能，并测量推理延迟以评估效率。\n\n### 结果与分析\n\n实验结果表明，YOLOv10在多个模型尺度上均取得了显著的性能和效率提升：\n\n1. **性能提升**：YOLOv10-S在相似的AP下比RT-DETR-R18快1.8倍，YOLOv10-B在相同性能下比YOLOv9-C减少了46%的延迟。\n2. **参数和计算量减少**：YOLOv10-S和YOLOv10-B分别比RT-DETR-R18和YOLOv9-C减少了2.8倍和25%的参数数量和FLOPs。\n3. **全面优势**：YOLOv10在多个模型尺度上均优于现有先进模型，展示了其在计算-准确性权衡上的优越性。\n\n### 总体结论\n\n本文通过提出一致双分配策略和全面的效率-准确性驱动的模型设计策略，成功提升了YOLO系列的性能和效率。实验结果验证了YOLOv10在多个模型尺度上的优越性，展示了其在实时物体检测领域的潜力。未来的工作将进一步探索减少小模型中一对多训练和无需NMS训练之间的性能差距的方法。\n\n# 研究方法\n\n### 无NMS训练的一致双重分配\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/5f60bbd9bccdfacc4ce2a53bce9011dc-image.png)\n\n- 双重标签分配。\n\n  为了实现无需NMS的训练，论文提出了一致的双重分配策略。该策略结合一对多和多对一的标签分配策略的优点。具体来说，双重分配策略包括一个多对一的头和一个一对一的头。在训练过程中，两个头共同优化，使得主干和颈部能够享受多对一分配提供的丰富监督信号。在推理过程中，丢弃多对一头，只使用一对一头来做出预测，从而实现无需NMS的高效端到端部署。\n\n- 一致的匹配度量。\n\n为了确保两个头之间的和谐监督，论文提出了一致的匹配度量。该度量公式如下：\n$$\nm(\\alpha,\\beta)=s\\cdot p^{\\alpha}\\cdot IoU(\\hat{b},b)^{\\beta}\n$$\n其中，*p* 是分类分数，$\\hat{b}$ 和 *b* 分别表示预测和实例的边界框，*s* 表示空间先验，指示预测的锚点是否在实例内，*α* 和 *β* 是两个重要的超参数，平衡语义预测任务和位置回归任务的影响。\n\n### 全局效率-准确性驱动的模型设计\n\n除了后处理，YOLOs的模型架构也对效率-准确性权衡提出了巨大挑战[50, 8, 29]。尽管以前的工作探索了各种设计策略，但仍然缺乏对YOLOs中各个组件的全面检查。因此，模型架构展现出不可忽视的计算冗余和受限能力，这阻碍了其实现高效率和性能的潜力。在这里，我们旨在从效率和准确性角度全面执行YOLOs的模型设计。\n\n效率驱动的模型设计。YOLO中的组件包括茎(stem)、下采样层、带有基本构建块的阶段以及头部。茎的计算成本较低，因此我们对其他三个部分采用效率驱动的模型设计。\n\n（1）轻量级分类头部。在YOLO中，分类和回归头部通常共享相同的架构。然而，它们在计算开销上表现出显著的差异。例如，在YOLOv8-S中，分类头部的FLOPs和参数数量分别为回归头部的2.5倍和2.4倍。然而，在分析分类错误和回归错误的影响后（见表6），我们发现回归头部对YOLOs的性能承担了更大的重要性。因此，我们可以减少分类头部的开销，而不用担心会大幅损害性能。因此，我们简单地采用轻量级的架构用于分类头部，它由两个深度可分离的卷积[25, 9]组成，核大小为3x3，然后是一个1x1卷积。\n\n（2）空间通道解耦的下采样。YOLO通常利用常规的3x3标准卷积，步长为2，同时实现空间下采样（从H x W到$\\frac{H}{2} \\times \\frac{W}{2}$）和通道变换（从C到2C）。这引入了不可忽视的计算成本，即O(29*H**W**C*2)，以及参数数量，即O(18$C^2$)。相反，我们提出将空间缩减和通道增加操作解耦，以实现更高效的缩减。具体来说，我们首先利用逐点卷积来调节通道维度，然后利用深度卷积来进行空间缩减。这将计算成本降低到$O(2HWC^2+ \\frac{9}{2}HWC)$，参数数量减少到$O(2C^2+18C)$。同时，在缩减过程中最大化信息保留，从而在延迟减少的同时具有竞争力。\n\n（3）基于rank引导的模块设计：YOLOs通常在所有阶段使用相同的基本构建块，例如YOLOv8中的瓶颈块。为了彻底检查YOLOs的这种同质设计，我们利用内在秩来分析每个阶段的冗余。具体来说，我们计算每个阶段中最后一个基本块的最后一个卷积的数值秩，这计算了大于阈值的奇异值的数量。图3.(a)展示了YOLOv8的结果，表明深层阶段和大型模型更容易表现出更多的冗余。这一观察表明，简单地为所有阶段应用相同的块设计对于最佳的容量-效率权衡是次优的。为了解决这个问题，我们提出了一种基于秩的块设计方案，旨在通过紧凑的架构设计降低被证明是冗余的阶段复杂度。我们首先提出了一个紧凑的倒置块（CIB）结构，它采用廉价的深度可分离卷积进行空间混合，以及成本效益高的点对点卷积进行通道混合，如图3.(b)所示。它可以作为高效的基本构建块，例如嵌入在ELAN结构中（图3.(b)）。然后，我们提倡一种基于秩的块分配策略，以实现最佳效率，同时保持有竞争力的容量。具体来说，给定一个模型，我们根据其内在秩按升序对所有阶段进行排序。我们进一步检查用CIB替换领先阶段的基本块的性能变化。如果与给定模型相比没有性能下降，我们就继续替换下一个阶段，否则就停止该过程。因此，我们可以在不同阶段和模型规模上实现自适应的紧凑块设计，实现更高的效率而不损害性能。\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/0af9b20597598897c15c90a02d5c1093-1733232140019-3.png)\n\n### 准确性驱动的模型设计。\n\n我们进一步探索大核卷积和自注意力在准确性驱动设计中的应用，旨在以最小的成本提升性能。\n\n（1）大核卷积。采用大核深度卷积是一种有效的方法来扩大感受野并增强模型的能力[10, 40, 39]。然而，简单地在所有阶段都利用它们可能会引入浅层特征中的污染，同时也会在高分辨率阶段引入显著的I/O开销和延迟[8]。因此，我们建议在深度阶段利用CIB中的大核深度卷积。具体来说，我们增加了CIB中第二个3x3深度卷积的核大小为7x7，随后[39]。此外，我们采用结构重参数化技术[11,10,59]来引入另一个3x3深度卷积分支，以缓解优化问题而不增加推理开销。此外，随着模型规模的增加，其感受野自然扩大，使用大核卷积的好处逐渐减弱。因此，我们只在小型模型规模上采用大核卷积。\n\n(2) 部分自注意力(PSA)。自注意力[58]由于其显著的全球建模能力[38, 14, 76]而被广泛应用于各种视觉任务。然而，它表现出高计算复杂性和内存占用。为了解决这个问题，鉴于普遍存在的注意力头重用[69]，我们提出了一个高效的局部自注意力(PSA)模块设计，如图3.(c)所示。具体来说，在1x1卷积之后，我们将通道中的特征均匀划分为两部分。我们只将一部分输入到由多头自注意力模块(MHSA)和前馈网络(FFN)组成的NPSA块中。然后将两部分通过1x1卷积连接并融合。此外，我们遵循[22]的方法，将查询和键的维度分配给MHSA中的值的一半，并用BatchNorm[27]替换LayerNorm[1]以进行快速推理。此外，PSA仅在分辨率最低的第4阶段之后放置，避免了过多的开销。\n\n# 实验\n\n### 实现细节\n\n我们选择YOLOv8[21]作为我们的基线模型，因为它在延迟准确性和各种模型规模的可用性方面表现良好。我们采用了一致的NMS-free训练双重分配，并基于此进行了全体的效率-准确性驱动的模型设计，从而带来了我们的YOLOv10模型。YOLOv10具有与YOLOv8相同的变体，即N/S/M/L/X。此外，我们通过简单增加YOLOv10-M的宽度尺度因子，推导出了一个新的变体YOLOv10-B。我们在相同的全局从零开始设置[21, 65, 62]下，在COCO[35]上验证了所提出的检测器。此外，所有模型的延迟都在T4 GPU和TensorRT FP16上进行测试，遵循[78]的方法。\n\n### 与最先进技术的比较\n\n如表1所示，我们的YOLOv10在各种模型规模上实现了最先进的性能和端到端的延迟。我们首先将YOLOv10与我们基线模型，即YOLOv8进行比较。在N/S/M/L/X五种变体中，我们的YOLOv10实现了1.2%/1.4%/0.5%/0.3%/0.5%的AP改进，参数减少了28%/ 36%/ 41%/ 44%/ 57%，计算减少了23%/ 24%/ 25%/ 27%/ 38%，延迟减少了70%/65%/50%/41%/37%。与其他YOLO相比，\n\n![image-20241203212603338](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212603338.png)\n\nYOLOv10在准确性和计算成本之间也展现了卓越的权衡。具体来说，对于轻量级和小型的模型，YOLOv10-N/S的性能超过了YOLOv6-3.0-N/S，分别提高了1.5 AP和2.0 AP，参数减少了51%，计算量减少了41%。对于中等规模的模型，与YOLOv9-C/ YOLO-MS相比，YOLOv10-B/M在相同或更好的性能下，延迟降低了46%/62%。对于大型模型，与Gold-YOLO-L相比，我们的YOLOv10-L在参数减少了68%，延迟降低了32%，并且AP显著提高了1.4%。此外，与RT-DETR相比，YOLOv10获得了显著的性能和延迟提升。值得注意的是，在相似的性能下，YOLOv10-S/X的推理速度比RT-DETR-R18/R101快了1.8倍和1.3倍。这些结果充分展示了YOLOv10作为实时端到端检测器的优越性。\n\n我们还使用原始的一对多训练方法将YOLOv10与其他YOLO进行了比较。在这种情况下，我们考虑了模型前向过程（Latencyf）的性能和延迟[62, 21, 60]。如表1所示，YOLOv10在不同模型规模上也展现了最先进的表现和效率，这表明了我们架构设计的有效性。\n\n###  模型分析\n\n![image-20241203212826090](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212826090.png)\n\n![image-20241203213021438](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213021438.png)\n\n![image-20241203213123173](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213123173.png)\n\n![image-20241203213200887](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213200887.png)\n\n1. **消融研究**：基于YOLOv10-S和YOLOv10-M的消融研究表明，无NMS训练结合一致的双标签分配显著降低了YOLOv10-S的端到端延迟，同时保持了44.3%的AP竞争力。此外，效率驱动的设计减少了参数数量和计算量，并显著降低了延迟。\n\n2. **双标签分配**：双标签分配为无NMS的YOLO提供了丰富的监督信息，并在推理时实现了高效性。一致匹配度量的引入进一步缩小了两个分支之间的监督差距，提高了性能。\n\n3. **效率驱动模型设计**：效率驱动模型设计通过轻量级分类头、空间-通道解耦下采样和紧凑倒置块（CIB）等组件，有效减少了参数数量、FLOPs和延迟，同时保持了竞争性的性能。\n\n4. **准确性驱动模型设计**：准确性驱动模型设计通过大核卷积和部分自注意力（PSA）模块，在不显著增加延迟的情况下提高了性能。\n\n5. **大核卷积**：大核卷积的使用扩大了感受野并增强了模型能力，但在小模型中效果更佳。\n\n6. **部分自注意力模块**：PSA模块通过减少自注意力头中的冗余来缓解优化问题，从而在不牺牲高效率的情况下提升了模型性能。\n\n# YOLOv10代码\n\n### C2fUIB介绍\n\n**C2fUIB只是用CIB结构替换了YOLOv8中 C2f的Bottleneck结构**\n\n**实现代码ultralytics/nn/modules/block.py**\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/8ed70c479c7530fe3d36f4f44fbbf2d8.png)\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/a3198a08d3b755ec2e1e85cb8979c2ee.png)\n\n```python\nclass CIB(nn.Module):\n    \"\"\"Standard bottleneck.\"\"\"\n\n    def __init__(self, c1, c2, shortcut=True, e=0.5, lk=False):\n        \"\"\"Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and\n        expansion.\n        \"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = nn.Sequential(\n            Conv(c1, c1, 3, g=c1),\n            Conv(c1, 2 * c_, 1),\n            Conv(2 * c_, 2 * c_, 3, g=2 * c_) if not lk else RepVGGDW(2 * c_),\n            Conv(2 * c_, c2, 1),\n            Conv(c2, c2, 3, g=c2),\n        )\n\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        \"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\n        return x + self.cv1(x) if self.add else self.cv1(x)\n\nclass C2fCIB(C2f):\n    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n\n    def __init__(self, c1, c2, n=1, shortcut=False, lk=False, g=1, e=0.5):\n        \"\"\"Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,\n        expansion.\n        \"\"\"\n        super().__init__(c1, c2, n, shortcut, g, e)\n        self.m = nn.ModuleList(CIB(self.c, self.c, shortcut, e=1.0, lk=lk) for _ in range(n))\n```\n\n### PSA介绍\n\n具体来说，我们在1×1卷积后将特征均匀地分为两部分。我们只将一部分输入到由多头自注意力模块（MHSA）和前馈网络（FFN）组成的NPSA块中。然后，两部分通过1×1卷积连接并融合。此外，遵循将查询和键的维度分配为值的一半，并用BatchNorm替换LayerNorm以实现快速推理。\n\n**实现代码ultralytics/nn/modules/block.py**\n\n![img](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/82d0ed0834ac712354683efcb0108bc6.png)\n\n```python\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8,\n                 attn_ratio=0.5):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.key_dim = int(self.head_dim * attn_ratio)\n        self.scale = self.key_dim ** -0.5\n        nh_kd = nh_kd = self.key_dim * num_heads\n        h = dim + nh_kd * 2\n        self.qkv = Conv(dim, h, 1, act=False)\n        self.proj = Conv(dim, dim, 1, act=False)\n        self.pe = Conv(dim, dim, 3, 1, g=dim, act=False)\n\n    def forward(self, x):\n        B, _, H, W = x.shape\n        N = H * W\n        qkv = self.qkv(x)\n        q, k, v = qkv.view(B, self.num_heads, -1, N).split([self.key_dim, self.key_dim, self.head_dim], dim=2)\n\n        attn = (\n            (q.transpose(-2, -1) @ k) * self.scale\n        )\n        attn = attn.softmax(dim=-1)\n        x = (v @ attn.transpose(-2, -1)).view(B, -1, H, W) + self.pe(v.reshape(B, -1, H, W))\n        x = self.proj(x)\n        return x\n\nclass PSA(nn.Module):\n\n    def __init__(self, c1, c2, e=0.5):\n        super().__init__()\n        assert(c1 == c2)\n        self.c = int(c1 * e)\n        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n        self.cv2 = Conv(2 * self.c, c1, 1)\n        \n        self.attn = Attention(self.c, attn_ratio=0.5, num_heads=self.c // 64)\n        self.ffn = nn.Sequential(\n            Conv(self.c, self.c*2, 1),\n            Conv(self.c*2, self.c, 1, act=False)\n        )\n        \n    def forward(self, x):\n        a, b = self.cv1(x).split((self.c, self.c), dim=1)\n        b = b + self.attn(b)\n        b = b + self.ffn(b)\n        return self.cv2(torch.cat((a, b), 1))\n```\n\n### SCDown\n\nOLOs通常利用常规的3×3标准卷积，步长为2，同时实现空间下采样（从H×W到H/2×W/2）和通道变换（从C到2C）。这引入了不可忽视的计算成本$O(9HWC^2)$和参数数量O$(18C^2)$。相反，我们提议将空间缩减和通道增加操作解耦，以实现更高效的下采样。具体来说，我们首先利用点对点卷积来调整通道维度，然后利用深度可分离卷积进行空间下采样。这将计算成本降低到O(2HWC^2 + 9HWC)，并将参数数量减少到O(2C^2 + 18C)。同时，它最大限度地保留了下采样过程中的信息，从而在减少延迟的同时保持了有竞争力的性能。\n\n**实现代码ultralytics/nn/modules/block.py**\n\n```\nclass SCDown(nn.Module):\n    def __init__(self, c1, c2, k, s):\n        super().__init__()\n        self.cv1 = Conv(c1, c2, 1, 1)\n        self.cv2 = Conv(c2, c2, k=k, s=s, g=c2, act=False)\n\n    def forward(self, x):\n        return self.cv2(self.cv1(x))\n```\n\n\n\n参考：[YOLOv10真正实时端到端目标检测（原理介绍+代码详见+结构框图）-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2426044)\n\n文章合集：https://github.com/chongzicbo/ReadWriteThink\n\n","slug":"computer-vision/CV010-YOLO V10详解","published":1,"updated":"2024-12-07T11:42:07.464Z","comments":1,"layout":"post","photos":[],"_id":"cm4e4hinl0002t4hi6yon8zxn","content":"<h1 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h1><h3 id=\"背景介绍\"><a href=\"#背景介绍\" class=\"headerlink\" title=\"背景介绍\"></a>背景介绍</h3><p>实时物体检测一直是计算机视觉领域的研究热点，旨在低延迟下准确预测图像中物体的类别和位置。该技术广泛应用于自动驾驶、机器人导航和物体跟踪等实际应用中。近年来，基于卷积神经网络（CNN）的物体检测器因其高效的性能而受到广泛关注，其中YOLO系列因其出色的性能和效率平衡而脱颖而出。</p>\n<h3 id=\"研究内容\"><a href=\"#研究内容\" class=\"headerlink\" title=\"研究内容\"></a>研究内容</h3><p>本文旨在解决YOLO系列在实际部署中依赖非极大值抑制（NMS）导致的推理延迟问题，并通过优化模型架构进一步提升其性能和效率。具体来说，本文提出了以下两个主要目标：</p>\n<ol>\n<li>提出一种无需NMS训练的一致双分配策略，以实现高效的端到端检测。</li>\n<li>提出一种全面的效率-准确性驱动的模型设计策略，从后处理和模型架构两方面提升YOLO的性能和效率。</li>\n</ol>\n<h3 id=\"研究难点\"><a href=\"#研究难点\" class=\"headerlink\" title=\"研究难点\"></a>研究难点</h3><p>YOLO系列在实际应用中面临的主要挑战包括：</p>\n<ol>\n<li><strong>NMS依赖性</strong>：传统的YOLO训练过程中采用一对多标签分配策略，导致推理过程中需要依赖NMS进行后处理，这不仅增加了推理延迟，还使得模型对NMS超参数敏感，难以实现最优的端到端部署。</li>\n<li><strong>模型架构设计</strong>：尽管已有大量研究探索了不同的模型架构设计策略，但YOLO系列在各个组件的设计上仍存在计算冗余，限制了模型的性能和效率。</li>\n</ol>\n<h3 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h3><p>本文回顾了现有的实时物体检测器和端到端物体检测器的相关研究：</p>\n<ol>\n<li><strong>传统YOLO系列</strong>：YOLOv1、YOLOv2和YOLOv3是典型的三部分检测架构，包括主干、颈部和头部。后续的YOLOv4和YOLOv5引入了CSPNet设计，并结合数据增强策略和多种模型尺度。YOLOv6、YOLOv7和YOLOv8分别提出了BiC、E-ELAN和C2f等新的组件设计。</li>\n<li><strong>端到端物体检测器</strong>：DETR系列通过引入Transformer架构和匈牙利损失实现了一对一匹配预测，消除了手工设计的组件和后处理。其他研究如Learnable NMS和关系网络也尝试通过不同的方法实现端到端检测。</li>\n</ol>\n<h3 id=\"研究方法\"><a href=\"#研究方法\" class=\"headerlink\" title=\"研究方法\"></a>研究方法</h3><p>本文提出了一种无需NMS训练的一致双分配策略和全面的效率-准确性驱动的模型设计策略：</p>\n<ol>\n<li><strong>一致双分配策略</strong>：通过引入双标签分配和一致的匹配度量，结合一对多和一对一标签分配的优势，实现高效的端到端检测。</li>\n<li><strong>效率-准确性驱动的模型设计策略</strong>：从模型架构的各个组件入手，提出了轻量级分类头、空间-通道解耦下采样和排名引导的块设计等优化方法，减少计算冗余并提升模型性能。</li>\n</ol>\n<h3 id=\"实验设计\"><a href=\"#实验设计\" class=\"headerlink\" title=\"实验设计\"></a>实验设计</h3><p>本文在COCO数据集上对提出的YOLOv10模型进行了广泛的实验验证，具体包括：</p>\n<ol>\n<li><strong>数据集</strong>：使用COCO数据集进行训练和评估，采用标准的训练-验证-测试划分。</li>\n<li><strong>实验设置</strong>：所有模型在8块NVIDIA 3090 GPU上进行训练，采用SGD优化器，并结合Mosaic、Mixup和复制粘贴等数据增强策略。</li>\n<li><strong>评估指标</strong>：使用标准平均精度（AP）和不同IoU阈值下的AP值评估模型性能，并测量推理延迟以评估效率。</li>\n</ol>\n<h3 id=\"结果与分析\"><a href=\"#结果与分析\" class=\"headerlink\" title=\"结果与分析\"></a>结果与分析</h3><p>实验结果表明，YOLOv10在多个模型尺度上均取得了显著的性能和效率提升：</p>\n<ol>\n<li><strong>性能提升</strong>：YOLOv10-S在相似的AP下比RT-DETR-R18快1.8倍，YOLOv10-B在相同性能下比YOLOv9-C减少了46%的延迟。</li>\n<li><strong>参数和计算量减少</strong>：YOLOv10-S和YOLOv10-B分别比RT-DETR-R18和YOLOv9-C减少了2.8倍和25%的参数数量和FLOPs。</li>\n<li><strong>全面优势</strong>：YOLOv10在多个模型尺度上均优于现有先进模型，展示了其在计算-准确性权衡上的优越性。</li>\n</ol>\n<h3 id=\"总体结论\"><a href=\"#总体结论\" class=\"headerlink\" title=\"总体结论\"></a>总体结论</h3><p>本文通过提出一致双分配策略和全面的效率-准确性驱动的模型设计策略，成功提升了YOLO系列的性能和效率。实验结果验证了YOLOv10在多个模型尺度上的优越性，展示了其在实时物体检测领域的潜力。未来的工作将进一步探索减少小模型中一对多训练和无需NMS训练之间的性能差距的方法。</p>\n<h1 id=\"研究方法-1\"><a href=\"#研究方法-1\" class=\"headerlink\" title=\"研究方法\"></a>研究方法</h1><h3 id=\"无NMS训练的一致双重分配\"><a href=\"#无NMS训练的一致双重分配\" class=\"headerlink\" title=\"无NMS训练的一致双重分配\"></a>无NMS训练的一致双重分配</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/5f60bbd9bccdfacc4ce2a53bce9011dc-image.png\" alt=\"img\"></p>\n<ul>\n<li><p>双重标签分配。</p>\n<p>为了实现无需NMS的训练，论文提出了一致的双重分配策略。该策略结合一对多和多对一的标签分配策略的优点。具体来说，双重分配策略包括一个多对一的头和一个一对一的头。在训练过程中，两个头共同优化，使得主干和颈部能够享受多对一分配提供的丰富监督信号。在推理过程中，丢弃多对一头，只使用一对一头来做出预测，从而实现无需NMS的高效端到端部署。</p>\n</li>\n<li><p>一致的匹配度量。</p>\n</li>\n</ul>\n<p>为了确保两个头之间的和谐监督，论文提出了一致的匹配度量。该度量公式如下：<br>$$<br>m(\\alpha,\\beta)&#x3D;s\\cdot p^{\\alpha}\\cdot IoU(\\hat{b},b)^{\\beta}<br>$$<br>其中，<em>p</em> 是分类分数，$\\hat{b}$ 和 <em>b</em> 分别表示预测和实例的边界框，<em>s</em> 表示空间先验，指示预测的锚点是否在实例内，<em>α</em> 和 <em>β</em> 是两个重要的超参数，平衡语义预测任务和位置回归任务的影响。</p>\n<h3 id=\"全局效率-准确性驱动的模型设计\"><a href=\"#全局效率-准确性驱动的模型设计\" class=\"headerlink\" title=\"全局效率-准确性驱动的模型设计\"></a>全局效率-准确性驱动的模型设计</h3><p>除了后处理，YOLOs的模型架构也对效率-准确性权衡提出了巨大挑战[50, 8, 29]。尽管以前的工作探索了各种设计策略，但仍然缺乏对YOLOs中各个组件的全面检查。因此，模型架构展现出不可忽视的计算冗余和受限能力，这阻碍了其实现高效率和性能的潜力。在这里，我们旨在从效率和准确性角度全面执行YOLOs的模型设计。</p>\n<p>效率驱动的模型设计。YOLO中的组件包括茎(stem)、下采样层、带有基本构建块的阶段以及头部。茎的计算成本较低，因此我们对其他三个部分采用效率驱动的模型设计。</p>\n<p>（1）轻量级分类头部。在YOLO中，分类和回归头部通常共享相同的架构。然而，它们在计算开销上表现出显著的差异。例如，在YOLOv8-S中，分类头部的FLOPs和参数数量分别为回归头部的2.5倍和2.4倍。然而，在分析分类错误和回归错误的影响后（见表6），我们发现回归头部对YOLOs的性能承担了更大的重要性。因此，我们可以减少分类头部的开销，而不用担心会大幅损害性能。因此，我们简单地采用轻量级的架构用于分类头部，它由两个深度可分离的卷积[25, 9]组成，核大小为3x3，然后是一个1x1卷积。</p>\n<p>（2）空间通道解耦的下采样。YOLO通常利用常规的3x3标准卷积，步长为2，同时实现空间下采样（从H x W到$\\frac{H}{2} \\times \\frac{W}{2}$）和通道变换（从C到2C）。这引入了不可忽视的计算成本，即O(29<em>H<strong>W</strong>C</em>2)，以及参数数量，即O(18$C^2$)。相反，我们提出将空间缩减和通道增加操作解耦，以实现更高效的缩减。具体来说，我们首先利用逐点卷积来调节通道维度，然后利用深度卷积来进行空间缩减。这将计算成本降低到$O(2HWC^2+ \\frac{9}{2}HWC)$，参数数量减少到$O(2C^2+18C)$。同时，在缩减过程中最大化信息保留，从而在延迟减少的同时具有竞争力。</p>\n<p>（3）基于rank引导的模块设计：YOLOs通常在所有阶段使用相同的基本构建块，例如YOLOv8中的瓶颈块。为了彻底检查YOLOs的这种同质设计，我们利用内在秩来分析每个阶段的冗余。具体来说，我们计算每个阶段中最后一个基本块的最后一个卷积的数值秩，这计算了大于阈值的奇异值的数量。图3.(a)展示了YOLOv8的结果，表明深层阶段和大型模型更容易表现出更多的冗余。这一观察表明，简单地为所有阶段应用相同的块设计对于最佳的容量-效率权衡是次优的。为了解决这个问题，我们提出了一种基于秩的块设计方案，旨在通过紧凑的架构设计降低被证明是冗余的阶段复杂度。我们首先提出了一个紧凑的倒置块（CIB）结构，它采用廉价的深度可分离卷积进行空间混合，以及成本效益高的点对点卷积进行通道混合，如图3.(b)所示。它可以作为高效的基本构建块，例如嵌入在ELAN结构中（图3.(b)）。然后，我们提倡一种基于秩的块分配策略，以实现最佳效率，同时保持有竞争力的容量。具体来说，给定一个模型，我们根据其内在秩按升序对所有阶段进行排序。我们进一步检查用CIB替换领先阶段的基本块的性能变化。如果与给定模型相比没有性能下降，我们就继续替换下一个阶段，否则就停止该过程。因此，我们可以在不同阶段和模型规模上实现自适应的紧凑块设计，实现更高的效率而不损害性能。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/0af9b20597598897c15c90a02d5c1093-1733232140019-3.png\" alt=\"img\"></p>\n<h3 id=\"准确性驱动的模型设计。\"><a href=\"#准确性驱动的模型设计。\" class=\"headerlink\" title=\"准确性驱动的模型设计。\"></a>准确性驱动的模型设计。</h3><p>我们进一步探索大核卷积和自注意力在准确性驱动设计中的应用，旨在以最小的成本提升性能。</p>\n<p>（1）大核卷积。采用大核深度卷积是一种有效的方法来扩大感受野并增强模型的能力[10, 40, 39]。然而，简单地在所有阶段都利用它们可能会引入浅层特征中的污染，同时也会在高分辨率阶段引入显著的I&#x2F;O开销和延迟[8]。因此，我们建议在深度阶段利用CIB中的大核深度卷积。具体来说，我们增加了CIB中第二个3x3深度卷积的核大小为7x7，随后[39]。此外，我们采用结构重参数化技术[11,10,59]来引入另一个3x3深度卷积分支，以缓解优化问题而不增加推理开销。此外，随着模型规模的增加，其感受野自然扩大，使用大核卷积的好处逐渐减弱。因此，我们只在小型模型规模上采用大核卷积。</p>\n<p>(2) 部分自注意力(PSA)。自注意力[58]由于其显著的全球建模能力[38, 14, 76]而被广泛应用于各种视觉任务。然而，它表现出高计算复杂性和内存占用。为了解决这个问题，鉴于普遍存在的注意力头重用[69]，我们提出了一个高效的局部自注意力(PSA)模块设计，如图3.(c)所示。具体来说，在1x1卷积之后，我们将通道中的特征均匀划分为两部分。我们只将一部分输入到由多头自注意力模块(MHSA)和前馈网络(FFN)组成的NPSA块中。然后将两部分通过1x1卷积连接并融合。此外，我们遵循[22]的方法，将查询和键的维度分配给MHSA中的值的一半，并用BatchNorm[27]替换LayerNorm[1]以进行快速推理。此外，PSA仅在分辨率最低的第4阶段之后放置，避免了过多的开销。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><h3 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h3><p>我们选择YOLOv8[21]作为我们的基线模型，因为它在延迟准确性和各种模型规模的可用性方面表现良好。我们采用了一致的NMS-free训练双重分配，并基于此进行了全体的效率-准确性驱动的模型设计，从而带来了我们的YOLOv10模型。YOLOv10具有与YOLOv8相同的变体，即N&#x2F;S&#x2F;M&#x2F;L&#x2F;X。此外，我们通过简单增加YOLOv10-M的宽度尺度因子，推导出了一个新的变体YOLOv10-B。我们在相同的全局从零开始设置[21, 65, 62]下，在COCO[35]上验证了所提出的检测器。此外，所有模型的延迟都在T4 GPU和TensorRT FP16上进行测试，遵循[78]的方法。</p>\n<h3 id=\"与最先进技术的比较\"><a href=\"#与最先进技术的比较\" class=\"headerlink\" title=\"与最先进技术的比较\"></a>与最先进技术的比较</h3><p>如表1所示，我们的YOLOv10在各种模型规模上实现了最先进的性能和端到端的延迟。我们首先将YOLOv10与我们基线模型，即YOLOv8进行比较。在N&#x2F;S&#x2F;M&#x2F;L&#x2F;X五种变体中，我们的YOLOv10实现了1.2%&#x2F;1.4%&#x2F;0.5%&#x2F;0.3%&#x2F;0.5%的AP改进，参数减少了28%&#x2F; 36%&#x2F; 41%&#x2F; 44%&#x2F; 57%，计算减少了23%&#x2F; 24%&#x2F; 25%&#x2F; 27%&#x2F; 38%，延迟减少了70%&#x2F;65%&#x2F;50%&#x2F;41%&#x2F;37%。与其他YOLO相比，</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212603338.png\" alt=\"image-20241203212603338\"></p>\n<p>YOLOv10在准确性和计算成本之间也展现了卓越的权衡。具体来说，对于轻量级和小型的模型，YOLOv10-N&#x2F;S的性能超过了YOLOv6-3.0-N&#x2F;S，分别提高了1.5 AP和2.0 AP，参数减少了51%，计算量减少了41%。对于中等规模的模型，与YOLOv9-C&#x2F; YOLO-MS相比，YOLOv10-B&#x2F;M在相同或更好的性能下，延迟降低了46%&#x2F;62%。对于大型模型，与Gold-YOLO-L相比，我们的YOLOv10-L在参数减少了68%，延迟降低了32%，并且AP显著提高了1.4%。此外，与RT-DETR相比，YOLOv10获得了显著的性能和延迟提升。值得注意的是，在相似的性能下，YOLOv10-S&#x2F;X的推理速度比RT-DETR-R18&#x2F;R101快了1.8倍和1.3倍。这些结果充分展示了YOLOv10作为实时端到端检测器的优越性。</p>\n<p>我们还使用原始的一对多训练方法将YOLOv10与其他YOLO进行了比较。在这种情况下，我们考虑了模型前向过程（Latencyf）的性能和延迟[62, 21, 60]。如表1所示，YOLOv10在不同模型规模上也展现了最先进的表现和效率，这表明了我们架构设计的有效性。</p>\n<h3 id=\"模型分析\"><a href=\"#模型分析\" class=\"headerlink\" title=\"模型分析\"></a>模型分析</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212826090.png\" alt=\"image-20241203212826090\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213021438.png\" alt=\"image-20241203213021438\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213123173.png\" alt=\"image-20241203213123173\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213200887.png\" alt=\"image-20241203213200887\"></p>\n<ol>\n<li><p><strong>消融研究</strong>：基于YOLOv10-S和YOLOv10-M的消融研究表明，无NMS训练结合一致的双标签分配显著降低了YOLOv10-S的端到端延迟，同时保持了44.3%的AP竞争力。此外，效率驱动的设计减少了参数数量和计算量，并显著降低了延迟。</p>\n</li>\n<li><p><strong>双标签分配</strong>：双标签分配为无NMS的YOLO提供了丰富的监督信息，并在推理时实现了高效性。一致匹配度量的引入进一步缩小了两个分支之间的监督差距，提高了性能。</p>\n</li>\n<li><p><strong>效率驱动模型设计</strong>：效率驱动模型设计通过轻量级分类头、空间-通道解耦下采样和紧凑倒置块（CIB）等组件，有效减少了参数数量、FLOPs和延迟，同时保持了竞争性的性能。</p>\n</li>\n<li><p><strong>准确性驱动模型设计</strong>：准确性驱动模型设计通过大核卷积和部分自注意力（PSA）模块，在不显著增加延迟的情况下提高了性能。</p>\n</li>\n<li><p><strong>大核卷积</strong>：大核卷积的使用扩大了感受野并增强了模型能力，但在小模型中效果更佳。</p>\n</li>\n<li><p><strong>部分自注意力模块</strong>：PSA模块通过减少自注意力头中的冗余来缓解优化问题，从而在不牺牲高效率的情况下提升了模型性能。</p>\n</li>\n</ol>\n<h1 id=\"YOLOv10代码\"><a href=\"#YOLOv10代码\" class=\"headerlink\" title=\"YOLOv10代码\"></a>YOLOv10代码</h1><h3 id=\"C2fUIB介绍\"><a href=\"#C2fUIB介绍\" class=\"headerlink\" title=\"C2fUIB介绍\"></a>C2fUIB介绍</h3><p><strong>C2fUIB只是用CIB结构替换了YOLOv8中 C2f的Bottleneck结构</strong></p>\n<p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/8ed70c479c7530fe3d36f4f44fbbf2d8.png\" alt=\"img\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/a3198a08d3b755ec2e1e85cb8979c2ee.png\" alt=\"img\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CIB</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Standard bottleneck.&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, c1, c2, shortcut=<span class=\"literal\">True</span>, e=<span class=\"number\">0.5</span>, lk=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and</span></span><br><span class=\"line\"><span class=\"string\">        expansion.</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        c_ = <span class=\"built_in\">int</span>(c2 * e)  <span class=\"comment\"># hidden channels</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.cv1 = nn.Sequential(</span><br><span class=\"line\">            Conv(c1, c1, <span class=\"number\">3</span>, g=c1),</span><br><span class=\"line\">            Conv(c1, <span class=\"number\">2</span> * c_, <span class=\"number\">1</span>),</span><br><span class=\"line\">            Conv(<span class=\"number\">2</span> * c_, <span class=\"number\">2</span> * c_, <span class=\"number\">3</span>, g=<span class=\"number\">2</span> * c_) <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> lk <span class=\"keyword\">else</span> RepVGGDW(<span class=\"number\">2</span> * c_),</span><br><span class=\"line\">            Conv(<span class=\"number\">2</span> * c_, c2, <span class=\"number\">1</span>),</span><br><span class=\"line\">            Conv(c2, c2, <span class=\"number\">3</span>, g=c2),</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.add = shortcut <span class=\"keyword\">and</span> c1 == c2</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;&#x27;forward()&#x27; applies the YOLO FPN to input data.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> x + <span class=\"variable language_\">self</span>.cv1(x) <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.add <span class=\"keyword\">else</span> <span class=\"variable language_\">self</span>.cv1(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">C2fCIB</span>(<span class=\"title class_ inherited__\">C2f</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Faster Implementation of CSP Bottleneck with 2 convolutions.&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, c1, c2, n=<span class=\"number\">1</span>, shortcut=<span class=\"literal\">False</span>, lk=<span class=\"literal\">False</span>, g=<span class=\"number\">1</span>, e=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,</span></span><br><span class=\"line\"><span class=\"string\">        expansion.</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(c1, c2, n, shortcut, g, e)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.m = nn.ModuleList(CIB(<span class=\"variable language_\">self</span>.c, <span class=\"variable language_\">self</span>.c, shortcut, e=<span class=\"number\">1.0</span>, lk=lk) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"PSA介绍\"><a href=\"#PSA介绍\" class=\"headerlink\" title=\"PSA介绍\"></a>PSA介绍</h3><p>具体来说，我们在1×1卷积后将特征均匀地分为两部分。我们只将一部分输入到由多头自注意力模块（MHSA）和前馈网络（FFN）组成的NPSA块中。然后，两部分通过1×1卷积连接并融合。此外，遵循将查询和键的维度分配为值的一半，并用BatchNorm替换LayerNorm以实现快速推理。</p>\n<p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/82d0ed0834ac712354683efcb0108bc6.png\" alt=\"img\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Attention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dim, num_heads=<span class=\"number\">8</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 attn_ratio=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.num_heads = num_heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.head_dim = dim // num_heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.key_dim = <span class=\"built_in\">int</span>(<span class=\"variable language_\">self</span>.head_dim * attn_ratio)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.scale = <span class=\"variable language_\">self</span>.key_dim ** -<span class=\"number\">0.5</span></span><br><span class=\"line\">        nh_kd = nh_kd = <span class=\"variable language_\">self</span>.key_dim * num_heads</span><br><span class=\"line\">        h = dim + nh_kd * <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.qkv = Conv(dim, h, <span class=\"number\">1</span>, act=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.proj = Conv(dim, dim, <span class=\"number\">1</span>, act=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pe = Conv(dim, dim, <span class=\"number\">3</span>, <span class=\"number\">1</span>, g=dim, act=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        B, _, H, W = x.shape</span><br><span class=\"line\">        N = H * W</span><br><span class=\"line\">        qkv = <span class=\"variable language_\">self</span>.qkv(x)</span><br><span class=\"line\">        q, k, v = qkv.view(B, <span class=\"variable language_\">self</span>.num_heads, -<span class=\"number\">1</span>, N).split([<span class=\"variable language_\">self</span>.key_dim, <span class=\"variable language_\">self</span>.key_dim, <span class=\"variable language_\">self</span>.head_dim], dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn = (</span><br><span class=\"line\">            (q.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>) @ k) * <span class=\"variable language_\">self</span>.scale</span><br><span class=\"line\">        )</span><br><span class=\"line\">        attn = attn.softmax(dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = (v @ attn.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)).view(B, -<span class=\"number\">1</span>, H, W) + <span class=\"variable language_\">self</span>.pe(v.reshape(B, -<span class=\"number\">1</span>, H, W))</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.proj(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PSA</span>(nn.Module):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, c1, c2, e=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(c1 == c2)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.c = <span class=\"built_in\">int</span>(c1 * e)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.cv1 = Conv(c1, <span class=\"number\">2</span> * <span class=\"variable language_\">self</span>.c, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.cv2 = Conv(<span class=\"number\">2</span> * <span class=\"variable language_\">self</span>.c, c1, <span class=\"number\">1</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.attn = Attention(<span class=\"variable language_\">self</span>.c, attn_ratio=<span class=\"number\">0.5</span>, num_heads=<span class=\"variable language_\">self</span>.c // <span class=\"number\">64</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.ffn = nn.Sequential(</span><br><span class=\"line\">            Conv(<span class=\"variable language_\">self</span>.c, <span class=\"variable language_\">self</span>.c*<span class=\"number\">2</span>, <span class=\"number\">1</span>),</span><br><span class=\"line\">            Conv(<span class=\"variable language_\">self</span>.c*<span class=\"number\">2</span>, <span class=\"variable language_\">self</span>.c, <span class=\"number\">1</span>, act=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        )</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        a, b = <span class=\"variable language_\">self</span>.cv1(x).split((<span class=\"variable language_\">self</span>.c, <span class=\"variable language_\">self</span>.c), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        b = b + <span class=\"variable language_\">self</span>.attn(b)</span><br><span class=\"line\">        b = b + <span class=\"variable language_\">self</span>.ffn(b)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.cv2(torch.cat((a, b), <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"SCDown\"><a href=\"#SCDown\" class=\"headerlink\" title=\"SCDown\"></a>SCDown</h3><p>OLOs通常利用常规的3×3标准卷积，步长为2，同时实现空间下采样（从H×W到H&#x2F;2×W&#x2F;2）和通道变换（从C到2C）。这引入了不可忽视的计算成本$O(9HWC^2)$和参数数量O$(18C^2)$。相反，我们提议将空间缩减和通道增加操作解耦，以实现更高效的下采样。具体来说，我们首先利用点对点卷积来调整通道维度，然后利用深度可分离卷积进行空间下采样。这将计算成本降低到O(2HWC^2 + 9HWC)，并将参数数量减少到O(2C^2 + 18C)。同时，它最大限度地保留了下采样过程中的信息，从而在减少延迟的同时保持了有竞争力的性能。</p>\n<p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class SCDown(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, k, s):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.cv1 = Conv(c1, c2, 1, 1)</span><br><span class=\"line\">        self.cv2 = Conv(c2, c2, k=k, s=s, g=c2, act=False)</span><br><span class=\"line\"></span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        return self.cv2(self.cv1(x))</span><br></pre></td></tr></table></figure>\n\n\n\n<p>参考：<a href=\"https://cloud.tencent.com/developer/article/2426044\">YOLOv10真正实时端到端目标检测（原理介绍+代码详见+结构框图）-腾讯云开发者社区-腾讯云</a></p>\n<p>文章合集：<a href=\"https://github.com/chongzicbo/ReadWriteThink\">https://github.com/chongzicbo/ReadWriteThink</a></p>\n","excerpt":"","more":"<h1 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h1><h3 id=\"背景介绍\"><a href=\"#背景介绍\" class=\"headerlink\" title=\"背景介绍\"></a>背景介绍</h3><p>实时物体检测一直是计算机视觉领域的研究热点，旨在低延迟下准确预测图像中物体的类别和位置。该技术广泛应用于自动驾驶、机器人导航和物体跟踪等实际应用中。近年来，基于卷积神经网络（CNN）的物体检测器因其高效的性能而受到广泛关注，其中YOLO系列因其出色的性能和效率平衡而脱颖而出。</p>\n<h3 id=\"研究内容\"><a href=\"#研究内容\" class=\"headerlink\" title=\"研究内容\"></a>研究内容</h3><p>本文旨在解决YOLO系列在实际部署中依赖非极大值抑制（NMS）导致的推理延迟问题，并通过优化模型架构进一步提升其性能和效率。具体来说，本文提出了以下两个主要目标：</p>\n<ol>\n<li>提出一种无需NMS训练的一致双分配策略，以实现高效的端到端检测。</li>\n<li>提出一种全面的效率-准确性驱动的模型设计策略，从后处理和模型架构两方面提升YOLO的性能和效率。</li>\n</ol>\n<h3 id=\"研究难点\"><a href=\"#研究难点\" class=\"headerlink\" title=\"研究难点\"></a>研究难点</h3><p>YOLO系列在实际应用中面临的主要挑战包括：</p>\n<ol>\n<li><strong>NMS依赖性</strong>：传统的YOLO训练过程中采用一对多标签分配策略，导致推理过程中需要依赖NMS进行后处理，这不仅增加了推理延迟，还使得模型对NMS超参数敏感，难以实现最优的端到端部署。</li>\n<li><strong>模型架构设计</strong>：尽管已有大量研究探索了不同的模型架构设计策略，但YOLO系列在各个组件的设计上仍存在计算冗余，限制了模型的性能和效率。</li>\n</ol>\n<h3 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h3><p>本文回顾了现有的实时物体检测器和端到端物体检测器的相关研究：</p>\n<ol>\n<li><strong>传统YOLO系列</strong>：YOLOv1、YOLOv2和YOLOv3是典型的三部分检测架构，包括主干、颈部和头部。后续的YOLOv4和YOLOv5引入了CSPNet设计，并结合数据增强策略和多种模型尺度。YOLOv6、YOLOv7和YOLOv8分别提出了BiC、E-ELAN和C2f等新的组件设计。</li>\n<li><strong>端到端物体检测器</strong>：DETR系列通过引入Transformer架构和匈牙利损失实现了一对一匹配预测，消除了手工设计的组件和后处理。其他研究如Learnable NMS和关系网络也尝试通过不同的方法实现端到端检测。</li>\n</ol>\n<h3 id=\"研究方法\"><a href=\"#研究方法\" class=\"headerlink\" title=\"研究方法\"></a>研究方法</h3><p>本文提出了一种无需NMS训练的一致双分配策略和全面的效率-准确性驱动的模型设计策略：</p>\n<ol>\n<li><strong>一致双分配策略</strong>：通过引入双标签分配和一致的匹配度量，结合一对多和一对一标签分配的优势，实现高效的端到端检测。</li>\n<li><strong>效率-准确性驱动的模型设计策略</strong>：从模型架构的各个组件入手，提出了轻量级分类头、空间-通道解耦下采样和排名引导的块设计等优化方法，减少计算冗余并提升模型性能。</li>\n</ol>\n<h3 id=\"实验设计\"><a href=\"#实验设计\" class=\"headerlink\" title=\"实验设计\"></a>实验设计</h3><p>本文在COCO数据集上对提出的YOLOv10模型进行了广泛的实验验证，具体包括：</p>\n<ol>\n<li><strong>数据集</strong>：使用COCO数据集进行训练和评估，采用标准的训练-验证-测试划分。</li>\n<li><strong>实验设置</strong>：所有模型在8块NVIDIA 3090 GPU上进行训练，采用SGD优化器，并结合Mosaic、Mixup和复制粘贴等数据增强策略。</li>\n<li><strong>评估指标</strong>：使用标准平均精度（AP）和不同IoU阈值下的AP值评估模型性能，并测量推理延迟以评估效率。</li>\n</ol>\n<h3 id=\"结果与分析\"><a href=\"#结果与分析\" class=\"headerlink\" title=\"结果与分析\"></a>结果与分析</h3><p>实验结果表明，YOLOv10在多个模型尺度上均取得了显著的性能和效率提升：</p>\n<ol>\n<li><strong>性能提升</strong>：YOLOv10-S在相似的AP下比RT-DETR-R18快1.8倍，YOLOv10-B在相同性能下比YOLOv9-C减少了46%的延迟。</li>\n<li><strong>参数和计算量减少</strong>：YOLOv10-S和YOLOv10-B分别比RT-DETR-R18和YOLOv9-C减少了2.8倍和25%的参数数量和FLOPs。</li>\n<li><strong>全面优势</strong>：YOLOv10在多个模型尺度上均优于现有先进模型，展示了其在计算-准确性权衡上的优越性。</li>\n</ol>\n<h3 id=\"总体结论\"><a href=\"#总体结论\" class=\"headerlink\" title=\"总体结论\"></a>总体结论</h3><p>本文通过提出一致双分配策略和全面的效率-准确性驱动的模型设计策略，成功提升了YOLO系列的性能和效率。实验结果验证了YOLOv10在多个模型尺度上的优越性，展示了其在实时物体检测领域的潜力。未来的工作将进一步探索减少小模型中一对多训练和无需NMS训练之间的性能差距的方法。</p>\n<h1 id=\"研究方法-1\"><a href=\"#研究方法-1\" class=\"headerlink\" title=\"研究方法\"></a>研究方法</h1><h3 id=\"无NMS训练的一致双重分配\"><a href=\"#无NMS训练的一致双重分配\" class=\"headerlink\" title=\"无NMS训练的一致双重分配\"></a>无NMS训练的一致双重分配</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/5f60bbd9bccdfacc4ce2a53bce9011dc-image.png\" alt=\"img\"></p>\n<ul>\n<li><p>双重标签分配。</p>\n<p>为了实现无需NMS的训练，论文提出了一致的双重分配策略。该策略结合一对多和多对一的标签分配策略的优点。具体来说，双重分配策略包括一个多对一的头和一个一对一的头。在训练过程中，两个头共同优化，使得主干和颈部能够享受多对一分配提供的丰富监督信号。在推理过程中，丢弃多对一头，只使用一对一头来做出预测，从而实现无需NMS的高效端到端部署。</p>\n</li>\n<li><p>一致的匹配度量。</p>\n</li>\n</ul>\n<p>为了确保两个头之间的和谐监督，论文提出了一致的匹配度量。该度量公式如下：<br>$$<br>m(\\alpha,\\beta)&#x3D;s\\cdot p^{\\alpha}\\cdot IoU(\\hat{b},b)^{\\beta}<br>$$<br>其中，<em>p</em> 是分类分数，$\\hat{b}$ 和 <em>b</em> 分别表示预测和实例的边界框，<em>s</em> 表示空间先验，指示预测的锚点是否在实例内，<em>α</em> 和 <em>β</em> 是两个重要的超参数，平衡语义预测任务和位置回归任务的影响。</p>\n<h3 id=\"全局效率-准确性驱动的模型设计\"><a href=\"#全局效率-准确性驱动的模型设计\" class=\"headerlink\" title=\"全局效率-准确性驱动的模型设计\"></a>全局效率-准确性驱动的模型设计</h3><p>除了后处理，YOLOs的模型架构也对效率-准确性权衡提出了巨大挑战[50, 8, 29]。尽管以前的工作探索了各种设计策略，但仍然缺乏对YOLOs中各个组件的全面检查。因此，模型架构展现出不可忽视的计算冗余和受限能力，这阻碍了其实现高效率和性能的潜力。在这里，我们旨在从效率和准确性角度全面执行YOLOs的模型设计。</p>\n<p>效率驱动的模型设计。YOLO中的组件包括茎(stem)、下采样层、带有基本构建块的阶段以及头部。茎的计算成本较低，因此我们对其他三个部分采用效率驱动的模型设计。</p>\n<p>（1）轻量级分类头部。在YOLO中，分类和回归头部通常共享相同的架构。然而，它们在计算开销上表现出显著的差异。例如，在YOLOv8-S中，分类头部的FLOPs和参数数量分别为回归头部的2.5倍和2.4倍。然而，在分析分类错误和回归错误的影响后（见表6），我们发现回归头部对YOLOs的性能承担了更大的重要性。因此，我们可以减少分类头部的开销，而不用担心会大幅损害性能。因此，我们简单地采用轻量级的架构用于分类头部，它由两个深度可分离的卷积[25, 9]组成，核大小为3x3，然后是一个1x1卷积。</p>\n<p>（2）空间通道解耦的下采样。YOLO通常利用常规的3x3标准卷积，步长为2，同时实现空间下采样（从H x W到$\\frac{H}{2} \\times \\frac{W}{2}$）和通道变换（从C到2C）。这引入了不可忽视的计算成本，即O(29<em>H<strong>W</strong>C</em>2)，以及参数数量，即O(18$C^2$)。相反，我们提出将空间缩减和通道增加操作解耦，以实现更高效的缩减。具体来说，我们首先利用逐点卷积来调节通道维度，然后利用深度卷积来进行空间缩减。这将计算成本降低到$O(2HWC^2+ \\frac{9}{2}HWC)$，参数数量减少到$O(2C^2+18C)$。同时，在缩减过程中最大化信息保留，从而在延迟减少的同时具有竞争力。</p>\n<p>（3）基于rank引导的模块设计：YOLOs通常在所有阶段使用相同的基本构建块，例如YOLOv8中的瓶颈块。为了彻底检查YOLOs的这种同质设计，我们利用内在秩来分析每个阶段的冗余。具体来说，我们计算每个阶段中最后一个基本块的最后一个卷积的数值秩，这计算了大于阈值的奇异值的数量。图3.(a)展示了YOLOv8的结果，表明深层阶段和大型模型更容易表现出更多的冗余。这一观察表明，简单地为所有阶段应用相同的块设计对于最佳的容量-效率权衡是次优的。为了解决这个问题，我们提出了一种基于秩的块设计方案，旨在通过紧凑的架构设计降低被证明是冗余的阶段复杂度。我们首先提出了一个紧凑的倒置块（CIB）结构，它采用廉价的深度可分离卷积进行空间混合，以及成本效益高的点对点卷积进行通道混合，如图3.(b)所示。它可以作为高效的基本构建块，例如嵌入在ELAN结构中（图3.(b)）。然后，我们提倡一种基于秩的块分配策略，以实现最佳效率，同时保持有竞争力的容量。具体来说，给定一个模型，我们根据其内在秩按升序对所有阶段进行排序。我们进一步检查用CIB替换领先阶段的基本块的性能变化。如果与给定模型相比没有性能下降，我们就继续替换下一个阶段，否则就停止该过程。因此，我们可以在不同阶段和模型规模上实现自适应的紧凑块设计，实现更高的效率而不损害性能。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/0af9b20597598897c15c90a02d5c1093-1733232140019-3.png\" alt=\"img\"></p>\n<h3 id=\"准确性驱动的模型设计。\"><a href=\"#准确性驱动的模型设计。\" class=\"headerlink\" title=\"准确性驱动的模型设计。\"></a>准确性驱动的模型设计。</h3><p>我们进一步探索大核卷积和自注意力在准确性驱动设计中的应用，旨在以最小的成本提升性能。</p>\n<p>（1）大核卷积。采用大核深度卷积是一种有效的方法来扩大感受野并增强模型的能力[10, 40, 39]。然而，简单地在所有阶段都利用它们可能会引入浅层特征中的污染，同时也会在高分辨率阶段引入显著的I&#x2F;O开销和延迟[8]。因此，我们建议在深度阶段利用CIB中的大核深度卷积。具体来说，我们增加了CIB中第二个3x3深度卷积的核大小为7x7，随后[39]。此外，我们采用结构重参数化技术[11,10,59]来引入另一个3x3深度卷积分支，以缓解优化问题而不增加推理开销。此外，随着模型规模的增加，其感受野自然扩大，使用大核卷积的好处逐渐减弱。因此，我们只在小型模型规模上采用大核卷积。</p>\n<p>(2) 部分自注意力(PSA)。自注意力[58]由于其显著的全球建模能力[38, 14, 76]而被广泛应用于各种视觉任务。然而，它表现出高计算复杂性和内存占用。为了解决这个问题，鉴于普遍存在的注意力头重用[69]，我们提出了一个高效的局部自注意力(PSA)模块设计，如图3.(c)所示。具体来说，在1x1卷积之后，我们将通道中的特征均匀划分为两部分。我们只将一部分输入到由多头自注意力模块(MHSA)和前馈网络(FFN)组成的NPSA块中。然后将两部分通过1x1卷积连接并融合。此外，我们遵循[22]的方法，将查询和键的维度分配给MHSA中的值的一半，并用BatchNorm[27]替换LayerNorm[1]以进行快速推理。此外，PSA仅在分辨率最低的第4阶段之后放置，避免了过多的开销。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><h3 id=\"实现细节\"><a href=\"#实现细节\" class=\"headerlink\" title=\"实现细节\"></a>实现细节</h3><p>我们选择YOLOv8[21]作为我们的基线模型，因为它在延迟准确性和各种模型规模的可用性方面表现良好。我们采用了一致的NMS-free训练双重分配，并基于此进行了全体的效率-准确性驱动的模型设计，从而带来了我们的YOLOv10模型。YOLOv10具有与YOLOv8相同的变体，即N&#x2F;S&#x2F;M&#x2F;L&#x2F;X。此外，我们通过简单增加YOLOv10-M的宽度尺度因子，推导出了一个新的变体YOLOv10-B。我们在相同的全局从零开始设置[21, 65, 62]下，在COCO[35]上验证了所提出的检测器。此外，所有模型的延迟都在T4 GPU和TensorRT FP16上进行测试，遵循[78]的方法。</p>\n<h3 id=\"与最先进技术的比较\"><a href=\"#与最先进技术的比较\" class=\"headerlink\" title=\"与最先进技术的比较\"></a>与最先进技术的比较</h3><p>如表1所示，我们的YOLOv10在各种模型规模上实现了最先进的性能和端到端的延迟。我们首先将YOLOv10与我们基线模型，即YOLOv8进行比较。在N&#x2F;S&#x2F;M&#x2F;L&#x2F;X五种变体中，我们的YOLOv10实现了1.2%&#x2F;1.4%&#x2F;0.5%&#x2F;0.3%&#x2F;0.5%的AP改进，参数减少了28%&#x2F; 36%&#x2F; 41%&#x2F; 44%&#x2F; 57%，计算减少了23%&#x2F; 24%&#x2F; 25%&#x2F; 27%&#x2F; 38%，延迟减少了70%&#x2F;65%&#x2F;50%&#x2F;41%&#x2F;37%。与其他YOLO相比，</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212603338.png\" alt=\"image-20241203212603338\"></p>\n<p>YOLOv10在准确性和计算成本之间也展现了卓越的权衡。具体来说，对于轻量级和小型的模型，YOLOv10-N&#x2F;S的性能超过了YOLOv6-3.0-N&#x2F;S，分别提高了1.5 AP和2.0 AP，参数减少了51%，计算量减少了41%。对于中等规模的模型，与YOLOv9-C&#x2F; YOLO-MS相比，YOLOv10-B&#x2F;M在相同或更好的性能下，延迟降低了46%&#x2F;62%。对于大型模型，与Gold-YOLO-L相比，我们的YOLOv10-L在参数减少了68%，延迟降低了32%，并且AP显著提高了1.4%。此外，与RT-DETR相比，YOLOv10获得了显著的性能和延迟提升。值得注意的是，在相似的性能下，YOLOv10-S&#x2F;X的推理速度比RT-DETR-R18&#x2F;R101快了1.8倍和1.3倍。这些结果充分展示了YOLOv10作为实时端到端检测器的优越性。</p>\n<p>我们还使用原始的一对多训练方法将YOLOv10与其他YOLO进行了比较。在这种情况下，我们考虑了模型前向过程（Latencyf）的性能和延迟[62, 21, 60]。如表1所示，YOLOv10在不同模型规模上也展现了最先进的表现和效率，这表明了我们架构设计的有效性。</p>\n<h3 id=\"模型分析\"><a href=\"#模型分析\" class=\"headerlink\" title=\"模型分析\"></a>模型分析</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212826090.png\" alt=\"image-20241203212826090\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213021438.png\" alt=\"image-20241203213021438\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213123173.png\" alt=\"image-20241203213123173\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213200887.png\" alt=\"image-20241203213200887\"></p>\n<ol>\n<li><p><strong>消融研究</strong>：基于YOLOv10-S和YOLOv10-M的消融研究表明，无NMS训练结合一致的双标签分配显著降低了YOLOv10-S的端到端延迟，同时保持了44.3%的AP竞争力。此外，效率驱动的设计减少了参数数量和计算量，并显著降低了延迟。</p>\n</li>\n<li><p><strong>双标签分配</strong>：双标签分配为无NMS的YOLO提供了丰富的监督信息，并在推理时实现了高效性。一致匹配度量的引入进一步缩小了两个分支之间的监督差距，提高了性能。</p>\n</li>\n<li><p><strong>效率驱动模型设计</strong>：效率驱动模型设计通过轻量级分类头、空间-通道解耦下采样和紧凑倒置块（CIB）等组件，有效减少了参数数量、FLOPs和延迟，同时保持了竞争性的性能。</p>\n</li>\n<li><p><strong>准确性驱动模型设计</strong>：准确性驱动模型设计通过大核卷积和部分自注意力（PSA）模块，在不显著增加延迟的情况下提高了性能。</p>\n</li>\n<li><p><strong>大核卷积</strong>：大核卷积的使用扩大了感受野并增强了模型能力，但在小模型中效果更佳。</p>\n</li>\n<li><p><strong>部分自注意力模块</strong>：PSA模块通过减少自注意力头中的冗余来缓解优化问题，从而在不牺牲高效率的情况下提升了模型性能。</p>\n</li>\n</ol>\n<h1 id=\"YOLOv10代码\"><a href=\"#YOLOv10代码\" class=\"headerlink\" title=\"YOLOv10代码\"></a>YOLOv10代码</h1><h3 id=\"C2fUIB介绍\"><a href=\"#C2fUIB介绍\" class=\"headerlink\" title=\"C2fUIB介绍\"></a>C2fUIB介绍</h3><p><strong>C2fUIB只是用CIB结构替换了YOLOv8中 C2f的Bottleneck结构</strong></p>\n<p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/8ed70c479c7530fe3d36f4f44fbbf2d8.png\" alt=\"img\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/a3198a08d3b755ec2e1e85cb8979c2ee.png\" alt=\"img\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CIB</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Standard bottleneck.&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, c1, c2, shortcut=<span class=\"literal\">True</span>, e=<span class=\"number\">0.5</span>, lk=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and</span></span><br><span class=\"line\"><span class=\"string\">        expansion.</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        c_ = <span class=\"built_in\">int</span>(c2 * e)  <span class=\"comment\"># hidden channels</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.cv1 = nn.Sequential(</span><br><span class=\"line\">            Conv(c1, c1, <span class=\"number\">3</span>, g=c1),</span><br><span class=\"line\">            Conv(c1, <span class=\"number\">2</span> * c_, <span class=\"number\">1</span>),</span><br><span class=\"line\">            Conv(<span class=\"number\">2</span> * c_, <span class=\"number\">2</span> * c_, <span class=\"number\">3</span>, g=<span class=\"number\">2</span> * c_) <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> lk <span class=\"keyword\">else</span> RepVGGDW(<span class=\"number\">2</span> * c_),</span><br><span class=\"line\">            Conv(<span class=\"number\">2</span> * c_, c2, <span class=\"number\">1</span>),</span><br><span class=\"line\">            Conv(c2, c2, <span class=\"number\">3</span>, g=c2),</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.add = shortcut <span class=\"keyword\">and</span> c1 == c2</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;&#x27;forward()&#x27; applies the YOLO FPN to input data.&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> x + <span class=\"variable language_\">self</span>.cv1(x) <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.add <span class=\"keyword\">else</span> <span class=\"variable language_\">self</span>.cv1(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">C2fCIB</span>(<span class=\"title class_ inherited__\">C2f</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Faster Implementation of CSP Bottleneck with 2 convolutions.&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, c1, c2, n=<span class=\"number\">1</span>, shortcut=<span class=\"literal\">False</span>, lk=<span class=\"literal\">False</span>, g=<span class=\"number\">1</span>, e=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,</span></span><br><span class=\"line\"><span class=\"string\">        expansion.</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(c1, c2, n, shortcut, g, e)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.m = nn.ModuleList(CIB(<span class=\"variable language_\">self</span>.c, <span class=\"variable language_\">self</span>.c, shortcut, e=<span class=\"number\">1.0</span>, lk=lk) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"PSA介绍\"><a href=\"#PSA介绍\" class=\"headerlink\" title=\"PSA介绍\"></a>PSA介绍</h3><p>具体来说，我们在1×1卷积后将特征均匀地分为两部分。我们只将一部分输入到由多头自注意力模块（MHSA）和前馈网络（FFN）组成的NPSA块中。然后，两部分通过1×1卷积连接并融合。此外，遵循将查询和键的维度分配为值的一半，并用BatchNorm替换LayerNorm以实现快速推理。</p>\n<p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/82d0ed0834ac712354683efcb0108bc6.png\" alt=\"img\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Attention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dim, num_heads=<span class=\"number\">8</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 attn_ratio=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.num_heads = num_heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.head_dim = dim // num_heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.key_dim = <span class=\"built_in\">int</span>(<span class=\"variable language_\">self</span>.head_dim * attn_ratio)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.scale = <span class=\"variable language_\">self</span>.key_dim ** -<span class=\"number\">0.5</span></span><br><span class=\"line\">        nh_kd = nh_kd = <span class=\"variable language_\">self</span>.key_dim * num_heads</span><br><span class=\"line\">        h = dim + nh_kd * <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.qkv = Conv(dim, h, <span class=\"number\">1</span>, act=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.proj = Conv(dim, dim, <span class=\"number\">1</span>, act=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pe = Conv(dim, dim, <span class=\"number\">3</span>, <span class=\"number\">1</span>, g=dim, act=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        B, _, H, W = x.shape</span><br><span class=\"line\">        N = H * W</span><br><span class=\"line\">        qkv = <span class=\"variable language_\">self</span>.qkv(x)</span><br><span class=\"line\">        q, k, v = qkv.view(B, <span class=\"variable language_\">self</span>.num_heads, -<span class=\"number\">1</span>, N).split([<span class=\"variable language_\">self</span>.key_dim, <span class=\"variable language_\">self</span>.key_dim, <span class=\"variable language_\">self</span>.head_dim], dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        attn = (</span><br><span class=\"line\">            (q.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>) @ k) * <span class=\"variable language_\">self</span>.scale</span><br><span class=\"line\">        )</span><br><span class=\"line\">        attn = attn.softmax(dim=-<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = (v @ attn.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>)).view(B, -<span class=\"number\">1</span>, H, W) + <span class=\"variable language_\">self</span>.pe(v.reshape(B, -<span class=\"number\">1</span>, H, W))</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.proj(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PSA</span>(nn.Module):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, c1, c2, e=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(c1 == c2)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.c = <span class=\"built_in\">int</span>(c1 * e)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.cv1 = Conv(c1, <span class=\"number\">2</span> * <span class=\"variable language_\">self</span>.c, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.cv2 = Conv(<span class=\"number\">2</span> * <span class=\"variable language_\">self</span>.c, c1, <span class=\"number\">1</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.attn = Attention(<span class=\"variable language_\">self</span>.c, attn_ratio=<span class=\"number\">0.5</span>, num_heads=<span class=\"variable language_\">self</span>.c // <span class=\"number\">64</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.ffn = nn.Sequential(</span><br><span class=\"line\">            Conv(<span class=\"variable language_\">self</span>.c, <span class=\"variable language_\">self</span>.c*<span class=\"number\">2</span>, <span class=\"number\">1</span>),</span><br><span class=\"line\">            Conv(<span class=\"variable language_\">self</span>.c*<span class=\"number\">2</span>, <span class=\"variable language_\">self</span>.c, <span class=\"number\">1</span>, act=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        )</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        a, b = <span class=\"variable language_\">self</span>.cv1(x).split((<span class=\"variable language_\">self</span>.c, <span class=\"variable language_\">self</span>.c), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        b = b + <span class=\"variable language_\">self</span>.attn(b)</span><br><span class=\"line\">        b = b + <span class=\"variable language_\">self</span>.ffn(b)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.cv2(torch.cat((a, b), <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"SCDown\"><a href=\"#SCDown\" class=\"headerlink\" title=\"SCDown\"></a>SCDown</h3><p>OLOs通常利用常规的3×3标准卷积，步长为2，同时实现空间下采样（从H×W到H&#x2F;2×W&#x2F;2）和通道变换（从C到2C）。这引入了不可忽视的计算成本$O(9HWC^2)$和参数数量O$(18C^2)$。相反，我们提议将空间缩减和通道增加操作解耦，以实现更高效的下采样。具体来说，我们首先利用点对点卷积来调整通道维度，然后利用深度可分离卷积进行空间下采样。这将计算成本降低到O(2HWC^2 + 9HWC)，并将参数数量减少到O(2C^2 + 18C)。同时，它最大限度地保留了下采样过程中的信息，从而在减少延迟的同时保持了有竞争力的性能。</p>\n<p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class SCDown(nn.Module):</span><br><span class=\"line\">    def __init__(self, c1, c2, k, s):</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.cv1 = Conv(c1, c2, 1, 1)</span><br><span class=\"line\">        self.cv2 = Conv(c2, c2, k=k, s=s, g=c2, act=False)</span><br><span class=\"line\"></span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        return self.cv2(self.cv1(x))</span><br></pre></td></tr></table></figure>\n\n\n\n<p>参考：<a href=\"https://cloud.tencent.com/developer/article/2426044\">YOLOv10真正实时端到端目标检测（原理介绍+代码详见+结构框图）-腾讯云开发者社区-腾讯云</a></p>\n<p>文章合集：<a href=\"https://github.com/chongzicbo/ReadWriteThink\">https://github.com/chongzicbo/ReadWriteThink</a></p>\n"},{"title":"YOLO模型的全面综述","date":"2024-12-07T10:30:00.000Z","_content":"\n\n\n# 摘要\n\n本研究对YOLO （You Only Look Once） 的各个版本进行了全面的基准测试分析，从 YOLOv3 到最新的算法。它代表了首次全面评估 YOLO11 性能的研究，YOLO11 是 YOLO 系列的最新成员。它评估了它们在三个不同数据集上的性能：交通标志（具有不同的对象大小）、非洲野生动物（具有不同的纵横比，每个图像至少有一个对象实例）以及船舶和船只（具有单个类别的小型对象），确保在具有不同挑战的数据集之间进行全面评估。为了确保稳健的评估，我们采用了一套全面的指标，包括精度、召回率、平均精度均值 （mAP）、处理时间、GFLOP 计数和模型大小。我们的分析强调了每个 YOLO 版本的独特优势和局限性。例如：YOLOv9 表现出很高的准确性，但在检测小物体和效率方面表现不佳，而 YOLOv10 表现出相对较低的准确性，因为架构选择会影响其在重叠物体检测方面的性能，但在速度和效率方面表现出色。此外，YOLO11 系列在准确性、速度、计算效率和模型大小方面始终表现出卓越的性能。YOLO11m 在准确性和效率之间取得了显著的平衡，在交通标志、非洲野生动物和船舶数据集上的mAP50-95得分分别为0.795、0.81和0.325，同时保持了2.4毫秒的平均推理时间，模型大小为38.8Mb，平均约为67.6 GFLOPs。这些结果为工业界和学术界提供了重要的见解，有助于为各种应用选择最合适的 YOLO 算法，并指导未来的增强功能。\n\n# 引言\n\n![Refer to caption](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/x1.png)\n\n​                             Figure 1:Evolution of YOLO Algorithms throughout the years.\n\n主要介绍了物体检测在计算机视觉系统中的重要性及其应用，并概述了YOLO（You Only Look Once）算法的发展历程和优势。\n\n- **物体检测的重要性**：物体检测是计算机视觉系统的关键组成部分，广泛应用于自动驾驶、机器人技术、库存管理、视频监控和体育分析等领域。\n- **传统方法的局限性**：传统的物体检测方法如Viola-Jones算法和DPM模型在鲁棒性和泛化能力上存在局限，而深度学习方法已成为主流。\n- **一阶段与两阶段方法**：一阶段方法如RetinaNet和SSD在速度和准确性之间取得平衡，而两阶段方法如R-CNN提供高精度但计算密集。\n- **YOLO算法的崛起**：YOLO算法以其鲁棒性和效率脱颖而出，自2015年首次提出以来，通过不断改进框架和设计，成为实时物体检测的领先算法。\n- **YOLO算法的演进**：YOLO算法的演进包括从YOLOv1到YOLOv11的多个版本，每个版本都引入了新的架构和技术来提高性能。\n- **Ultralytics的角色**：Ultralytics在YOLO算法的发展中扮演了重要角色，通过维护和改进模型，使其更易于访问和定制。\n- **研究目的**：本研究旨在对YOLO算法的演变进行全面比较分析，特别是对最新成员YOLO11进行首次全面评估，并探讨其在不同应用场景中的优势和局限性。\n- **研究方法**：研究使用了三个多样化的数据集，并采用了一致的超参数设置，以确保公平和无偏见的比较。\n- **研究贡献**：研究的贡献在于提供了对YOLO11及其前身的全面比较，深入分析了这些算法的结构演变，并扩展了性能评估指标，为选择最适合特定用例的YOLO算法提供了宝贵的见解。\n\n# 相关工作\n\n主要回顾了YOLO算法的演变、不同版本的架构、以及与其他计算机视觉算法的基准测试。以下是对该章节的详细总结分析：\n\n### YOLO算法的演变：\n\n- 论文[14]分析了包括YOLOv8在内的七种语义分割和检测算法，用于云层分割的遥感图像。\n- 论文[22]回顾了YOLO从版本1到版本8的演变，但没有考虑YOLOv9、YOLOv10和YOLO11。\n- 论文[12]详细分析了从YOLOv1到YOLOv4的单阶段物体检测器，并比较了两阶段和单阶段物体检测器。\n- 论文[53]探讨了YOLO从版本1到10的演变，强调了其在汽车安全、医疗保健等领域的应用。\n- 论文[61]讨论了YOLO算法的发展直到第四版，并提出了新的方法和挑战。\n- 论文[27]分析了YOLO算法的发展和性能，比较了从第8版到第8版的YOLO版本。\n\n### YOLO算法的应用：\n\n- YOLO算法在自动驾驶、医疗保健、工业制造、监控和农业等领域有广泛应用。\n- YOLOv8提供了多种应用，包括实例分割、姿态估计和定向物体检测（OOB）。\n\n### YOLO算法的基准测试：\n\n- 论文[14]进行了云层分割的基准测试，评估了不同算法的架构方法和性能。\n- 论文[22]提出了结合联邦学习以提高隐私、适应性和协作训练的通用性。\n- 论文[12]提供了单阶段和两阶段物体检测器的比较。\n- 论文[53]探讨了YOLO算法对未来AI驱动应用的潜在整合。\n- 论文[61]强调了YOLO算法在物体检测方面的挑战和需要进一步研究的地方。\n\n### YOLO算法的挑战：\n\n- YOLO算法在处理小物体和不同旋转角度的物体时面临挑战。\n- YOLOv9、YOLOv10和YOLO11的最新模型在准确性和效率方面表现出色，但在某些情况下仍需改进。\n\n### YOLO算法的改进：\n\n- YOLOv9引入了信息瓶颈原理和可逆函数来保留数据，提高了模型的收敛性和性能。\n- YOLOv10通过增强的CSP-Net主干和PAN层提高了梯度流动和减少了计算冗余。\n- YOLO11引入了C2PSA模块，结合了跨阶段部分网络和自注意力机制，提高了检测精度。\n\n### YOLO算法的未来方向：\n\n- 未来的研究可以专注于优化YOLOv10以提高其准确性，同时保持其速度和效率优势。\n- 继续改进架构设计可能会带来更先进的YOLO算法。\n\n### 研究贡献：\n\n- 本研究首次全面比较了YOLO11及其前身，并在三个多样化的数据集上评估了它们的性能。\n- 研究结果为工业界和学术界提供了选择最适合特定应用场景的YOLO算法的宝贵见解。\n\n通过这些分析，可以看出YOLO算法在不断演进和改进，以适应不同的应用需求和挑战。\n\n# Benchmark 设置\n\n### 数据集\n\n介绍了三种数据集，分别是Traffic Signs Dataset、Africa Wildlife Dataset和Ships/Vessels Dataset。以下是对这三种数据集的详细介绍：\n\n#### 1. Traffic Signs Dataset（交通标志数据集）\n\n- **来源**：由Radu Oprea在Kaggle上提供的开源数据集。\n- 特点：\n  - 包含约55个类别的交通标志图像。\n  - 训练集包含3253张图像，验证集包含1128张图像。\n  - 图像大小不一，初始尺寸为640x640像素。\n  - 为了平衡不同类别的数量，采用了欠采样技术。\n- **应用领域**：自动驾驶、交通管理、道路安全和智能交通系统。\n- 挑战：\n  - 目标物体大小变化较大。\n  - 不同类别之间的模式相似，增加了检测难度。\n\n#### 2. Africa Wildlife Dataset（非洲野生动物数据集）\n\n- **来源**：由Bianca Ferreira在Kaggle上设计的开源数据集。\n- 特点：\n  - 包含四种常见的非洲动物类别：水牛、大象、犀牛和斑马。\n  - 每个类别至少有376张图像，通过Google图像搜索收集并手动标注为YOLO格式。\n  - 数据集分为训练集、验证集和测试集，比例为70%、20%和10%。\n- **应用领域**：野生动物保护、反偷猎、生物多样性监测和生态研究。\n- 挑战：\n  - 目标物体的宽高比变化较大。\n  - 每张图像至少包含一种指定的动物类别，可能还包含其他类别的多个实例或发生情况。\n  - 目标物体重叠，增加了检测难度。\n\n#### 3. Ships/Vessels Dataset（船舶数据集）\n\n- **来源**：由Siddharth Sah从多个Roboflow数据集中收集并整理的开源数据集。\n- 特点：\n  - 包含约13.5k张图像，专门用于船舶检测。\n  - 每张图像都使用YOLO格式手动标注了边界框。\n  - 数据集分为训练集、验证集和测试集，比例为70%、20%和10%。\n- **应用领域**：海事安全、渔业管理、海洋污染监测、国防、海事安全和更多实际应用。\n- 挑战：\n  - 目标物体（船舶）相对较小。\n  - 目标物体具有不同的旋转角度，增加了检测难度。\n\n这些数据集在对象检测研究中具有重要意义，因为它们涵盖了不同大小、形状和密度的对象，能够全面评估YOLO算法在不同场景下的性能。\n\n### 模型\n\n#### 比较分析：Ultralytics vs 原始YOLO模型\n\n![](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134407205.png)\n\n在Traffic Signs数据集上，对Ultralytics提供的版本和原始模型进行比较分析，使用相同的超参数设置如表V所示。目标是为了强调突出Ultralytics提供的版本和原始模型之间的差异。由于Ultraytics缺乏对YOLO v4、YOLO v6、YOLO v7的支持，因此本文将这几个YOLO版本排除在外了。\n\n##### Ultralytics支持库中的模型和任务\n\n![image-20241202134825220](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134825220.png)\n\n根据表I，Ultralytics库为研究人员和程序员提供了各种YOLO模型，用于推理、验证、训练和导出。我们注意到Ultralytics不支持YOLOv1、YOLOv2、YOLOv4和YOLOv7。对于YOLOv6，库只支持配置文件.yaml，而不支持预训练的.pt模型。\n\n##### Ultralytics和原始模型的性能比较\n\n通过对Ultralytics模型及其原始版本在交通标志数据集上的比较分析，我们观察到Ultralytics版本和原始版本之间存在显著差异。例如，Ultralytics版本的YOLOv5n（nano）和YOLOv3表现优越，突显了Ultralytics所做的增强和优化。相反，原始版本的YOLOv9c（compact）略微优于其Ultralytics版本，可能是由于Ultralytics对该较新模型的优化不足。这些观察结果表明，Ultralytics模型经过了大量修改，直接比较原始版本和Ultralytics版本是不公平和不准确的。因此，本文将专注于Ultralytics支持的版本，以确保基准测试的一致性和公平性。\n\n###### YOLOv3u\n\n![image-20241202135426435](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135426435.png)\n\nYOLOv3基于其前身，旨在提高定位错误和检测效率，特别是对于较小的物体。它使用Darknet-53框架，该框架有53个卷积层，速度是ResNet-152的两倍。YOLOv3还结合了特征金字塔网络（FPN）的元素，如残差块、跳跃连接和上采样，以增强跨不同尺度的物体检测能力。该算法生成三个不同尺度的特征图，以32、16和8的因子对输入进行下采样，并使用三尺度检测机制来检测大、中、小尺寸物体，分别使用不同的特征图。尽管有所改进，YOLOv3在检测中等和大型物体时仍面临挑战，因此Ultralytics发布了YOLOv3u。YOLOv3u是YOLOv3的改进版本，使用无锚点检测方法，并提高了YOLOv3的准确性和速度，特别是对于中等和大型物体。\n\n###### YOLOv5u\n\n![image-20241202135925381](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135925381.png)\n\nYOLOv5由Glenn Jocher提出，从Darknet框架过渡到PyTorch，保留了YOLOv4的许多改进，并使用CSPDarknet作为其骨干。CSPDarknet是原始Darknet架构的修改版本，通过将特征图分成单独的路径来实现更高效的特征提取和减少计算成本。YOLOv5采用步幅卷积层，旨在减少内存和计算成本。此外，该版本采用空间金字塔池化快速（SPPF）模块，通过在不同尺度上池化特征并提供多尺度表示来工作。YOLOv5实现了多种增强，如马赛克、复制粘贴、随机仿射、MixUp、HSV增强和随机水平翻转。Ultralytics通过YOLOv5u积极改进该模型，采用无锚点检测方法，并在复杂物体的不同尺寸上实现了更好的整体性能。\n\n###### YOLOv8\n\n![image-20241202140006851](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140006851.png)\n\nUltralytics引入了YOLOv8，这是YOLO系列的重大进化，包括五个缩放版本。除了物体检测外，YOLOv8还提供了图像分类、姿态估计、实例分割和定向物体检测（OOB）等多种应用。关键特性包括类似于YOLOv5的主干，调整后的CSPLayer（现称为C2f模块），结合了高级特征和上下文信息以提高检测精度。YOLOv8还引入了一个语义分割模型YOLOv8-Seg，结合了CSPDarknet53特征提取器和C2F模块，在物体检测和语义分割基准测试中取得了最先进的结果，同时保持了高效率。\n\n###### YOLOv9\n\n![image-20241202140048800](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140048800.png)\n\nYOLOv9由Chien-Yao Wang、I-Hau Yeh和Hong-Yuan Mark Liao开发，使用信息瓶颈原理和可逆函数来在网络深度中保留关键数据，确保可靠的梯度生成并提高模型收敛性和性能。可逆函数可以在不丢失信息的情况下反转，这是YOLOv9架构的另一个基石。这种属性允许网络保持完整的信息流，使模型参数的更新更加准确。此外，YOLOv9提供了五个缩放版本，重点是轻量级模型，这些模型通常欠参数化，并且在前向过程中容易丢失重要信息。可编程梯度信息（PGI）是YOLOv9引入的一项重大进步。PGI是一种在训练期间动态调整梯度信息的方法，通过选择性关注最具信息量的梯度来优化学习效率。通过这种方式，PGI有助于保留可能在轻量级模型中丢失的关键信息。此外，YOLOv9还包括GELAN（梯度增强轻量级架构网络），这是一种新的架构改进，旨在通过优化网络内的计算路径来提高参数利用和计算效率。\n\n###### YOLOv10\n\n![image-20241202140137372](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140137372.png)\n\nYOLOv10由清华大学的研究人员开发，基于先前模型的优势进行了关键创新。该架构具有增强的CSP-Net（跨阶段部分网络）主干，以提高梯度流动和减少计算冗余。网络结构分为三部分：主干、颈部和检测头。颈部包括PAN（路径聚合网络）层，用于有效的多尺度特征融合。PAN旨在通过聚合不同层的特征来增强信息流，使网络能够更好地捕捉和结合不同尺度的细节，这对于检测不同大小的物体至关重要。此外，该版本还提供五个缩放版本，从纳米到超大。对于推理，One-to-One Head为每个物体生成单个最佳预测，消除了对非极大值抑制（NMS）的需求。通过移除对NMS的需求，YOLOv10减少了延迟并提高了后处理速度。此外，YOLOv10还包括NMS-Free Training，使用一致的双重分配来减少推理延迟，并优化了从效率和准确性角度的各种组件，包括轻量级分类头、空间-通道解耦下采样和排名引导块设计。此外，该模型还包括大核卷积和部分自注意力模块，以在不显著增加计算成本的情况下提高性能。\n\n###### YOLO11\n\n![image-20241202140334746](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140334746.png)\n\nYOLO11是Ultralytics推出的最新创新，基于其前身的发展，特别是YOLOv8。这一迭代提供了从纳米到超大的五种缩放模型，适用于各种应用。与YOLOv8一样，YOLO11包括物体检测、实例分割、图像分类、姿态估计和定向物体检测（OBB）等多种应用。关键改进包括引入C2PSA（跨阶段部分自注意力）模块，结合了跨阶段部分网络和自注意力机制的优势。这使得模型能够在多个层次上更有效地捕获上下文信息，提高物体检测精度，特别是对于小型和重叠物体。此外，在YOLO11中，C2f块被C3k2块取代，C3k2是CSP Bottleneck的自定义实现，使用两个卷积而不是YOLOv8中使用的一个大卷积。这个块使用较小的内核，在保持精度的同时提高了效率和速度。\n\n### 硬件和软件设置\n\n- 表III：实验的软件设置\n- 表IV：6个YOLO版本的不同尺寸的模型\n\n![image-20241202140352257](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140352257.png)\n\n![image-20241202140542480](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140542480.png)\n\n总结了用于评估YOLO模型的硬件和软件环境设置。\n\n1. **软件环境**：实验使用了Python 3.12、Ubuntu 22.04、CUDA 12.5、cuDNN 8.9.7、Ultralytics 8.2.55和WandB 0.17.4等软件包。\n2. **硬件环境**：实验在两块NVIDIA RTX 4090 GPU上进行，每块GPU拥有16,384个CUDA核心。\n3. **数据集处理**：针对交通标志数据集，应用了欠采样技术以确保数据集平衡，并将图像数量从4381减少到3233张。\n4. **训练验证测试分割**：非洲野生动物数据集和船只数据集分别按照70%训练、20%验证和10%测试的比例进行分割。\n5. **模型训练**：实验中训练了23个模型，涵盖了5种不同的YOLO版本，并使用了相似的超参数以确保公平比较。\n6. **模型规模**：交通标志数据集包含24个类别，平均每个类别约100张图像；非洲野生动物数据集包含4个类别，每个类别至少有376张图像；船只数据集专注于单一类别的小型物体检测。\n\n### 评估指标\n\n评估指标包括准确性、计算效率和模型大小三个方面：\n\n#### 准确性指标\n1. **Precision（精确率）**：\n   - 定义：正确预测的观察值与总预测观察值的比率。\n   - 计算公式：$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n   - 其中，TP（True Positives）为真正例，FP（False Positives）为假正例。\n\n2. **Recall（召回率）**：\n   - 定义：正确预测的观察值与所有实际观察值的比率。\n   - 计算公式：$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n   - 其中，FN（False Negatives）为假反例。\n\n3. **mAP50（Mean Average Precision at an IoU threshold of 0.50）**：\n   - 定义：在IoU（Intersection over Union）阈值为0.50时的平均精度均值。\n   - 计算公式：$$ \\text{mAP50} = \\frac{1}{|C|} \\sum_{c \\in C} \\text{AP}_c $$\n   - 其中，$C$ 是类别集合，$\\text{AP}_c$ 是类别 $c$ 的平均精度。\n\n4. **mAP50-95（Mean Average Precision across IoU thresholds from 0.50 to 0.95）**：\n   - 定义：在IoU阈值从0.50到0.95范围内的平均精度均值。\n   - 计算公式：$$ \\text{mAP50-95} = \\frac{1}{15} \\sum_{r=1}^{15} \\text{AP}_{0.50 + \\frac{r-1}{14} \\times 0.05} $$\n   - 其中，$r$ 表示IoU阈值的范围。\n\n#### 计算效率指标\n1. **Preprocessing Time（预处理时间）**：\n   - 定义：准备原始数据以输入模型所需的持续时间。\n\n2. **Inference Time（推理时间）**：\n   - 定义：模型处理输入数据并生成预测所需的持续时间。\n\n3. **Postprocessing Time（后处理时间）**：\n   - 定义：将模型的原始预测转换为最终可用格式所需的时间。\n\n4. **Total Time（总时间）**：\n   - 定义：预处理时间、推理时间和后处理时间的总和。\n\n5. **GFLOPs（Giga Floating-Point Operations Per Second）**：\n   - 定义：模型训练的计算能力，反映其效率。\n\n#### 模型大小指标\n1. **Size（大小）**：\n   - 定义：模型的实际磁盘大小及其参数数量。\n\n这些指标提供了对YOLO模型性能的全面概述，有助于在不同真实世界场景中选择最优的YOLO算法。\n\n# 实验结果和讨论\n\n![image-20241202142033886](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142033886.png)\n\n### 实验结果\n\n#### 交通信号数据集\n\nYOLO模型在检测交通标志方面的有效性，展示了各种精度范围。最高的mAP50-95为0.799，而最低的精度为0.64。另一方面，最高的mAP50为0.893，而最低的为0.722。mAP50和mAP50-95之间的显著差距表明，模型在处理不同大小的交通标志时，在较高阈值下遇到了困难，这反映了其检测算法中潜在的改进领域。\n\na) 准确性：如图8所示，YOLOv5ul展示了最高的准确性，实现了mAP50为0.866和mAP50-95为0.799。紧随其后的是YOLO11m，其mAP50-95为0.795，YOLO11l的mAP50-95为0.794。相比之下，YOLOv10n展示了最低的精度，其mAP50为0.722，mAP50-95为0.64，紧随其后的是YOLOv5un，其mAP50-95为0.665，如数据点在图8中所证明的。\n\n![image-20241202142326776](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142326776.png)\n\nb) 精度和召回率：图9阐明了考虑模型大小的情况下精度和召回率之间的权衡。像YOLO11m、YOLO10l、YOLOv9m、YOLOv5ux和YOLO111这样的模型展示了高精度和召回率，特别是YOLO11m实现了0.898的精度和0.826的召回率，同时模型大小为67.9Mb，而YOLOv10l实现了0.873的精度和0.807的召回率，但模型大小显著更大（126.8 Mb）。相比之下，较小的模型如YOLOv10n（精度0.722，召回率0.602）、YOLOv8n（精度0.749，召回率0.688）和YOLO11n（精度0.768，召回率0.695）在两个指标上都表现不佳。这突显了较大模型在交通标志数据集上的优越性能。此外，YOLOv5um的高精度（0.849）和低召回率（0.701）表明了对假阴性的倾向，而YOLOv3u的高召回率（0.849）和低精度（0.75）则表明了对假阳性的倾向。\n\n![image-20241202142423060](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142423060.png)\n\nc) 计算效率：在计算效率方面，YOLOv10n是最有效的，每张图片的处理时间为2ms，GFLOPs计数为8.3，如图10和11所示。YOLO11n紧随其后，处理时间为2.2ms，GFLOPs计数为6.4，而YOLOv3u-tiny的处理时间为2.4ms，GFLOPs计数为19，与其他快速模型相比，这使得它在计算上相对低效。然而，数据显示YOLOv9e、YOLOv9m、YOLOv9c和YOLOv9s是效率最低的，推理时间分别为16.1ms、12.1ms、11.6ms和11.1ms，GFLOPs计数分别为189.4、76.7、102.6和26.8。这些发现描绘了一个明显的权衡，即在精度和计算效率之间。\n\nd) 整体性能：在评估整体性能时，包括准确性、大小和模型效率，YOLO11m作为一个一致的表现最佳的模型脱颖而出。它实现了mAP50-95为0.795，推理时间为2.4ms，模型大小为38.8Mb，GFLOPs计数为67.9，如图8、10、11和表VI中详细说明的。紧随其后的是YOLO111（mAP50-95为0.794，推理时间为4.6ms，大小为49Mb，GFLOPs计数为86.8）和YOLOv10m（mAP50-95为0.781，推理时间为2.4ms，大小为32.1Mb，63.8 GFLOPs计数）。这些结果突显了这些模型在检测各种大小的交通标志方面的稳健性，同时保持了较短的推理时间和较小的模型大小。值得注意的是，YOLO11和YOLOv10家族在准确性和计算效率方面显著优于其他YOLO家族，因为它们的模型在这些数据集上一致超越了其他家族的对应物。\n\n![image-20241202142515870](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142515870.png)\n\n#### 非洲野生动物数据集\n\n![image-20241202142815914](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142815914.png)\n\n表 VII 展示了 YOLO 模型在非洲野生动物数据集上的性能。该数据集包含大型物体尺寸，重点关注 YOLO 模型预测大型物体的能力以及由于数据集大小而导致过拟合的风险。模型在各个方面的准确性都表现出色，最高性能的模型 mAP50-95 范围从 0.832 到 0.725。这个相对较短的范围反映了模型在检测和分类大型野生动物物体时保持高准确性的有效性。\n\na) 准确性：如图 12 所示，YOLOv9s 展现了出色的性能，具有高达 0.832 的 mAP50-95 和 0.956 的 mAP50，展示了其在各种 IoU 阈值下的稳健准确性。YOLOv9c 和 YOLOv9t 紧随其后，mAP50 分数分别为 0.96 和 0.948，召回率分别为 0.896。值得注意的是，YOLOv8n 实现了 mAP50-95 得分分别为 0.83 和 0.825。这些结果突出了 YOLOv9 系列从少量图像样本中有效学习模式的能力，使其特别适合于较小型的数据集。相比之下，YOLOv5un、YOLOv10n 和 YOLOv3u-tiny 显示出较低的 mAP50-95 得分，分别为 0.791、0.786 和 0.725，表明它们在准确性方面的局限性。较大的模型如 YOLO11x、YOLOv5ux、YOLOv5ul 和 YOLOv10l 的表现不佳，可以归因于过拟合，特别是考虑到数据集规模较小。\n\n![image-20241202142853898](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142853898.png)\n\nb) 精度和召回率：图 13 表明 YOLO8l 和 YOLO111 实现了最高的精度和召回率，精度值分别为 0.942 和 0.937，召回率分别为 0.898 和 0.896。值得注意的是，YOLOv8n 实现了 0.932 的精度和 0.908 的召回率。总体而言，YOLOv8l 和 YOLO111 在精度和召回率方面表现最佳，YOLOv8n 的表现也相当出色。然而，YOLOv11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。\n\n![image-20241202142924537](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142924537.png)\n\nc) 计算效率：如图 14 和 15 所示，YOLOv10n、YOLOv8n 和 YOLOv3u-tiny 是最快的模型，处理时间分别为 2ms 和 1.8ms，GFLOPs 计数分别为 8.2 和 19.1。前两个模型具有相同的处理速度和 GFLOPs 计数，如表 VII 中所示。相比之下，YOLOv9e 展现了最慢的处理时间，为 11.2ms，GFLOPs 计数为 189.3，其次是 YOLOv5ux，处理时间为 7.5ms，GFLOPs 计数为 246.2 GFLOPs 计数。这些结果表明，较大的模型通常需要更多的处理时间和硬件资源，强调了模型大小和处理效率之间的权衡。\n\n![image-20241202143004149](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143004149.png)\n\nd) 整体性能：表 VII 和图 13、14 和 15 中的结果表明，YOLOv9t 和 YOLOv9s 在各个指标上持续表现出色，提供高准确性，同时保持较小的模型大小、低 GFLOPs 和短的处理时间，展示了 YOLOv9 较小型模型的稳健性及其在小数据集上的有效性。相比之下，YOLO5ux 和 YOLO11x 尽管具有较大的尺寸和较长的推理时间，但准确性表现不佳，可能是由于过拟合所致。大多数大型模型在这个数据集上的表现都不尽如人意，YOLOv10x 是一个例外，得益于现代架构防止过拟合，表现优异。\n\n![image-20241202143030167](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143030167.png)\n\n#### 船只和船舶数据集：\n\n![image-20241202143313627](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143313627.png)\n\n表 VIII 展示了 YOLO 模型在船只和船舶数据集上的性能，这是一个包含微小物体且旋转变化多样的大型数据集。总体而言，模型在检测船只和船舶方面表现出中等效果，mAP50-95 的范围从 0.273 到 0.327。这一表现表明 YOLO 算法在准确检测较小物体方面可能面临挑战，数据集中物体尺寸和旋转的多样性为测试模型能力提供了全面的测试。\n\na) 准确性：图 16 中 mAP50-95 和 mAP50 之间的差异凸显了 YOLO 模型在检测小物体时面临的挑战，尤其是在更高的 IoU 阈值下。此外，YOLO 模型在检测不同旋转的物体时也遇到困难。在各个模型中，YOLO11x 实现了最高的准确性，mAP50 为 0.529，mAP50-95 为 0.327，紧随其后的是 YOLO111、YOLO11m 和 YOLO11s，它们记录的 mAP50 值分别为 0.529、0.528 和 0.53，mAP50-95 值分别为 0.327、0.325 和 0.325。这些结果突出了 YOLO11 系列在检测小型和微小物体方面的稳健性。相比之下，YOLOv3u-tiny、YOLOv8n、YOLOv3u 和 YOLOv5n 展示了最低的准确性，mAP50 分数分别为 0.489、0.515、0.519 和 0.514，mAP50-95 分数分别为 0.273、0.297、0.298 和 0.298。这表明 YOLOv3u 的过时架构以及由于数据集规模较大而导致的小型模型的潜在欠拟合。\n\n![image-20241202143334789](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143334789.png)\n\nb) 精度和召回率：图 17 表明 YOLOv5ux 的表现优于其他模型，实现了 0.668 的精度和 0.555 的召回率。它紧随其后的是 YOLOv9m（精度为 0.668，召回率为 0.551）和 YOLOv8m（精度为 0.669，召回率为 0.525），两者在尺寸上显著较小（YOLOv9m 为 40.98 Mb，YOLOv8m 为 52.12 Mb）。相比之下，YOLO11n 和 YOLOv10s 表现较差，精度分别为 0.574 和 0.586，召回率分别为 0.51 和 0.511，这可能是由于欠拟合问题。总体而言，YOLO11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。\n\n![image-20241202143403226](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143403226.png)\n\nc) 计算效率：如图 18 和 19 所示，YOLOv3u-tiny 实现了最快的处理时间，为 2 毫秒，紧随其后的是 YOLOv8n 和 YOLOv5un，两者均记录了 2.3 毫秒。YOLOv10 和 YOLO11 模型也在速度上表现出色，YOLOv10n 和 YOLO11n 分别实现了 2.4 毫秒和 2.5 毫秒的快速推理时间，以及 8.2 和 6.3 的 GFLOPs 计数。相比之下，YOLOv9e 展现了最慢的速度，推理时间为 7.6 毫秒，GFLOPs 计数为 189.3，突显了 YOLOv9 系列在准确性和效率之间的权衡。\n\n![image-20241202143422503](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143422503.png)\n\nd) 整体性能：表 VIII 和图 16、17 和 18 中的结果表明，YOLO11s 和 YOLOv10s 在准确性方面表现优异，同时保持了紧凑的尺寸、低 GFLOPs 和快速的处理时间。相比之下，YOLOv3u、YOLOv8x 和 YOLOv8l 未能达到预期，尽管它们的尺寸较大且处理时间较长。这些发现突出了 YOLO11 系列的稳健性和可靠性，特别是在提高 YOLO 系列检测小型和微小物体的性能方面，同时确保高效处理。此外，结果还揭示了 YOLOv9 模型在面对大型数据集和小物体时的表现不佳，尽管它们具有现代架构。\n\n![image-20241202143442543](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143442543.png)\n\n### 讨论\n\n![image-20241202143815028](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143815028.png)\n\n基于三个数据集上模型的性能，我们按准确性、速度、GFLOps 计数和大小对它们进行了排名，如表 IX 所示，以便进行全面评估。对于准确性，由于 mAP50-95 指标能够评估模型在一系列 IoU 阈值下的表现，因此我们采用了该指标。对于速度，模型根据总处理时间进行排序，总处理时间包括预处理、推理和后处理持续时间。排名范围从第 1 名（表示最高性能）到第 28 名（表示最低性能），表中的相应排名已加粗显示。\n\n表 IX 的分析得出了几个关键观察结果：\n\n1) 准确性：YOLO11m 一致地成为顶级表现者，经常位居前列，紧随其后的是 YOLOv10x、YOLO111、YOLOv9m 和 YOLO11x。这突显了 YOLO11 系列在各种 IoU 阈值和物体大小下的稳健性能，这可以归因于它们使用 C2PSA 来保留上下文信息，从而提高了收敛性和整体性能。此外，大核卷积和部分自注意力模块的实施有助于提高算法的性能。\n\n相比之下，YOLOv3u-tiny 展现了最低的准确性，特别是在非洲野生动物和船只及船舶数据集上，YOLOv5un 和 YOLOv8n 的表现稍好但仍不理想。这表明 YOLO11 模型目前是要求高准确性的应用中最可靠的。\n\n紧随 YOLO11 系列之后，YOLOv9 模型在检测各种大小和不同 IoU 阈值的物体方面表现出色。然而，它们可能在检测小物体时遇到困难，这在船只和船舶数据集上可见。相比之下，YOLOv10 系列尽管推出较晚，但在交通标志和非洲动物数据集上的准确性相对较低，导致平均准确性下降了 2.075%，这可以归因于它们采用一对一头部方法而不是非极大值抑制（NMS）来定义边界框。这种策略在捕捉物体时可能会遇到困难，特别是在处理重叠物品时，因为它依赖于每个物体的单个预测。这一限制有助于解释第二个数据集中观察到的相对较差的结果。\n\nYOLOv3u 的过时架构也导致了其性能不佳，平均准确性比 YOLO11 模型低 6.5%。这种下降可以追溯到其对 2018 年首次引入的较旧 Darknet-53 框架的依赖，该框架可能无法充分应对当代检测挑战。\n\n2) 计算效率：YOLOv10n 在速度和 GFLOPs 计数方面始终表现优异，在所有三个数据集上均名列前茅，在速度方面排名第 1，在 GFLOPs 计数方面排名第 5。YOLOv3u-tiny、YOLOv10s 和 YOLO11n 也展示了显著的计算效率。\n\nYOLOv9e 展现了最慢的推理时间和非常高的 GFLOPs 计数，突显了准确性与效率之间的权衡。YOLO11 的速度提升可归因于它们使用的 C3k2 块，使其适用于需要快速处理的场景，超过了 YOLOv10 和 YOLOv9 模型，分别在速度上平均快了 %1.41 和 %31。\n\n虽然 YOLOv9 模型在准确性方面表现出色，但它们的推理时间却是最慢的，使它们不太适合对时间敏感的应用。相比之下，YOLOv10 模型虽然略慢于 YOLO11 变体，但仍提供了效率与速度之间的值得称赞的平衡。它们的表现非常适合时间敏感的场景，提供快速处理而不显著牺牲准确性，使它们成为实时应用的可行选择。\n\n3) 模型大小：YOLOv9t 是最小的模型，在所有三个数据集上均排名第一，其次是 YOLO11n 和 YOLOv10n。这种模型大小的效率突显了较新 YOLO 版本，特别是 YOLOv10，在高效参数利用方面的进步，实施了空间-通道解耦下采样。\n\nYOLOv3u 是最大的模型，突显了与其更现代的对应物相比，它的效率低下。\n\n4) 整体性能：考虑到准确性、速度、大小和 GFLOPs，YOLO11m、YOLOv11n、YOLO11s 和 YOLOv10s 成为最一致的表现者。它们实现了高准确性、低处理时间和功率以及高效的磁盘使用，使其适用于广泛的应用，其中速度和准确性都至关重要。\n\n相反，YOLOv9e、YOLOv5ux 和 YOLOv3u 在所有指标上的表现都较差，计算效率低下且相对于其大小表现不佳。YOLO11 模型显示出最佳的整体性能，可能是由于最近的增强功能，如 C3k2 块和 C2PSA 模块。紧随其后的是 YOLOv10 模型，尽管在准确性方面略有逊色，但由于其一对一头部用于预测的实施，在效率方面表现出色。虽然 YOLOv9 在计算效率方面表现不佳，但它在准确性方面仍然具有竞争力，这要归功于其 PGI 集成。这使 YOLOv9 成为优先考虑精度而非速度的应用的可行选择。\n\n此外，YOLOv8 和 YOLOv5u 展示了竞争性结果，超过了 YOLOv3u 的准确性，这可能是由于 YOLOv3u 的较旧架构。然而，它们的准确性仍然显著低于较新的模型，如 YOLOv9、YOLOv10 和 YOLO11。虽然 YOLOv8 和 YOLOv5u 的处理时间比 YOLOv9 快，但它们的整体表现仍然不如较新的模型。\n\n5) 物体大小和旋转检测：YOLO 算法在检测大中型物体方面效果很好，如非洲野生动物和交通标志数据集所证明的那样，准确性很高。然而，它在检测小物体方面存在困难，可能是由于将图像划分为网格，使得识别小而分辨率低的物体变得具有挑战性。此外，YOLO 在处理不同旋转的物体时也面临挑战，因为无法包围旋转物体，导致整体结果不佳。\n\n为了处理旋转物体，可以实现像 YOLO11 OBB[26] 和 YOLOv8 OBB[25]（定向边界框）这样的模型。保持与标准 YOLOv8 和 YOLO11 相同的基础架构，YOLOv8 OBB 和 YOLO11 OBB 用预测旋转矩形四个角点的头部替换了标准边界框预测头部，允许更准确的定位和表示任意方向的物体。\n\n6) YOLO11 对 YOLOv8 的崛起：尽管 YOLOv8[25] 因其在姿态估计、实例分割和定向物体检测（OBB）任务中的多功能性而成为算法的首选，但 YOLO11[26] 已经成为一个更高效和准确的替代品。通过处理相同任务的同时提供改进的上下文理解和更好的架构模块，YOLO11 设定了新的性能标准，在各种应用中的速度和准确性方面都超过了 YOLOv8。\n\n7) 数据集大小：数据集的大小显著影响 YOLO 模型的性能。例如，大型模型在小型非洲野生动物数据集上的表现不如在交通标志和船只及船舶数据集上的表现，因为它们更容易过拟合。相反，像 YOLOv9t 和 YOLOv9s 这样的小模型在非洲野生动物数据集上的表现显著更好，展示了小规模模型在处理有限数据集时的有效性。\n\n8) 训练数据集的影响：如表 VI、VII 和 VIII 所示，YOLO 模型的性能受到所使用的训练数据集的影响。不同的数据集产生不同的结果和顶尖表现者，表明数据集复杂性影响算法性能。这突显了在基准测试期间使用多样化数据集以获得每个模型优缺点全面结果的重要性。\n\n这次讨论强调了在选择 YOLO 模型进行特定应用时，需要平衡考虑准确性、速度和模型大小。YOLO11 模型在各个指标上的一致表现使它们非常适合于需要准确性和速度的多功能场景。同时，YOLOv10 模型可以在保持更快处理时间和更小模型大小的同时，类似地执行。此外，YOLOv9 可以在准确性方面提供可比的结果，但牺牲了速度，使其适用于优先考虑精度而非快速处理的应用。\n\n# 结论\n\n这项基准研究全面评估了各种 YOLO 算法的性能。它是首个对 YOLO11 及其前辈进行全面比较的研究，评估了它们在三个多样化数据集上的表现：交通标志、非洲野生动物和船只及船舶。这些数据集经过精心挑选，包含了广泛的物体属性，包括不同的物体大小、宽高比和物体密度。我们通过检查精度、召回率、平均精度均值（mAP）、处理时间、GFLOPs 计数和模型大小等一系列指标，展示了每个 YOLO 版本和家族的优势和劣势。我们的研究解决了以下关键研究问题：\n\n● 哪个 YOLO 算法在一系列综合指标上展示了卓越的性能？\n\n● 不同的 YOLO 版本在具有不同物体特征（如大小、宽高比和密度）的数据集上的表现如何？\n\n● 每个 YOLO 版本的具体优势和局限性是什么，这些见解如何指导选择最适合各种应用的算法？\n\n特别是，YOLO11 系列作为最一致的表现在各个指标上脱颖而出，YOLO11m 在准确性、效率、模型大小之间取得了最佳平衡。虽然 YOLOv10 的准确性略低于 YOLO11，但它在速度和效率方面表现出色，使其成为需要效率和快速处理的应用的强有力选择。此外，YOLOv9 总体上也表现良好，特别是在较小的数据集上表现尤为突出。这些发现为工业界和学术界提供了宝贵的见解，指导选择最适合的 YOLO 算法，并为未来的发展和改进提供信息。虽然评估的算法展示了有希望的性能，但仍有一些改进的空间。未来的研究可以专注于优化 YOLOv10，以提高其准确性，同时保持其速度和效率优势。此外，架构设计的持续进步可能为更突破性的 YOLO 算法铺平道路。我们未来的工作包括深入研究这些算法中确定的差距，并提出改进措施，以展示它们对整体效率的潜在影响。\n\n# 其它问题\n\n- **在交通标志数据集上，YOLOv5ul和YOLOv10n的性能差异是什么？原因是什么？**\n\n在交通标志数据集上，YOLOv5ul的mAP50-95达到了0.799，而YOLOv10n的mAP50-95仅为0.64，相差显著。YOLOv5ul在精度和召回率上都表现更好，具体来说，YOLOv5ul的Precision为0.866，Recall为0.849，而YOLOv10n的Precision为0.722，Recall为0.602。这种差异的原因可能包括：\n\n1. **模型架构改进**：YOLOv5ul采用了更先进的CSPDarknet53作为主干网络，并引入了Spatial Pyramid Pooling Fast（SPPF）模块，这些改进提高了模型的特征提取能力和多尺度适应性。\n2. **数据增强**：YOLOv5ul使用了多种数据增强技术，如Mosaic、Copy-Paste、Random Affine等，这些技术有助于提高模型的泛化能力和鲁棒性。\n3. **优化策略**：YOLOv5ul在训练过程中使用了更有效的优化策略，如AdamW优化器和学习率调度，这些策略有助于模型更快地收敛和提高性能。\n\n- **在船舶与船只数据集上，YOLOv11x为什么表现最好？与其他模型相比有哪些优势？**\n\n​\t在船舶与船只数据集上，YOLOv11x的mAP50-95达到了0.327，表现最好。与其他模型相比，YOLOv11x有以下优势：\n\n1. **小对象检测**：YOLOv11x特别适用于检测小对象，能够在复杂的图像环境中准确识别和定位小尺寸的船只。\n2. **架构改进**：YOLOv11x引入了C3k2块和C2PSA（Cross-Stage Partial with Self-Attention）模块，这些改进提高了模型的空间注意力和特征提取能力，特别是在处理重叠和小的对象时表现优异。\n3. **计算效率**：尽管YOLOv11x在精度上有所提升，但其推理时间仍然保持在2.4ms左右，保持了较高的计算效率，适合实时应用。\n\n- **在非洲野生动物数据集上，YOLOv9s的表现优于其他模型的原因是什么？**\n\n  在非洲野生动物数据集上，YOLOv9s的mAP50-95达到了0.832，表现优于其他模型。YOLOv9s之所以表现优异，主要原因包括：\n\n  1. **小型数据集适应性**：YOLOv9s在小型数据集上表现出色，能够有效学习对象的模式。其小型模型（如YOLOv9t）在非洲野生动物数据集上表现尤为突出，mAP50-95为0.832，mAP50为0.956。\n  2. **特征提取能力**：YOLOv9s采用了CSPNet（Cross-Stage Partial Network），这种结构通过分割特征图来提高特征提取效率，减少了计算复杂度。\n  3. **正则化技术**：YOLOv9s使用了GelAN（Gradient Enhanced Lightweight Architecture Network），这种技术通过优化网络内的计算路径，提高了参数利用率和计算效率。\n\n[Evaluating the Evolution of YOLO (You Only Look Once) Models: A Comprehensive Benchmark Study of YOLO11 and Its Predecessors](https://arxiv.org/html/2411.00201v1)\n\n![二维码](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg)","source":"_posts/computer-vision/CV008-评估 YOLO （You Only Look Once） 模型的演变：YOLO11 及其前身的全面基准研究.md","raw":"---\ntitle: 'YOLO模型的全面综述'\ndate: 2024-12-7 18:30:00\ncategories:\n  - computer-vision\ntags:\n  - yolo\n  - 目标检测\n---\n\n\n\n# 摘要\n\n本研究对YOLO （You Only Look Once） 的各个版本进行了全面的基准测试分析，从 YOLOv3 到最新的算法。它代表了首次全面评估 YOLO11 性能的研究，YOLO11 是 YOLO 系列的最新成员。它评估了它们在三个不同数据集上的性能：交通标志（具有不同的对象大小）、非洲野生动物（具有不同的纵横比，每个图像至少有一个对象实例）以及船舶和船只（具有单个类别的小型对象），确保在具有不同挑战的数据集之间进行全面评估。为了确保稳健的评估，我们采用了一套全面的指标，包括精度、召回率、平均精度均值 （mAP）、处理时间、GFLOP 计数和模型大小。我们的分析强调了每个 YOLO 版本的独特优势和局限性。例如：YOLOv9 表现出很高的准确性，但在检测小物体和效率方面表现不佳，而 YOLOv10 表现出相对较低的准确性，因为架构选择会影响其在重叠物体检测方面的性能，但在速度和效率方面表现出色。此外，YOLO11 系列在准确性、速度、计算效率和模型大小方面始终表现出卓越的性能。YOLO11m 在准确性和效率之间取得了显著的平衡，在交通标志、非洲野生动物和船舶数据集上的mAP50-95得分分别为0.795、0.81和0.325，同时保持了2.4毫秒的平均推理时间，模型大小为38.8Mb，平均约为67.6 GFLOPs。这些结果为工业界和学术界提供了重要的见解，有助于为各种应用选择最合适的 YOLO 算法，并指导未来的增强功能。\n\n# 引言\n\n![Refer to caption](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/x1.png)\n\n​                             Figure 1:Evolution of YOLO Algorithms throughout the years.\n\n主要介绍了物体检测在计算机视觉系统中的重要性及其应用，并概述了YOLO（You Only Look Once）算法的发展历程和优势。\n\n- **物体检测的重要性**：物体检测是计算机视觉系统的关键组成部分，广泛应用于自动驾驶、机器人技术、库存管理、视频监控和体育分析等领域。\n- **传统方法的局限性**：传统的物体检测方法如Viola-Jones算法和DPM模型在鲁棒性和泛化能力上存在局限，而深度学习方法已成为主流。\n- **一阶段与两阶段方法**：一阶段方法如RetinaNet和SSD在速度和准确性之间取得平衡，而两阶段方法如R-CNN提供高精度但计算密集。\n- **YOLO算法的崛起**：YOLO算法以其鲁棒性和效率脱颖而出，自2015年首次提出以来，通过不断改进框架和设计，成为实时物体检测的领先算法。\n- **YOLO算法的演进**：YOLO算法的演进包括从YOLOv1到YOLOv11的多个版本，每个版本都引入了新的架构和技术来提高性能。\n- **Ultralytics的角色**：Ultralytics在YOLO算法的发展中扮演了重要角色，通过维护和改进模型，使其更易于访问和定制。\n- **研究目的**：本研究旨在对YOLO算法的演变进行全面比较分析，特别是对最新成员YOLO11进行首次全面评估，并探讨其在不同应用场景中的优势和局限性。\n- **研究方法**：研究使用了三个多样化的数据集，并采用了一致的超参数设置，以确保公平和无偏见的比较。\n- **研究贡献**：研究的贡献在于提供了对YOLO11及其前身的全面比较，深入分析了这些算法的结构演变，并扩展了性能评估指标，为选择最适合特定用例的YOLO算法提供了宝贵的见解。\n\n# 相关工作\n\n主要回顾了YOLO算法的演变、不同版本的架构、以及与其他计算机视觉算法的基准测试。以下是对该章节的详细总结分析：\n\n### YOLO算法的演变：\n\n- 论文[14]分析了包括YOLOv8在内的七种语义分割和检测算法，用于云层分割的遥感图像。\n- 论文[22]回顾了YOLO从版本1到版本8的演变，但没有考虑YOLOv9、YOLOv10和YOLO11。\n- 论文[12]详细分析了从YOLOv1到YOLOv4的单阶段物体检测器，并比较了两阶段和单阶段物体检测器。\n- 论文[53]探讨了YOLO从版本1到10的演变，强调了其在汽车安全、医疗保健等领域的应用。\n- 论文[61]讨论了YOLO算法的发展直到第四版，并提出了新的方法和挑战。\n- 论文[27]分析了YOLO算法的发展和性能，比较了从第8版到第8版的YOLO版本。\n\n### YOLO算法的应用：\n\n- YOLO算法在自动驾驶、医疗保健、工业制造、监控和农业等领域有广泛应用。\n- YOLOv8提供了多种应用，包括实例分割、姿态估计和定向物体检测（OOB）。\n\n### YOLO算法的基准测试：\n\n- 论文[14]进行了云层分割的基准测试，评估了不同算法的架构方法和性能。\n- 论文[22]提出了结合联邦学习以提高隐私、适应性和协作训练的通用性。\n- 论文[12]提供了单阶段和两阶段物体检测器的比较。\n- 论文[53]探讨了YOLO算法对未来AI驱动应用的潜在整合。\n- 论文[61]强调了YOLO算法在物体检测方面的挑战和需要进一步研究的地方。\n\n### YOLO算法的挑战：\n\n- YOLO算法在处理小物体和不同旋转角度的物体时面临挑战。\n- YOLOv9、YOLOv10和YOLO11的最新模型在准确性和效率方面表现出色，但在某些情况下仍需改进。\n\n### YOLO算法的改进：\n\n- YOLOv9引入了信息瓶颈原理和可逆函数来保留数据，提高了模型的收敛性和性能。\n- YOLOv10通过增强的CSP-Net主干和PAN层提高了梯度流动和减少了计算冗余。\n- YOLO11引入了C2PSA模块，结合了跨阶段部分网络和自注意力机制，提高了检测精度。\n\n### YOLO算法的未来方向：\n\n- 未来的研究可以专注于优化YOLOv10以提高其准确性，同时保持其速度和效率优势。\n- 继续改进架构设计可能会带来更先进的YOLO算法。\n\n### 研究贡献：\n\n- 本研究首次全面比较了YOLO11及其前身，并在三个多样化的数据集上评估了它们的性能。\n- 研究结果为工业界和学术界提供了选择最适合特定应用场景的YOLO算法的宝贵见解。\n\n通过这些分析，可以看出YOLO算法在不断演进和改进，以适应不同的应用需求和挑战。\n\n# Benchmark 设置\n\n### 数据集\n\n介绍了三种数据集，分别是Traffic Signs Dataset、Africa Wildlife Dataset和Ships/Vessels Dataset。以下是对这三种数据集的详细介绍：\n\n#### 1. Traffic Signs Dataset（交通标志数据集）\n\n- **来源**：由Radu Oprea在Kaggle上提供的开源数据集。\n- 特点：\n  - 包含约55个类别的交通标志图像。\n  - 训练集包含3253张图像，验证集包含1128张图像。\n  - 图像大小不一，初始尺寸为640x640像素。\n  - 为了平衡不同类别的数量，采用了欠采样技术。\n- **应用领域**：自动驾驶、交通管理、道路安全和智能交通系统。\n- 挑战：\n  - 目标物体大小变化较大。\n  - 不同类别之间的模式相似，增加了检测难度。\n\n#### 2. Africa Wildlife Dataset（非洲野生动物数据集）\n\n- **来源**：由Bianca Ferreira在Kaggle上设计的开源数据集。\n- 特点：\n  - 包含四种常见的非洲动物类别：水牛、大象、犀牛和斑马。\n  - 每个类别至少有376张图像，通过Google图像搜索收集并手动标注为YOLO格式。\n  - 数据集分为训练集、验证集和测试集，比例为70%、20%和10%。\n- **应用领域**：野生动物保护、反偷猎、生物多样性监测和生态研究。\n- 挑战：\n  - 目标物体的宽高比变化较大。\n  - 每张图像至少包含一种指定的动物类别，可能还包含其他类别的多个实例或发生情况。\n  - 目标物体重叠，增加了检测难度。\n\n#### 3. Ships/Vessels Dataset（船舶数据集）\n\n- **来源**：由Siddharth Sah从多个Roboflow数据集中收集并整理的开源数据集。\n- 特点：\n  - 包含约13.5k张图像，专门用于船舶检测。\n  - 每张图像都使用YOLO格式手动标注了边界框。\n  - 数据集分为训练集、验证集和测试集，比例为70%、20%和10%。\n- **应用领域**：海事安全、渔业管理、海洋污染监测、国防、海事安全和更多实际应用。\n- 挑战：\n  - 目标物体（船舶）相对较小。\n  - 目标物体具有不同的旋转角度，增加了检测难度。\n\n这些数据集在对象检测研究中具有重要意义，因为它们涵盖了不同大小、形状和密度的对象，能够全面评估YOLO算法在不同场景下的性能。\n\n### 模型\n\n#### 比较分析：Ultralytics vs 原始YOLO模型\n\n![](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134407205.png)\n\n在Traffic Signs数据集上，对Ultralytics提供的版本和原始模型进行比较分析，使用相同的超参数设置如表V所示。目标是为了强调突出Ultralytics提供的版本和原始模型之间的差异。由于Ultraytics缺乏对YOLO v4、YOLO v6、YOLO v7的支持，因此本文将这几个YOLO版本排除在外了。\n\n##### Ultralytics支持库中的模型和任务\n\n![image-20241202134825220](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134825220.png)\n\n根据表I，Ultralytics库为研究人员和程序员提供了各种YOLO模型，用于推理、验证、训练和导出。我们注意到Ultralytics不支持YOLOv1、YOLOv2、YOLOv4和YOLOv7。对于YOLOv6，库只支持配置文件.yaml，而不支持预训练的.pt模型。\n\n##### Ultralytics和原始模型的性能比较\n\n通过对Ultralytics模型及其原始版本在交通标志数据集上的比较分析，我们观察到Ultralytics版本和原始版本之间存在显著差异。例如，Ultralytics版本的YOLOv5n（nano）和YOLOv3表现优越，突显了Ultralytics所做的增强和优化。相反，原始版本的YOLOv9c（compact）略微优于其Ultralytics版本，可能是由于Ultralytics对该较新模型的优化不足。这些观察结果表明，Ultralytics模型经过了大量修改，直接比较原始版本和Ultralytics版本是不公平和不准确的。因此，本文将专注于Ultralytics支持的版本，以确保基准测试的一致性和公平性。\n\n###### YOLOv3u\n\n![image-20241202135426435](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135426435.png)\n\nYOLOv3基于其前身，旨在提高定位错误和检测效率，特别是对于较小的物体。它使用Darknet-53框架，该框架有53个卷积层，速度是ResNet-152的两倍。YOLOv3还结合了特征金字塔网络（FPN）的元素，如残差块、跳跃连接和上采样，以增强跨不同尺度的物体检测能力。该算法生成三个不同尺度的特征图，以32、16和8的因子对输入进行下采样，并使用三尺度检测机制来检测大、中、小尺寸物体，分别使用不同的特征图。尽管有所改进，YOLOv3在检测中等和大型物体时仍面临挑战，因此Ultralytics发布了YOLOv3u。YOLOv3u是YOLOv3的改进版本，使用无锚点检测方法，并提高了YOLOv3的准确性和速度，特别是对于中等和大型物体。\n\n###### YOLOv5u\n\n![image-20241202135925381](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135925381.png)\n\nYOLOv5由Glenn Jocher提出，从Darknet框架过渡到PyTorch，保留了YOLOv4的许多改进，并使用CSPDarknet作为其骨干。CSPDarknet是原始Darknet架构的修改版本，通过将特征图分成单独的路径来实现更高效的特征提取和减少计算成本。YOLOv5采用步幅卷积层，旨在减少内存和计算成本。此外，该版本采用空间金字塔池化快速（SPPF）模块，通过在不同尺度上池化特征并提供多尺度表示来工作。YOLOv5实现了多种增强，如马赛克、复制粘贴、随机仿射、MixUp、HSV增强和随机水平翻转。Ultralytics通过YOLOv5u积极改进该模型，采用无锚点检测方法，并在复杂物体的不同尺寸上实现了更好的整体性能。\n\n###### YOLOv8\n\n![image-20241202140006851](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140006851.png)\n\nUltralytics引入了YOLOv8，这是YOLO系列的重大进化，包括五个缩放版本。除了物体检测外，YOLOv8还提供了图像分类、姿态估计、实例分割和定向物体检测（OOB）等多种应用。关键特性包括类似于YOLOv5的主干，调整后的CSPLayer（现称为C2f模块），结合了高级特征和上下文信息以提高检测精度。YOLOv8还引入了一个语义分割模型YOLOv8-Seg，结合了CSPDarknet53特征提取器和C2F模块，在物体检测和语义分割基准测试中取得了最先进的结果，同时保持了高效率。\n\n###### YOLOv9\n\n![image-20241202140048800](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140048800.png)\n\nYOLOv9由Chien-Yao Wang、I-Hau Yeh和Hong-Yuan Mark Liao开发，使用信息瓶颈原理和可逆函数来在网络深度中保留关键数据，确保可靠的梯度生成并提高模型收敛性和性能。可逆函数可以在不丢失信息的情况下反转，这是YOLOv9架构的另一个基石。这种属性允许网络保持完整的信息流，使模型参数的更新更加准确。此外，YOLOv9提供了五个缩放版本，重点是轻量级模型，这些模型通常欠参数化，并且在前向过程中容易丢失重要信息。可编程梯度信息（PGI）是YOLOv9引入的一项重大进步。PGI是一种在训练期间动态调整梯度信息的方法，通过选择性关注最具信息量的梯度来优化学习效率。通过这种方式，PGI有助于保留可能在轻量级模型中丢失的关键信息。此外，YOLOv9还包括GELAN（梯度增强轻量级架构网络），这是一种新的架构改进，旨在通过优化网络内的计算路径来提高参数利用和计算效率。\n\n###### YOLOv10\n\n![image-20241202140137372](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140137372.png)\n\nYOLOv10由清华大学的研究人员开发，基于先前模型的优势进行了关键创新。该架构具有增强的CSP-Net（跨阶段部分网络）主干，以提高梯度流动和减少计算冗余。网络结构分为三部分：主干、颈部和检测头。颈部包括PAN（路径聚合网络）层，用于有效的多尺度特征融合。PAN旨在通过聚合不同层的特征来增强信息流，使网络能够更好地捕捉和结合不同尺度的细节，这对于检测不同大小的物体至关重要。此外，该版本还提供五个缩放版本，从纳米到超大。对于推理，One-to-One Head为每个物体生成单个最佳预测，消除了对非极大值抑制（NMS）的需求。通过移除对NMS的需求，YOLOv10减少了延迟并提高了后处理速度。此外，YOLOv10还包括NMS-Free Training，使用一致的双重分配来减少推理延迟，并优化了从效率和准确性角度的各种组件，包括轻量级分类头、空间-通道解耦下采样和排名引导块设计。此外，该模型还包括大核卷积和部分自注意力模块，以在不显著增加计算成本的情况下提高性能。\n\n###### YOLO11\n\n![image-20241202140334746](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140334746.png)\n\nYOLO11是Ultralytics推出的最新创新，基于其前身的发展，特别是YOLOv8。这一迭代提供了从纳米到超大的五种缩放模型，适用于各种应用。与YOLOv8一样，YOLO11包括物体检测、实例分割、图像分类、姿态估计和定向物体检测（OBB）等多种应用。关键改进包括引入C2PSA（跨阶段部分自注意力）模块，结合了跨阶段部分网络和自注意力机制的优势。这使得模型能够在多个层次上更有效地捕获上下文信息，提高物体检测精度，特别是对于小型和重叠物体。此外，在YOLO11中，C2f块被C3k2块取代，C3k2是CSP Bottleneck的自定义实现，使用两个卷积而不是YOLOv8中使用的一个大卷积。这个块使用较小的内核，在保持精度的同时提高了效率和速度。\n\n### 硬件和软件设置\n\n- 表III：实验的软件设置\n- 表IV：6个YOLO版本的不同尺寸的模型\n\n![image-20241202140352257](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140352257.png)\n\n![image-20241202140542480](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140542480.png)\n\n总结了用于评估YOLO模型的硬件和软件环境设置。\n\n1. **软件环境**：实验使用了Python 3.12、Ubuntu 22.04、CUDA 12.5、cuDNN 8.9.7、Ultralytics 8.2.55和WandB 0.17.4等软件包。\n2. **硬件环境**：实验在两块NVIDIA RTX 4090 GPU上进行，每块GPU拥有16,384个CUDA核心。\n3. **数据集处理**：针对交通标志数据集，应用了欠采样技术以确保数据集平衡，并将图像数量从4381减少到3233张。\n4. **训练验证测试分割**：非洲野生动物数据集和船只数据集分别按照70%训练、20%验证和10%测试的比例进行分割。\n5. **模型训练**：实验中训练了23个模型，涵盖了5种不同的YOLO版本，并使用了相似的超参数以确保公平比较。\n6. **模型规模**：交通标志数据集包含24个类别，平均每个类别约100张图像；非洲野生动物数据集包含4个类别，每个类别至少有376张图像；船只数据集专注于单一类别的小型物体检测。\n\n### 评估指标\n\n评估指标包括准确性、计算效率和模型大小三个方面：\n\n#### 准确性指标\n1. **Precision（精确率）**：\n   - 定义：正确预测的观察值与总预测观察值的比率。\n   - 计算公式：$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n   - 其中，TP（True Positives）为真正例，FP（False Positives）为假正例。\n\n2. **Recall（召回率）**：\n   - 定义：正确预测的观察值与所有实际观察值的比率。\n   - 计算公式：$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n   - 其中，FN（False Negatives）为假反例。\n\n3. **mAP50（Mean Average Precision at an IoU threshold of 0.50）**：\n   - 定义：在IoU（Intersection over Union）阈值为0.50时的平均精度均值。\n   - 计算公式：$$ \\text{mAP50} = \\frac{1}{|C|} \\sum_{c \\in C} \\text{AP}_c $$\n   - 其中，$C$ 是类别集合，$\\text{AP}_c$ 是类别 $c$ 的平均精度。\n\n4. **mAP50-95（Mean Average Precision across IoU thresholds from 0.50 to 0.95）**：\n   - 定义：在IoU阈值从0.50到0.95范围内的平均精度均值。\n   - 计算公式：$$ \\text{mAP50-95} = \\frac{1}{15} \\sum_{r=1}^{15} \\text{AP}_{0.50 + \\frac{r-1}{14} \\times 0.05} $$\n   - 其中，$r$ 表示IoU阈值的范围。\n\n#### 计算效率指标\n1. **Preprocessing Time（预处理时间）**：\n   - 定义：准备原始数据以输入模型所需的持续时间。\n\n2. **Inference Time（推理时间）**：\n   - 定义：模型处理输入数据并生成预测所需的持续时间。\n\n3. **Postprocessing Time（后处理时间）**：\n   - 定义：将模型的原始预测转换为最终可用格式所需的时间。\n\n4. **Total Time（总时间）**：\n   - 定义：预处理时间、推理时间和后处理时间的总和。\n\n5. **GFLOPs（Giga Floating-Point Operations Per Second）**：\n   - 定义：模型训练的计算能力，反映其效率。\n\n#### 模型大小指标\n1. **Size（大小）**：\n   - 定义：模型的实际磁盘大小及其参数数量。\n\n这些指标提供了对YOLO模型性能的全面概述，有助于在不同真实世界场景中选择最优的YOLO算法。\n\n# 实验结果和讨论\n\n![image-20241202142033886](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142033886.png)\n\n### 实验结果\n\n#### 交通信号数据集\n\nYOLO模型在检测交通标志方面的有效性，展示了各种精度范围。最高的mAP50-95为0.799，而最低的精度为0.64。另一方面，最高的mAP50为0.893，而最低的为0.722。mAP50和mAP50-95之间的显著差距表明，模型在处理不同大小的交通标志时，在较高阈值下遇到了困难，这反映了其检测算法中潜在的改进领域。\n\na) 准确性：如图8所示，YOLOv5ul展示了最高的准确性，实现了mAP50为0.866和mAP50-95为0.799。紧随其后的是YOLO11m，其mAP50-95为0.795，YOLO11l的mAP50-95为0.794。相比之下，YOLOv10n展示了最低的精度，其mAP50为0.722，mAP50-95为0.64，紧随其后的是YOLOv5un，其mAP50-95为0.665，如数据点在图8中所证明的。\n\n![image-20241202142326776](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142326776.png)\n\nb) 精度和召回率：图9阐明了考虑模型大小的情况下精度和召回率之间的权衡。像YOLO11m、YOLO10l、YOLOv9m、YOLOv5ux和YOLO111这样的模型展示了高精度和召回率，特别是YOLO11m实现了0.898的精度和0.826的召回率，同时模型大小为67.9Mb，而YOLOv10l实现了0.873的精度和0.807的召回率，但模型大小显著更大（126.8 Mb）。相比之下，较小的模型如YOLOv10n（精度0.722，召回率0.602）、YOLOv8n（精度0.749，召回率0.688）和YOLO11n（精度0.768，召回率0.695）在两个指标上都表现不佳。这突显了较大模型在交通标志数据集上的优越性能。此外，YOLOv5um的高精度（0.849）和低召回率（0.701）表明了对假阴性的倾向，而YOLOv3u的高召回率（0.849）和低精度（0.75）则表明了对假阳性的倾向。\n\n![image-20241202142423060](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142423060.png)\n\nc) 计算效率：在计算效率方面，YOLOv10n是最有效的，每张图片的处理时间为2ms，GFLOPs计数为8.3，如图10和11所示。YOLO11n紧随其后，处理时间为2.2ms，GFLOPs计数为6.4，而YOLOv3u-tiny的处理时间为2.4ms，GFLOPs计数为19，与其他快速模型相比，这使得它在计算上相对低效。然而，数据显示YOLOv9e、YOLOv9m、YOLOv9c和YOLOv9s是效率最低的，推理时间分别为16.1ms、12.1ms、11.6ms和11.1ms，GFLOPs计数分别为189.4、76.7、102.6和26.8。这些发现描绘了一个明显的权衡，即在精度和计算效率之间。\n\nd) 整体性能：在评估整体性能时，包括准确性、大小和模型效率，YOLO11m作为一个一致的表现最佳的模型脱颖而出。它实现了mAP50-95为0.795，推理时间为2.4ms，模型大小为38.8Mb，GFLOPs计数为67.9，如图8、10、11和表VI中详细说明的。紧随其后的是YOLO111（mAP50-95为0.794，推理时间为4.6ms，大小为49Mb，GFLOPs计数为86.8）和YOLOv10m（mAP50-95为0.781，推理时间为2.4ms，大小为32.1Mb，63.8 GFLOPs计数）。这些结果突显了这些模型在检测各种大小的交通标志方面的稳健性，同时保持了较短的推理时间和较小的模型大小。值得注意的是，YOLO11和YOLOv10家族在准确性和计算效率方面显著优于其他YOLO家族，因为它们的模型在这些数据集上一致超越了其他家族的对应物。\n\n![image-20241202142515870](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142515870.png)\n\n#### 非洲野生动物数据集\n\n![image-20241202142815914](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142815914.png)\n\n表 VII 展示了 YOLO 模型在非洲野生动物数据集上的性能。该数据集包含大型物体尺寸，重点关注 YOLO 模型预测大型物体的能力以及由于数据集大小而导致过拟合的风险。模型在各个方面的准确性都表现出色，最高性能的模型 mAP50-95 范围从 0.832 到 0.725。这个相对较短的范围反映了模型在检测和分类大型野生动物物体时保持高准确性的有效性。\n\na) 准确性：如图 12 所示，YOLOv9s 展现了出色的性能，具有高达 0.832 的 mAP50-95 和 0.956 的 mAP50，展示了其在各种 IoU 阈值下的稳健准确性。YOLOv9c 和 YOLOv9t 紧随其后，mAP50 分数分别为 0.96 和 0.948，召回率分别为 0.896。值得注意的是，YOLOv8n 实现了 mAP50-95 得分分别为 0.83 和 0.825。这些结果突出了 YOLOv9 系列从少量图像样本中有效学习模式的能力，使其特别适合于较小型的数据集。相比之下，YOLOv5un、YOLOv10n 和 YOLOv3u-tiny 显示出较低的 mAP50-95 得分，分别为 0.791、0.786 和 0.725，表明它们在准确性方面的局限性。较大的模型如 YOLO11x、YOLOv5ux、YOLOv5ul 和 YOLOv10l 的表现不佳，可以归因于过拟合，特别是考虑到数据集规模较小。\n\n![image-20241202142853898](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142853898.png)\n\nb) 精度和召回率：图 13 表明 YOLO8l 和 YOLO111 实现了最高的精度和召回率，精度值分别为 0.942 和 0.937，召回率分别为 0.898 和 0.896。值得注意的是，YOLOv8n 实现了 0.932 的精度和 0.908 的召回率。总体而言，YOLOv8l 和 YOLO111 在精度和召回率方面表现最佳，YOLOv8n 的表现也相当出色。然而，YOLOv11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。\n\n![image-20241202142924537](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142924537.png)\n\nc) 计算效率：如图 14 和 15 所示，YOLOv10n、YOLOv8n 和 YOLOv3u-tiny 是最快的模型，处理时间分别为 2ms 和 1.8ms，GFLOPs 计数分别为 8.2 和 19.1。前两个模型具有相同的处理速度和 GFLOPs 计数，如表 VII 中所示。相比之下，YOLOv9e 展现了最慢的处理时间，为 11.2ms，GFLOPs 计数为 189.3，其次是 YOLOv5ux，处理时间为 7.5ms，GFLOPs 计数为 246.2 GFLOPs 计数。这些结果表明，较大的模型通常需要更多的处理时间和硬件资源，强调了模型大小和处理效率之间的权衡。\n\n![image-20241202143004149](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143004149.png)\n\nd) 整体性能：表 VII 和图 13、14 和 15 中的结果表明，YOLOv9t 和 YOLOv9s 在各个指标上持续表现出色，提供高准确性，同时保持较小的模型大小、低 GFLOPs 和短的处理时间，展示了 YOLOv9 较小型模型的稳健性及其在小数据集上的有效性。相比之下，YOLO5ux 和 YOLO11x 尽管具有较大的尺寸和较长的推理时间，但准确性表现不佳，可能是由于过拟合所致。大多数大型模型在这个数据集上的表现都不尽如人意，YOLOv10x 是一个例外，得益于现代架构防止过拟合，表现优异。\n\n![image-20241202143030167](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143030167.png)\n\n#### 船只和船舶数据集：\n\n![image-20241202143313627](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143313627.png)\n\n表 VIII 展示了 YOLO 模型在船只和船舶数据集上的性能，这是一个包含微小物体且旋转变化多样的大型数据集。总体而言，模型在检测船只和船舶方面表现出中等效果，mAP50-95 的范围从 0.273 到 0.327。这一表现表明 YOLO 算法在准确检测较小物体方面可能面临挑战，数据集中物体尺寸和旋转的多样性为测试模型能力提供了全面的测试。\n\na) 准确性：图 16 中 mAP50-95 和 mAP50 之间的差异凸显了 YOLO 模型在检测小物体时面临的挑战，尤其是在更高的 IoU 阈值下。此外，YOLO 模型在检测不同旋转的物体时也遇到困难。在各个模型中，YOLO11x 实现了最高的准确性，mAP50 为 0.529，mAP50-95 为 0.327，紧随其后的是 YOLO111、YOLO11m 和 YOLO11s，它们记录的 mAP50 值分别为 0.529、0.528 和 0.53，mAP50-95 值分别为 0.327、0.325 和 0.325。这些结果突出了 YOLO11 系列在检测小型和微小物体方面的稳健性。相比之下，YOLOv3u-tiny、YOLOv8n、YOLOv3u 和 YOLOv5n 展示了最低的准确性，mAP50 分数分别为 0.489、0.515、0.519 和 0.514，mAP50-95 分数分别为 0.273、0.297、0.298 和 0.298。这表明 YOLOv3u 的过时架构以及由于数据集规模较大而导致的小型模型的潜在欠拟合。\n\n![image-20241202143334789](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143334789.png)\n\nb) 精度和召回率：图 17 表明 YOLOv5ux 的表现优于其他模型，实现了 0.668 的精度和 0.555 的召回率。它紧随其后的是 YOLOv9m（精度为 0.668，召回率为 0.551）和 YOLOv8m（精度为 0.669，召回率为 0.525），两者在尺寸上显著较小（YOLOv9m 为 40.98 Mb，YOLOv8m 为 52.12 Mb）。相比之下，YOLO11n 和 YOLOv10s 表现较差，精度分别为 0.574 和 0.586，召回率分别为 0.51 和 0.511，这可能是由于欠拟合问题。总体而言，YOLO11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。\n\n![image-20241202143403226](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143403226.png)\n\nc) 计算效率：如图 18 和 19 所示，YOLOv3u-tiny 实现了最快的处理时间，为 2 毫秒，紧随其后的是 YOLOv8n 和 YOLOv5un，两者均记录了 2.3 毫秒。YOLOv10 和 YOLO11 模型也在速度上表现出色，YOLOv10n 和 YOLO11n 分别实现了 2.4 毫秒和 2.5 毫秒的快速推理时间，以及 8.2 和 6.3 的 GFLOPs 计数。相比之下，YOLOv9e 展现了最慢的速度，推理时间为 7.6 毫秒，GFLOPs 计数为 189.3，突显了 YOLOv9 系列在准确性和效率之间的权衡。\n\n![image-20241202143422503](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143422503.png)\n\nd) 整体性能：表 VIII 和图 16、17 和 18 中的结果表明，YOLO11s 和 YOLOv10s 在准确性方面表现优异，同时保持了紧凑的尺寸、低 GFLOPs 和快速的处理时间。相比之下，YOLOv3u、YOLOv8x 和 YOLOv8l 未能达到预期，尽管它们的尺寸较大且处理时间较长。这些发现突出了 YOLO11 系列的稳健性和可靠性，特别是在提高 YOLO 系列检测小型和微小物体的性能方面，同时确保高效处理。此外，结果还揭示了 YOLOv9 模型在面对大型数据集和小物体时的表现不佳，尽管它们具有现代架构。\n\n![image-20241202143442543](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143442543.png)\n\n### 讨论\n\n![image-20241202143815028](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143815028.png)\n\n基于三个数据集上模型的性能，我们按准确性、速度、GFLOps 计数和大小对它们进行了排名，如表 IX 所示，以便进行全面评估。对于准确性，由于 mAP50-95 指标能够评估模型在一系列 IoU 阈值下的表现，因此我们采用了该指标。对于速度，模型根据总处理时间进行排序，总处理时间包括预处理、推理和后处理持续时间。排名范围从第 1 名（表示最高性能）到第 28 名（表示最低性能），表中的相应排名已加粗显示。\n\n表 IX 的分析得出了几个关键观察结果：\n\n1) 准确性：YOLO11m 一致地成为顶级表现者，经常位居前列，紧随其后的是 YOLOv10x、YOLO111、YOLOv9m 和 YOLO11x。这突显了 YOLO11 系列在各种 IoU 阈值和物体大小下的稳健性能，这可以归因于它们使用 C2PSA 来保留上下文信息，从而提高了收敛性和整体性能。此外，大核卷积和部分自注意力模块的实施有助于提高算法的性能。\n\n相比之下，YOLOv3u-tiny 展现了最低的准确性，特别是在非洲野生动物和船只及船舶数据集上，YOLOv5un 和 YOLOv8n 的表现稍好但仍不理想。这表明 YOLO11 模型目前是要求高准确性的应用中最可靠的。\n\n紧随 YOLO11 系列之后，YOLOv9 模型在检测各种大小和不同 IoU 阈值的物体方面表现出色。然而，它们可能在检测小物体时遇到困难，这在船只和船舶数据集上可见。相比之下，YOLOv10 系列尽管推出较晚，但在交通标志和非洲动物数据集上的准确性相对较低，导致平均准确性下降了 2.075%，这可以归因于它们采用一对一头部方法而不是非极大值抑制（NMS）来定义边界框。这种策略在捕捉物体时可能会遇到困难，特别是在处理重叠物品时，因为它依赖于每个物体的单个预测。这一限制有助于解释第二个数据集中观察到的相对较差的结果。\n\nYOLOv3u 的过时架构也导致了其性能不佳，平均准确性比 YOLO11 模型低 6.5%。这种下降可以追溯到其对 2018 年首次引入的较旧 Darknet-53 框架的依赖，该框架可能无法充分应对当代检测挑战。\n\n2) 计算效率：YOLOv10n 在速度和 GFLOPs 计数方面始终表现优异，在所有三个数据集上均名列前茅，在速度方面排名第 1，在 GFLOPs 计数方面排名第 5。YOLOv3u-tiny、YOLOv10s 和 YOLO11n 也展示了显著的计算效率。\n\nYOLOv9e 展现了最慢的推理时间和非常高的 GFLOPs 计数，突显了准确性与效率之间的权衡。YOLO11 的速度提升可归因于它们使用的 C3k2 块，使其适用于需要快速处理的场景，超过了 YOLOv10 和 YOLOv9 模型，分别在速度上平均快了 %1.41 和 %31。\n\n虽然 YOLOv9 模型在准确性方面表现出色，但它们的推理时间却是最慢的，使它们不太适合对时间敏感的应用。相比之下，YOLOv10 模型虽然略慢于 YOLO11 变体，但仍提供了效率与速度之间的值得称赞的平衡。它们的表现非常适合时间敏感的场景，提供快速处理而不显著牺牲准确性，使它们成为实时应用的可行选择。\n\n3) 模型大小：YOLOv9t 是最小的模型，在所有三个数据集上均排名第一，其次是 YOLO11n 和 YOLOv10n。这种模型大小的效率突显了较新 YOLO 版本，特别是 YOLOv10，在高效参数利用方面的进步，实施了空间-通道解耦下采样。\n\nYOLOv3u 是最大的模型，突显了与其更现代的对应物相比，它的效率低下。\n\n4) 整体性能：考虑到准确性、速度、大小和 GFLOPs，YOLO11m、YOLOv11n、YOLO11s 和 YOLOv10s 成为最一致的表现者。它们实现了高准确性、低处理时间和功率以及高效的磁盘使用，使其适用于广泛的应用，其中速度和准确性都至关重要。\n\n相反，YOLOv9e、YOLOv5ux 和 YOLOv3u 在所有指标上的表现都较差，计算效率低下且相对于其大小表现不佳。YOLO11 模型显示出最佳的整体性能，可能是由于最近的增强功能，如 C3k2 块和 C2PSA 模块。紧随其后的是 YOLOv10 模型，尽管在准确性方面略有逊色，但由于其一对一头部用于预测的实施，在效率方面表现出色。虽然 YOLOv9 在计算效率方面表现不佳，但它在准确性方面仍然具有竞争力，这要归功于其 PGI 集成。这使 YOLOv9 成为优先考虑精度而非速度的应用的可行选择。\n\n此外，YOLOv8 和 YOLOv5u 展示了竞争性结果，超过了 YOLOv3u 的准确性，这可能是由于 YOLOv3u 的较旧架构。然而，它们的准确性仍然显著低于较新的模型，如 YOLOv9、YOLOv10 和 YOLO11。虽然 YOLOv8 和 YOLOv5u 的处理时间比 YOLOv9 快，但它们的整体表现仍然不如较新的模型。\n\n5) 物体大小和旋转检测：YOLO 算法在检测大中型物体方面效果很好，如非洲野生动物和交通标志数据集所证明的那样，准确性很高。然而，它在检测小物体方面存在困难，可能是由于将图像划分为网格，使得识别小而分辨率低的物体变得具有挑战性。此外，YOLO 在处理不同旋转的物体时也面临挑战，因为无法包围旋转物体，导致整体结果不佳。\n\n为了处理旋转物体，可以实现像 YOLO11 OBB[26] 和 YOLOv8 OBB[25]（定向边界框）这样的模型。保持与标准 YOLOv8 和 YOLO11 相同的基础架构，YOLOv8 OBB 和 YOLO11 OBB 用预测旋转矩形四个角点的头部替换了标准边界框预测头部，允许更准确的定位和表示任意方向的物体。\n\n6) YOLO11 对 YOLOv8 的崛起：尽管 YOLOv8[25] 因其在姿态估计、实例分割和定向物体检测（OBB）任务中的多功能性而成为算法的首选，但 YOLO11[26] 已经成为一个更高效和准确的替代品。通过处理相同任务的同时提供改进的上下文理解和更好的架构模块，YOLO11 设定了新的性能标准，在各种应用中的速度和准确性方面都超过了 YOLOv8。\n\n7) 数据集大小：数据集的大小显著影响 YOLO 模型的性能。例如，大型模型在小型非洲野生动物数据集上的表现不如在交通标志和船只及船舶数据集上的表现，因为它们更容易过拟合。相反，像 YOLOv9t 和 YOLOv9s 这样的小模型在非洲野生动物数据集上的表现显著更好，展示了小规模模型在处理有限数据集时的有效性。\n\n8) 训练数据集的影响：如表 VI、VII 和 VIII 所示，YOLO 模型的性能受到所使用的训练数据集的影响。不同的数据集产生不同的结果和顶尖表现者，表明数据集复杂性影响算法性能。这突显了在基准测试期间使用多样化数据集以获得每个模型优缺点全面结果的重要性。\n\n这次讨论强调了在选择 YOLO 模型进行特定应用时，需要平衡考虑准确性、速度和模型大小。YOLO11 模型在各个指标上的一致表现使它们非常适合于需要准确性和速度的多功能场景。同时，YOLOv10 模型可以在保持更快处理时间和更小模型大小的同时，类似地执行。此外，YOLOv9 可以在准确性方面提供可比的结果，但牺牲了速度，使其适用于优先考虑精度而非快速处理的应用。\n\n# 结论\n\n这项基准研究全面评估了各种 YOLO 算法的性能。它是首个对 YOLO11 及其前辈进行全面比较的研究，评估了它们在三个多样化数据集上的表现：交通标志、非洲野生动物和船只及船舶。这些数据集经过精心挑选，包含了广泛的物体属性，包括不同的物体大小、宽高比和物体密度。我们通过检查精度、召回率、平均精度均值（mAP）、处理时间、GFLOPs 计数和模型大小等一系列指标，展示了每个 YOLO 版本和家族的优势和劣势。我们的研究解决了以下关键研究问题：\n\n● 哪个 YOLO 算法在一系列综合指标上展示了卓越的性能？\n\n● 不同的 YOLO 版本在具有不同物体特征（如大小、宽高比和密度）的数据集上的表现如何？\n\n● 每个 YOLO 版本的具体优势和局限性是什么，这些见解如何指导选择最适合各种应用的算法？\n\n特别是，YOLO11 系列作为最一致的表现在各个指标上脱颖而出，YOLO11m 在准确性、效率、模型大小之间取得了最佳平衡。虽然 YOLOv10 的准确性略低于 YOLO11，但它在速度和效率方面表现出色，使其成为需要效率和快速处理的应用的强有力选择。此外，YOLOv9 总体上也表现良好，特别是在较小的数据集上表现尤为突出。这些发现为工业界和学术界提供了宝贵的见解，指导选择最适合的 YOLO 算法，并为未来的发展和改进提供信息。虽然评估的算法展示了有希望的性能，但仍有一些改进的空间。未来的研究可以专注于优化 YOLOv10，以提高其准确性，同时保持其速度和效率优势。此外，架构设计的持续进步可能为更突破性的 YOLO 算法铺平道路。我们未来的工作包括深入研究这些算法中确定的差距，并提出改进措施，以展示它们对整体效率的潜在影响。\n\n# 其它问题\n\n- **在交通标志数据集上，YOLOv5ul和YOLOv10n的性能差异是什么？原因是什么？**\n\n在交通标志数据集上，YOLOv5ul的mAP50-95达到了0.799，而YOLOv10n的mAP50-95仅为0.64，相差显著。YOLOv5ul在精度和召回率上都表现更好，具体来说，YOLOv5ul的Precision为0.866，Recall为0.849，而YOLOv10n的Precision为0.722，Recall为0.602。这种差异的原因可能包括：\n\n1. **模型架构改进**：YOLOv5ul采用了更先进的CSPDarknet53作为主干网络，并引入了Spatial Pyramid Pooling Fast（SPPF）模块，这些改进提高了模型的特征提取能力和多尺度适应性。\n2. **数据增强**：YOLOv5ul使用了多种数据增强技术，如Mosaic、Copy-Paste、Random Affine等，这些技术有助于提高模型的泛化能力和鲁棒性。\n3. **优化策略**：YOLOv5ul在训练过程中使用了更有效的优化策略，如AdamW优化器和学习率调度，这些策略有助于模型更快地收敛和提高性能。\n\n- **在船舶与船只数据集上，YOLOv11x为什么表现最好？与其他模型相比有哪些优势？**\n\n​\t在船舶与船只数据集上，YOLOv11x的mAP50-95达到了0.327，表现最好。与其他模型相比，YOLOv11x有以下优势：\n\n1. **小对象检测**：YOLOv11x特别适用于检测小对象，能够在复杂的图像环境中准确识别和定位小尺寸的船只。\n2. **架构改进**：YOLOv11x引入了C3k2块和C2PSA（Cross-Stage Partial with Self-Attention）模块，这些改进提高了模型的空间注意力和特征提取能力，特别是在处理重叠和小的对象时表现优异。\n3. **计算效率**：尽管YOLOv11x在精度上有所提升，但其推理时间仍然保持在2.4ms左右，保持了较高的计算效率，适合实时应用。\n\n- **在非洲野生动物数据集上，YOLOv9s的表现优于其他模型的原因是什么？**\n\n  在非洲野生动物数据集上，YOLOv9s的mAP50-95达到了0.832，表现优于其他模型。YOLOv9s之所以表现优异，主要原因包括：\n\n  1. **小型数据集适应性**：YOLOv9s在小型数据集上表现出色，能够有效学习对象的模式。其小型模型（如YOLOv9t）在非洲野生动物数据集上表现尤为突出，mAP50-95为0.832，mAP50为0.956。\n  2. **特征提取能力**：YOLOv9s采用了CSPNet（Cross-Stage Partial Network），这种结构通过分割特征图来提高特征提取效率，减少了计算复杂度。\n  3. **正则化技术**：YOLOv9s使用了GelAN（Gradient Enhanced Lightweight Architecture Network），这种技术通过优化网络内的计算路径，提高了参数利用率和计算效率。\n\n[Evaluating the Evolution of YOLO (You Only Look Once) Models: A Comprehensive Benchmark Study of YOLO11 and Its Predecessors](https://arxiv.org/html/2411.00201v1)\n\n![二维码](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg)","slug":"computer-vision/CV008-评估 YOLO （You Only Look Once） 模型的演变：YOLO11 及其前身的全面基准研究","published":1,"updated":"2024-12-07T13:07:19.881Z","_id":"cm4e4hino0005t4hi9q9w17sd","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>本研究对YOLO （You Only Look Once） 的各个版本进行了全面的基准测试分析，从 YOLOv3 到最新的算法。它代表了首次全面评估 YOLO11 性能的研究，YOLO11 是 YOLO 系列的最新成员。它评估了它们在三个不同数据集上的性能：交通标志（具有不同的对象大小）、非洲野生动物（具有不同的纵横比，每个图像至少有一个对象实例）以及船舶和船只（具有单个类别的小型对象），确保在具有不同挑战的数据集之间进行全面评估。为了确保稳健的评估，我们采用了一套全面的指标，包括精度、召回率、平均精度均值 （mAP）、处理时间、GFLOP 计数和模型大小。我们的分析强调了每个 YOLO 版本的独特优势和局限性。例如：YOLOv9 表现出很高的准确性，但在检测小物体和效率方面表现不佳，而 YOLOv10 表现出相对较低的准确性，因为架构选择会影响其在重叠物体检测方面的性能，但在速度和效率方面表现出色。此外，YOLO11 系列在准确性、速度、计算效率和模型大小方面始终表现出卓越的性能。YOLO11m 在准确性和效率之间取得了显著的平衡，在交通标志、非洲野生动物和船舶数据集上的mAP50-95得分分别为0.795、0.81和0.325，同时保持了2.4毫秒的平均推理时间，模型大小为38.8Mb，平均约为67.6 GFLOPs。这些结果为工业界和学术界提供了重要的见解，有助于为各种应用选择最合适的 YOLO 算法，并指导未来的增强功能。</p>\n<h1 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h1><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/x1.png\" alt=\"Refer to caption\"></p>\n<p>​                             Figure 1:Evolution of YOLO Algorithms throughout the years.</p>\n<p>主要介绍了物体检测在计算机视觉系统中的重要性及其应用，并概述了YOLO（You Only Look Once）算法的发展历程和优势。</p>\n<ul>\n<li><strong>物体检测的重要性</strong>：物体检测是计算机视觉系统的关键组成部分，广泛应用于自动驾驶、机器人技术、库存管理、视频监控和体育分析等领域。</li>\n<li><strong>传统方法的局限性</strong>：传统的物体检测方法如Viola-Jones算法和DPM模型在鲁棒性和泛化能力上存在局限，而深度学习方法已成为主流。</li>\n<li><strong>一阶段与两阶段方法</strong>：一阶段方法如RetinaNet和SSD在速度和准确性之间取得平衡，而两阶段方法如R-CNN提供高精度但计算密集。</li>\n<li><strong>YOLO算法的崛起</strong>：YOLO算法以其鲁棒性和效率脱颖而出，自2015年首次提出以来，通过不断改进框架和设计，成为实时物体检测的领先算法。</li>\n<li><strong>YOLO算法的演进</strong>：YOLO算法的演进包括从YOLOv1到YOLOv11的多个版本，每个版本都引入了新的架构和技术来提高性能。</li>\n<li><strong>Ultralytics的角色</strong>：Ultralytics在YOLO算法的发展中扮演了重要角色，通过维护和改进模型，使其更易于访问和定制。</li>\n<li><strong>研究目的</strong>：本研究旨在对YOLO算法的演变进行全面比较分析，特别是对最新成员YOLO11进行首次全面评估，并探讨其在不同应用场景中的优势和局限性。</li>\n<li><strong>研究方法</strong>：研究使用了三个多样化的数据集，并采用了一致的超参数设置，以确保公平和无偏见的比较。</li>\n<li><strong>研究贡献</strong>：研究的贡献在于提供了对YOLO11及其前身的全面比较，深入分析了这些算法的结构演变，并扩展了性能评估指标，为选择最适合特定用例的YOLO算法提供了宝贵的见解。</li>\n</ul>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p>主要回顾了YOLO算法的演变、不同版本的架构、以及与其他计算机视觉算法的基准测试。以下是对该章节的详细总结分析：</p>\n<h3 id=\"YOLO算法的演变：\"><a href=\"#YOLO算法的演变：\" class=\"headerlink\" title=\"YOLO算法的演变：\"></a>YOLO算法的演变：</h3><ul>\n<li>论文[14]分析了包括YOLOv8在内的七种语义分割和检测算法，用于云层分割的遥感图像。</li>\n<li>论文[22]回顾了YOLO从版本1到版本8的演变，但没有考虑YOLOv9、YOLOv10和YOLO11。</li>\n<li>论文[12]详细分析了从YOLOv1到YOLOv4的单阶段物体检测器，并比较了两阶段和单阶段物体检测器。</li>\n<li>论文[53]探讨了YOLO从版本1到10的演变，强调了其在汽车安全、医疗保健等领域的应用。</li>\n<li>论文[61]讨论了YOLO算法的发展直到第四版，并提出了新的方法和挑战。</li>\n<li>论文[27]分析了YOLO算法的发展和性能，比较了从第8版到第8版的YOLO版本。</li>\n</ul>\n<h3 id=\"YOLO算法的应用：\"><a href=\"#YOLO算法的应用：\" class=\"headerlink\" title=\"YOLO算法的应用：\"></a>YOLO算法的应用：</h3><ul>\n<li>YOLO算法在自动驾驶、医疗保健、工业制造、监控和农业等领域有广泛应用。</li>\n<li>YOLOv8提供了多种应用，包括实例分割、姿态估计和定向物体检测（OOB）。</li>\n</ul>\n<h3 id=\"YOLO算法的基准测试：\"><a href=\"#YOLO算法的基准测试：\" class=\"headerlink\" title=\"YOLO算法的基准测试：\"></a>YOLO算法的基准测试：</h3><ul>\n<li>论文[14]进行了云层分割的基准测试，评估了不同算法的架构方法和性能。</li>\n<li>论文[22]提出了结合联邦学习以提高隐私、适应性和协作训练的通用性。</li>\n<li>论文[12]提供了单阶段和两阶段物体检测器的比较。</li>\n<li>论文[53]探讨了YOLO算法对未来AI驱动应用的潜在整合。</li>\n<li>论文[61]强调了YOLO算法在物体检测方面的挑战和需要进一步研究的地方。</li>\n</ul>\n<h3 id=\"YOLO算法的挑战：\"><a href=\"#YOLO算法的挑战：\" class=\"headerlink\" title=\"YOLO算法的挑战：\"></a>YOLO算法的挑战：</h3><ul>\n<li>YOLO算法在处理小物体和不同旋转角度的物体时面临挑战。</li>\n<li>YOLOv9、YOLOv10和YOLO11的最新模型在准确性和效率方面表现出色，但在某些情况下仍需改进。</li>\n</ul>\n<h3 id=\"YOLO算法的改进：\"><a href=\"#YOLO算法的改进：\" class=\"headerlink\" title=\"YOLO算法的改进：\"></a>YOLO算法的改进：</h3><ul>\n<li>YOLOv9引入了信息瓶颈原理和可逆函数来保留数据，提高了模型的收敛性和性能。</li>\n<li>YOLOv10通过增强的CSP-Net主干和PAN层提高了梯度流动和减少了计算冗余。</li>\n<li>YOLO11引入了C2PSA模块，结合了跨阶段部分网络和自注意力机制，提高了检测精度。</li>\n</ul>\n<h3 id=\"YOLO算法的未来方向：\"><a href=\"#YOLO算法的未来方向：\" class=\"headerlink\" title=\"YOLO算法的未来方向：\"></a>YOLO算法的未来方向：</h3><ul>\n<li>未来的研究可以专注于优化YOLOv10以提高其准确性，同时保持其速度和效率优势。</li>\n<li>继续改进架构设计可能会带来更先进的YOLO算法。</li>\n</ul>\n<h3 id=\"研究贡献：\"><a href=\"#研究贡献：\" class=\"headerlink\" title=\"研究贡献：\"></a>研究贡献：</h3><ul>\n<li>本研究首次全面比较了YOLO11及其前身，并在三个多样化的数据集上评估了它们的性能。</li>\n<li>研究结果为工业界和学术界提供了选择最适合特定应用场景的YOLO算法的宝贵见解。</li>\n</ul>\n<p>通过这些分析，可以看出YOLO算法在不断演进和改进，以适应不同的应用需求和挑战。</p>\n<h1 id=\"Benchmark-设置\"><a href=\"#Benchmark-设置\" class=\"headerlink\" title=\"Benchmark 设置\"></a>Benchmark 设置</h1><h3 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h3><p>介绍了三种数据集，分别是Traffic Signs Dataset、Africa Wildlife Dataset和Ships&#x2F;Vessels Dataset。以下是对这三种数据集的详细介绍：</p>\n<h4 id=\"1-Traffic-Signs-Dataset（交通标志数据集）\"><a href=\"#1-Traffic-Signs-Dataset（交通标志数据集）\" class=\"headerlink\" title=\"1. Traffic Signs Dataset（交通标志数据集）\"></a>1. Traffic Signs Dataset（交通标志数据集）</h4><ul>\n<li><strong>来源</strong>：由Radu Oprea在Kaggle上提供的开源数据集。</li>\n<li>特点：<ul>\n<li>包含约55个类别的交通标志图像。</li>\n<li>训练集包含3253张图像，验证集包含1128张图像。</li>\n<li>图像大小不一，初始尺寸为640x640像素。</li>\n<li>为了平衡不同类别的数量，采用了欠采样技术。</li>\n</ul>\n</li>\n<li><strong>应用领域</strong>：自动驾驶、交通管理、道路安全和智能交通系统。</li>\n<li>挑战：<ul>\n<li>目标物体大小变化较大。</li>\n<li>不同类别之间的模式相似，增加了检测难度。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"2-Africa-Wildlife-Dataset（非洲野生动物数据集）\"><a href=\"#2-Africa-Wildlife-Dataset（非洲野生动物数据集）\" class=\"headerlink\" title=\"2. Africa Wildlife Dataset（非洲野生动物数据集）\"></a>2. Africa Wildlife Dataset（非洲野生动物数据集）</h4><ul>\n<li><strong>来源</strong>：由Bianca Ferreira在Kaggle上设计的开源数据集。</li>\n<li>特点：<ul>\n<li>包含四种常见的非洲动物类别：水牛、大象、犀牛和斑马。</li>\n<li>每个类别至少有376张图像，通过Google图像搜索收集并手动标注为YOLO格式。</li>\n<li>数据集分为训练集、验证集和测试集，比例为70%、20%和10%。</li>\n</ul>\n</li>\n<li><strong>应用领域</strong>：野生动物保护、反偷猎、生物多样性监测和生态研究。</li>\n<li>挑战：<ul>\n<li>目标物体的宽高比变化较大。</li>\n<li>每张图像至少包含一种指定的动物类别，可能还包含其他类别的多个实例或发生情况。</li>\n<li>目标物体重叠，增加了检测难度。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"3-Ships-Vessels-Dataset（船舶数据集）\"><a href=\"#3-Ships-Vessels-Dataset（船舶数据集）\" class=\"headerlink\" title=\"3. Ships&#x2F;Vessels Dataset（船舶数据集）\"></a>3. Ships&#x2F;Vessels Dataset（船舶数据集）</h4><ul>\n<li><strong>来源</strong>：由Siddharth Sah从多个Roboflow数据集中收集并整理的开源数据集。</li>\n<li>特点：<ul>\n<li>包含约13.5k张图像，专门用于船舶检测。</li>\n<li>每张图像都使用YOLO格式手动标注了边界框。</li>\n<li>数据集分为训练集、验证集和测试集，比例为70%、20%和10%。</li>\n</ul>\n</li>\n<li><strong>应用领域</strong>：海事安全、渔业管理、海洋污染监测、国防、海事安全和更多实际应用。</li>\n<li>挑战：<ul>\n<li>目标物体（船舶）相对较小。</li>\n<li>目标物体具有不同的旋转角度，增加了检测难度。</li>\n</ul>\n</li>\n</ul>\n<p>这些数据集在对象检测研究中具有重要意义，因为它们涵盖了不同大小、形状和密度的对象，能够全面评估YOLO算法在不同场景下的性能。</p>\n<h3 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h3><h4 id=\"比较分析：Ultralytics-vs-原始YOLO模型\"><a href=\"#比较分析：Ultralytics-vs-原始YOLO模型\" class=\"headerlink\" title=\"比较分析：Ultralytics vs 原始YOLO模型\"></a>比较分析：Ultralytics vs 原始YOLO模型</h4><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134407205.png\"></p>\n<p>在Traffic Signs数据集上，对Ultralytics提供的版本和原始模型进行比较分析，使用相同的超参数设置如表V所示。目标是为了强调突出Ultralytics提供的版本和原始模型之间的差异。由于Ultraytics缺乏对YOLO v4、YOLO v6、YOLO v7的支持，因此本文将这几个YOLO版本排除在外了。</p>\n<h5 id=\"Ultralytics支持库中的模型和任务\"><a href=\"#Ultralytics支持库中的模型和任务\" class=\"headerlink\" title=\"Ultralytics支持库中的模型和任务\"></a>Ultralytics支持库中的模型和任务</h5><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134825220.png\" alt=\"image-20241202134825220\"></p>\n<p>根据表I，Ultralytics库为研究人员和程序员提供了各种YOLO模型，用于推理、验证、训练和导出。我们注意到Ultralytics不支持YOLOv1、YOLOv2、YOLOv4和YOLOv7。对于YOLOv6，库只支持配置文件.yaml，而不支持预训练的.pt模型。</p>\n<h5 id=\"Ultralytics和原始模型的性能比较\"><a href=\"#Ultralytics和原始模型的性能比较\" class=\"headerlink\" title=\"Ultralytics和原始模型的性能比较\"></a>Ultralytics和原始模型的性能比较</h5><p>通过对Ultralytics模型及其原始版本在交通标志数据集上的比较分析，我们观察到Ultralytics版本和原始版本之间存在显著差异。例如，Ultralytics版本的YOLOv5n（nano）和YOLOv3表现优越，突显了Ultralytics所做的增强和优化。相反，原始版本的YOLOv9c（compact）略微优于其Ultralytics版本，可能是由于Ultralytics对该较新模型的优化不足。这些观察结果表明，Ultralytics模型经过了大量修改，直接比较原始版本和Ultralytics版本是不公平和不准确的。因此，本文将专注于Ultralytics支持的版本，以确保基准测试的一致性和公平性。</p>\n<h6 id=\"YOLOv3u\"><a href=\"#YOLOv3u\" class=\"headerlink\" title=\"YOLOv3u\"></a>YOLOv3u</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135426435.png\" alt=\"image-20241202135426435\"></p>\n<p>YOLOv3基于其前身，旨在提高定位错误和检测效率，特别是对于较小的物体。它使用Darknet-53框架，该框架有53个卷积层，速度是ResNet-152的两倍。YOLOv3还结合了特征金字塔网络（FPN）的元素，如残差块、跳跃连接和上采样，以增强跨不同尺度的物体检测能力。该算法生成三个不同尺度的特征图，以32、16和8的因子对输入进行下采样，并使用三尺度检测机制来检测大、中、小尺寸物体，分别使用不同的特征图。尽管有所改进，YOLOv3在检测中等和大型物体时仍面临挑战，因此Ultralytics发布了YOLOv3u。YOLOv3u是YOLOv3的改进版本，使用无锚点检测方法，并提高了YOLOv3的准确性和速度，特别是对于中等和大型物体。</p>\n<h6 id=\"YOLOv5u\"><a href=\"#YOLOv5u\" class=\"headerlink\" title=\"YOLOv5u\"></a>YOLOv5u</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135925381.png\" alt=\"image-20241202135925381\"></p>\n<p>YOLOv5由Glenn Jocher提出，从Darknet框架过渡到PyTorch，保留了YOLOv4的许多改进，并使用CSPDarknet作为其骨干。CSPDarknet是原始Darknet架构的修改版本，通过将特征图分成单独的路径来实现更高效的特征提取和减少计算成本。YOLOv5采用步幅卷积层，旨在减少内存和计算成本。此外，该版本采用空间金字塔池化快速（SPPF）模块，通过在不同尺度上池化特征并提供多尺度表示来工作。YOLOv5实现了多种增强，如马赛克、复制粘贴、随机仿射、MixUp、HSV增强和随机水平翻转。Ultralytics通过YOLOv5u积极改进该模型，采用无锚点检测方法，并在复杂物体的不同尺寸上实现了更好的整体性能。</p>\n<h6 id=\"YOLOv8\"><a href=\"#YOLOv8\" class=\"headerlink\" title=\"YOLOv8\"></a>YOLOv8</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140006851.png\" alt=\"image-20241202140006851\"></p>\n<p>Ultralytics引入了YOLOv8，这是YOLO系列的重大进化，包括五个缩放版本。除了物体检测外，YOLOv8还提供了图像分类、姿态估计、实例分割和定向物体检测（OOB）等多种应用。关键特性包括类似于YOLOv5的主干，调整后的CSPLayer（现称为C2f模块），结合了高级特征和上下文信息以提高检测精度。YOLOv8还引入了一个语义分割模型YOLOv8-Seg，结合了CSPDarknet53特征提取器和C2F模块，在物体检测和语义分割基准测试中取得了最先进的结果，同时保持了高效率。</p>\n<h6 id=\"YOLOv9\"><a href=\"#YOLOv9\" class=\"headerlink\" title=\"YOLOv9\"></a>YOLOv9</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140048800.png\" alt=\"image-20241202140048800\"></p>\n<p>YOLOv9由Chien-Yao Wang、I-Hau Yeh和Hong-Yuan Mark Liao开发，使用信息瓶颈原理和可逆函数来在网络深度中保留关键数据，确保可靠的梯度生成并提高模型收敛性和性能。可逆函数可以在不丢失信息的情况下反转，这是YOLOv9架构的另一个基石。这种属性允许网络保持完整的信息流，使模型参数的更新更加准确。此外，YOLOv9提供了五个缩放版本，重点是轻量级模型，这些模型通常欠参数化，并且在前向过程中容易丢失重要信息。可编程梯度信息（PGI）是YOLOv9引入的一项重大进步。PGI是一种在训练期间动态调整梯度信息的方法，通过选择性关注最具信息量的梯度来优化学习效率。通过这种方式，PGI有助于保留可能在轻量级模型中丢失的关键信息。此外，YOLOv9还包括GELAN（梯度增强轻量级架构网络），这是一种新的架构改进，旨在通过优化网络内的计算路径来提高参数利用和计算效率。</p>\n<h6 id=\"YOLOv10\"><a href=\"#YOLOv10\" class=\"headerlink\" title=\"YOLOv10\"></a>YOLOv10</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140137372.png\" alt=\"image-20241202140137372\"></p>\n<p>YOLOv10由清华大学的研究人员开发，基于先前模型的优势进行了关键创新。该架构具有增强的CSP-Net（跨阶段部分网络）主干，以提高梯度流动和减少计算冗余。网络结构分为三部分：主干、颈部和检测头。颈部包括PAN（路径聚合网络）层，用于有效的多尺度特征融合。PAN旨在通过聚合不同层的特征来增强信息流，使网络能够更好地捕捉和结合不同尺度的细节，这对于检测不同大小的物体至关重要。此外，该版本还提供五个缩放版本，从纳米到超大。对于推理，One-to-One Head为每个物体生成单个最佳预测，消除了对非极大值抑制（NMS）的需求。通过移除对NMS的需求，YOLOv10减少了延迟并提高了后处理速度。此外，YOLOv10还包括NMS-Free Training，使用一致的双重分配来减少推理延迟，并优化了从效率和准确性角度的各种组件，包括轻量级分类头、空间-通道解耦下采样和排名引导块设计。此外，该模型还包括大核卷积和部分自注意力模块，以在不显著增加计算成本的情况下提高性能。</p>\n<h6 id=\"YOLO11\"><a href=\"#YOLO11\" class=\"headerlink\" title=\"YOLO11\"></a>YOLO11</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140334746.png\" alt=\"image-20241202140334746\"></p>\n<p>YOLO11是Ultralytics推出的最新创新，基于其前身的发展，特别是YOLOv8。这一迭代提供了从纳米到超大的五种缩放模型，适用于各种应用。与YOLOv8一样，YOLO11包括物体检测、实例分割、图像分类、姿态估计和定向物体检测（OBB）等多种应用。关键改进包括引入C2PSA（跨阶段部分自注意力）模块，结合了跨阶段部分网络和自注意力机制的优势。这使得模型能够在多个层次上更有效地捕获上下文信息，提高物体检测精度，特别是对于小型和重叠物体。此外，在YOLO11中，C2f块被C3k2块取代，C3k2是CSP Bottleneck的自定义实现，使用两个卷积而不是YOLOv8中使用的一个大卷积。这个块使用较小的内核，在保持精度的同时提高了效率和速度。</p>\n<h3 id=\"硬件和软件设置\"><a href=\"#硬件和软件设置\" class=\"headerlink\" title=\"硬件和软件设置\"></a>硬件和软件设置</h3><ul>\n<li>表III：实验的软件设置</li>\n<li>表IV：6个YOLO版本的不同尺寸的模型</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140352257.png\" alt=\"image-20241202140352257\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140542480.png\" alt=\"image-20241202140542480\"></p>\n<p>总结了用于评估YOLO模型的硬件和软件环境设置。</p>\n<ol>\n<li><strong>软件环境</strong>：实验使用了Python 3.12、Ubuntu 22.04、CUDA 12.5、cuDNN 8.9.7、Ultralytics 8.2.55和WandB 0.17.4等软件包。</li>\n<li><strong>硬件环境</strong>：实验在两块NVIDIA RTX 4090 GPU上进行，每块GPU拥有16,384个CUDA核心。</li>\n<li><strong>数据集处理</strong>：针对交通标志数据集，应用了欠采样技术以确保数据集平衡，并将图像数量从4381减少到3233张。</li>\n<li><strong>训练验证测试分割</strong>：非洲野生动物数据集和船只数据集分别按照70%训练、20%验证和10%测试的比例进行分割。</li>\n<li><strong>模型训练</strong>：实验中训练了23个模型，涵盖了5种不同的YOLO版本，并使用了相似的超参数以确保公平比较。</li>\n<li><strong>模型规模</strong>：交通标志数据集包含24个类别，平均每个类别约100张图像；非洲野生动物数据集包含4个类别，每个类别至少有376张图像；船只数据集专注于单一类别的小型物体检测。</li>\n</ol>\n<h3 id=\"评估指标\"><a href=\"#评估指标\" class=\"headerlink\" title=\"评估指标\"></a>评估指标</h3><p>评估指标包括准确性、计算效率和模型大小三个方面：</p>\n<h4 id=\"准确性指标\"><a href=\"#准确性指标\" class=\"headerlink\" title=\"准确性指标\"></a>准确性指标</h4><ol>\n<li><p><strong>Precision（精确率）</strong>：</p>\n<ul>\n<li>定义：正确预测的观察值与总预测观察值的比率。</li>\n<li>计算公式：$$ \\text{Precision} &#x3D; \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$</li>\n<li>其中，TP（True Positives）为真正例，FP（False Positives）为假正例。</li>\n</ul>\n</li>\n<li><p><strong>Recall（召回率）</strong>：</p>\n<ul>\n<li>定义：正确预测的观察值与所有实际观察值的比率。</li>\n<li>计算公式：$$ \\text{Recall} &#x3D; \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$</li>\n<li>其中，FN（False Negatives）为假反例。</li>\n</ul>\n</li>\n<li><p><strong>mAP50（Mean Average Precision at an IoU threshold of 0.50）</strong>：</p>\n<ul>\n<li>定义：在IoU（Intersection over Union）阈值为0.50时的平均精度均值。</li>\n<li>计算公式：$$ \\text{mAP50} &#x3D; \\frac{1}{|C|} \\sum_{c \\in C} \\text{AP}_c $$</li>\n<li>其中，$C$ 是类别集合，$\\text{AP}_c$ 是类别 $c$ 的平均精度。</li>\n</ul>\n</li>\n<li><p><strong>mAP50-95（Mean Average Precision across IoU thresholds from 0.50 to 0.95）</strong>：</p>\n<ul>\n<li>定义：在IoU阈值从0.50到0.95范围内的平均精度均值。</li>\n<li>计算公式：$$ \\text{mAP50-95} &#x3D; \\frac{1}{15} \\sum_{r&#x3D;1}^{15} \\text{AP}_{0.50 + \\frac{r-1}{14} \\times 0.05} $$</li>\n<li>其中，$r$ 表示IoU阈值的范围。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"计算效率指标\"><a href=\"#计算效率指标\" class=\"headerlink\" title=\"计算效率指标\"></a>计算效率指标</h4><ol>\n<li><p><strong>Preprocessing Time（预处理时间）</strong>：</p>\n<ul>\n<li>定义：准备原始数据以输入模型所需的持续时间。</li>\n</ul>\n</li>\n<li><p><strong>Inference Time（推理时间）</strong>：</p>\n<ul>\n<li>定义：模型处理输入数据并生成预测所需的持续时间。</li>\n</ul>\n</li>\n<li><p><strong>Postprocessing Time（后处理时间）</strong>：</p>\n<ul>\n<li>定义：将模型的原始预测转换为最终可用格式所需的时间。</li>\n</ul>\n</li>\n<li><p><strong>Total Time（总时间）</strong>：</p>\n<ul>\n<li>定义：预处理时间、推理时间和后处理时间的总和。</li>\n</ul>\n</li>\n<li><p><strong>GFLOPs（Giga Floating-Point Operations Per Second）</strong>：</p>\n<ul>\n<li>定义：模型训练的计算能力，反映其效率。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"模型大小指标\"><a href=\"#模型大小指标\" class=\"headerlink\" title=\"模型大小指标\"></a>模型大小指标</h4><ol>\n<li><strong>Size（大小）</strong>：<ul>\n<li>定义：模型的实际磁盘大小及其参数数量。</li>\n</ul>\n</li>\n</ol>\n<p>这些指标提供了对YOLO模型性能的全面概述，有助于在不同真实世界场景中选择最优的YOLO算法。</p>\n<h1 id=\"实验结果和讨论\"><a href=\"#实验结果和讨论\" class=\"headerlink\" title=\"实验结果和讨论\"></a>实验结果和讨论</h1><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142033886.png\" alt=\"image-20241202142033886\"></p>\n<h3 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h3><h4 id=\"交通信号数据集\"><a href=\"#交通信号数据集\" class=\"headerlink\" title=\"交通信号数据集\"></a>交通信号数据集</h4><p>YOLO模型在检测交通标志方面的有效性，展示了各种精度范围。最高的mAP50-95为0.799，而最低的精度为0.64。另一方面，最高的mAP50为0.893，而最低的为0.722。mAP50和mAP50-95之间的显著差距表明，模型在处理不同大小的交通标志时，在较高阈值下遇到了困难，这反映了其检测算法中潜在的改进领域。</p>\n<p>a) 准确性：如图8所示，YOLOv5ul展示了最高的准确性，实现了mAP50为0.866和mAP50-95为0.799。紧随其后的是YOLO11m，其mAP50-95为0.795，YOLO11l的mAP50-95为0.794。相比之下，YOLOv10n展示了最低的精度，其mAP50为0.722，mAP50-95为0.64，紧随其后的是YOLOv5un，其mAP50-95为0.665，如数据点在图8中所证明的。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142326776.png\" alt=\"image-20241202142326776\"></p>\n<p>b) 精度和召回率：图9阐明了考虑模型大小的情况下精度和召回率之间的权衡。像YOLO11m、YOLO10l、YOLOv9m、YOLOv5ux和YOLO111这样的模型展示了高精度和召回率，特别是YOLO11m实现了0.898的精度和0.826的召回率，同时模型大小为67.9Mb，而YOLOv10l实现了0.873的精度和0.807的召回率，但模型大小显著更大（126.8 Mb）。相比之下，较小的模型如YOLOv10n（精度0.722，召回率0.602）、YOLOv8n（精度0.749，召回率0.688）和YOLO11n（精度0.768，召回率0.695）在两个指标上都表现不佳。这突显了较大模型在交通标志数据集上的优越性能。此外，YOLOv5um的高精度（0.849）和低召回率（0.701）表明了对假阴性的倾向，而YOLOv3u的高召回率（0.849）和低精度（0.75）则表明了对假阳性的倾向。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142423060.png\" alt=\"image-20241202142423060\"></p>\n<p>c) 计算效率：在计算效率方面，YOLOv10n是最有效的，每张图片的处理时间为2ms，GFLOPs计数为8.3，如图10和11所示。YOLO11n紧随其后，处理时间为2.2ms，GFLOPs计数为6.4，而YOLOv3u-tiny的处理时间为2.4ms，GFLOPs计数为19，与其他快速模型相比，这使得它在计算上相对低效。然而，数据显示YOLOv9e、YOLOv9m、YOLOv9c和YOLOv9s是效率最低的，推理时间分别为16.1ms、12.1ms、11.6ms和11.1ms，GFLOPs计数分别为189.4、76.7、102.6和26.8。这些发现描绘了一个明显的权衡，即在精度和计算效率之间。</p>\n<p>d) 整体性能：在评估整体性能时，包括准确性、大小和模型效率，YOLO11m作为一个一致的表现最佳的模型脱颖而出。它实现了mAP50-95为0.795，推理时间为2.4ms，模型大小为38.8Mb，GFLOPs计数为67.9，如图8、10、11和表VI中详细说明的。紧随其后的是YOLO111（mAP50-95为0.794，推理时间为4.6ms，大小为49Mb，GFLOPs计数为86.8）和YOLOv10m（mAP50-95为0.781，推理时间为2.4ms，大小为32.1Mb，63.8 GFLOPs计数）。这些结果突显了这些模型在检测各种大小的交通标志方面的稳健性，同时保持了较短的推理时间和较小的模型大小。值得注意的是，YOLO11和YOLOv10家族在准确性和计算效率方面显著优于其他YOLO家族，因为它们的模型在这些数据集上一致超越了其他家族的对应物。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142515870.png\" alt=\"image-20241202142515870\"></p>\n<h4 id=\"非洲野生动物数据集\"><a href=\"#非洲野生动物数据集\" class=\"headerlink\" title=\"非洲野生动物数据集\"></a>非洲野生动物数据集</h4><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142815914.png\" alt=\"image-20241202142815914\"></p>\n<p>表 VII 展示了 YOLO 模型在非洲野生动物数据集上的性能。该数据集包含大型物体尺寸，重点关注 YOLO 模型预测大型物体的能力以及由于数据集大小而导致过拟合的风险。模型在各个方面的准确性都表现出色，最高性能的模型 mAP50-95 范围从 0.832 到 0.725。这个相对较短的范围反映了模型在检测和分类大型野生动物物体时保持高准确性的有效性。</p>\n<p>a) 准确性：如图 12 所示，YOLOv9s 展现了出色的性能，具有高达 0.832 的 mAP50-95 和 0.956 的 mAP50，展示了其在各种 IoU 阈值下的稳健准确性。YOLOv9c 和 YOLOv9t 紧随其后，mAP50 分数分别为 0.96 和 0.948，召回率分别为 0.896。值得注意的是，YOLOv8n 实现了 mAP50-95 得分分别为 0.83 和 0.825。这些结果突出了 YOLOv9 系列从少量图像样本中有效学习模式的能力，使其特别适合于较小型的数据集。相比之下，YOLOv5un、YOLOv10n 和 YOLOv3u-tiny 显示出较低的 mAP50-95 得分，分别为 0.791、0.786 和 0.725，表明它们在准确性方面的局限性。较大的模型如 YOLO11x、YOLOv5ux、YOLOv5ul 和 YOLOv10l 的表现不佳，可以归因于过拟合，特别是考虑到数据集规模较小。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142853898.png\" alt=\"image-20241202142853898\"></p>\n<p>b) 精度和召回率：图 13 表明 YOLO8l 和 YOLO111 实现了最高的精度和召回率，精度值分别为 0.942 和 0.937，召回率分别为 0.898 和 0.896。值得注意的是，YOLOv8n 实现了 0.932 的精度和 0.908 的召回率。总体而言，YOLOv8l 和 YOLO111 在精度和召回率方面表现最佳，YOLOv8n 的表现也相当出色。然而，YOLOv11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142924537.png\" alt=\"image-20241202142924537\"></p>\n<p>c) 计算效率：如图 14 和 15 所示，YOLOv10n、YOLOv8n 和 YOLOv3u-tiny 是最快的模型，处理时间分别为 2ms 和 1.8ms，GFLOPs 计数分别为 8.2 和 19.1。前两个模型具有相同的处理速度和 GFLOPs 计数，如表 VII 中所示。相比之下，YOLOv9e 展现了最慢的处理时间，为 11.2ms，GFLOPs 计数为 189.3，其次是 YOLOv5ux，处理时间为 7.5ms，GFLOPs 计数为 246.2 GFLOPs 计数。这些结果表明，较大的模型通常需要更多的处理时间和硬件资源，强调了模型大小和处理效率之间的权衡。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143004149.png\" alt=\"image-20241202143004149\"></p>\n<p>d) 整体性能：表 VII 和图 13、14 和 15 中的结果表明，YOLOv9t 和 YOLOv9s 在各个指标上持续表现出色，提供高准确性，同时保持较小的模型大小、低 GFLOPs 和短的处理时间，展示了 YOLOv9 较小型模型的稳健性及其在小数据集上的有效性。相比之下，YOLO5ux 和 YOLO11x 尽管具有较大的尺寸和较长的推理时间，但准确性表现不佳，可能是由于过拟合所致。大多数大型模型在这个数据集上的表现都不尽如人意，YOLOv10x 是一个例外，得益于现代架构防止过拟合，表现优异。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143030167.png\" alt=\"image-20241202143030167\"></p>\n<h4 id=\"船只和船舶数据集：\"><a href=\"#船只和船舶数据集：\" class=\"headerlink\" title=\"船只和船舶数据集：\"></a>船只和船舶数据集：</h4><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143313627.png\" alt=\"image-20241202143313627\"></p>\n<p>表 VIII 展示了 YOLO 模型在船只和船舶数据集上的性能，这是一个包含微小物体且旋转变化多样的大型数据集。总体而言，模型在检测船只和船舶方面表现出中等效果，mAP50-95 的范围从 0.273 到 0.327。这一表现表明 YOLO 算法在准确检测较小物体方面可能面临挑战，数据集中物体尺寸和旋转的多样性为测试模型能力提供了全面的测试。</p>\n<p>a) 准确性：图 16 中 mAP50-95 和 mAP50 之间的差异凸显了 YOLO 模型在检测小物体时面临的挑战，尤其是在更高的 IoU 阈值下。此外，YOLO 模型在检测不同旋转的物体时也遇到困难。在各个模型中，YOLO11x 实现了最高的准确性，mAP50 为 0.529，mAP50-95 为 0.327，紧随其后的是 YOLO111、YOLO11m 和 YOLO11s，它们记录的 mAP50 值分别为 0.529、0.528 和 0.53，mAP50-95 值分别为 0.327、0.325 和 0.325。这些结果突出了 YOLO11 系列在检测小型和微小物体方面的稳健性。相比之下，YOLOv3u-tiny、YOLOv8n、YOLOv3u 和 YOLOv5n 展示了最低的准确性，mAP50 分数分别为 0.489、0.515、0.519 和 0.514，mAP50-95 分数分别为 0.273、0.297、0.298 和 0.298。这表明 YOLOv3u 的过时架构以及由于数据集规模较大而导致的小型模型的潜在欠拟合。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143334789.png\" alt=\"image-20241202143334789\"></p>\n<p>b) 精度和召回率：图 17 表明 YOLOv5ux 的表现优于其他模型，实现了 0.668 的精度和 0.555 的召回率。它紧随其后的是 YOLOv9m（精度为 0.668，召回率为 0.551）和 YOLOv8m（精度为 0.669，召回率为 0.525），两者在尺寸上显著较小（YOLOv9m 为 40.98 Mb，YOLOv8m 为 52.12 Mb）。相比之下，YOLO11n 和 YOLOv10s 表现较差，精度分别为 0.574 和 0.586，召回率分别为 0.51 和 0.511，这可能是由于欠拟合问题。总体而言，YOLO11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143403226.png\" alt=\"image-20241202143403226\"></p>\n<p>c) 计算效率：如图 18 和 19 所示，YOLOv3u-tiny 实现了最快的处理时间，为 2 毫秒，紧随其后的是 YOLOv8n 和 YOLOv5un，两者均记录了 2.3 毫秒。YOLOv10 和 YOLO11 模型也在速度上表现出色，YOLOv10n 和 YOLO11n 分别实现了 2.4 毫秒和 2.5 毫秒的快速推理时间，以及 8.2 和 6.3 的 GFLOPs 计数。相比之下，YOLOv9e 展现了最慢的速度，推理时间为 7.6 毫秒，GFLOPs 计数为 189.3，突显了 YOLOv9 系列在准确性和效率之间的权衡。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143422503.png\" alt=\"image-20241202143422503\"></p>\n<p>d) 整体性能：表 VIII 和图 16、17 和 18 中的结果表明，YOLO11s 和 YOLOv10s 在准确性方面表现优异，同时保持了紧凑的尺寸、低 GFLOPs 和快速的处理时间。相比之下，YOLOv3u、YOLOv8x 和 YOLOv8l 未能达到预期，尽管它们的尺寸较大且处理时间较长。这些发现突出了 YOLO11 系列的稳健性和可靠性，特别是在提高 YOLO 系列检测小型和微小物体的性能方面，同时确保高效处理。此外，结果还揭示了 YOLOv9 模型在面对大型数据集和小物体时的表现不佳，尽管它们具有现代架构。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143442543.png\" alt=\"image-20241202143442543\"></p>\n<h3 id=\"讨论\"><a href=\"#讨论\" class=\"headerlink\" title=\"讨论\"></a>讨论</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143815028.png\" alt=\"image-20241202143815028\"></p>\n<p>基于三个数据集上模型的性能，我们按准确性、速度、GFLOps 计数和大小对它们进行了排名，如表 IX 所示，以便进行全面评估。对于准确性，由于 mAP50-95 指标能够评估模型在一系列 IoU 阈值下的表现，因此我们采用了该指标。对于速度，模型根据总处理时间进行排序，总处理时间包括预处理、推理和后处理持续时间。排名范围从第 1 名（表示最高性能）到第 28 名（表示最低性能），表中的相应排名已加粗显示。</p>\n<p>表 IX 的分析得出了几个关键观察结果：</p>\n<ol>\n<li>准确性：YOLO11m 一致地成为顶级表现者，经常位居前列，紧随其后的是 YOLOv10x、YOLO111、YOLOv9m 和 YOLO11x。这突显了 YOLO11 系列在各种 IoU 阈值和物体大小下的稳健性能，这可以归因于它们使用 C2PSA 来保留上下文信息，从而提高了收敛性和整体性能。此外，大核卷积和部分自注意力模块的实施有助于提高算法的性能。</li>\n</ol>\n<p>相比之下，YOLOv3u-tiny 展现了最低的准确性，特别是在非洲野生动物和船只及船舶数据集上，YOLOv5un 和 YOLOv8n 的表现稍好但仍不理想。这表明 YOLO11 模型目前是要求高准确性的应用中最可靠的。</p>\n<p>紧随 YOLO11 系列之后，YOLOv9 模型在检测各种大小和不同 IoU 阈值的物体方面表现出色。然而，它们可能在检测小物体时遇到困难，这在船只和船舶数据集上可见。相比之下，YOLOv10 系列尽管推出较晚，但在交通标志和非洲动物数据集上的准确性相对较低，导致平均准确性下降了 2.075%，这可以归因于它们采用一对一头部方法而不是非极大值抑制（NMS）来定义边界框。这种策略在捕捉物体时可能会遇到困难，特别是在处理重叠物品时，因为它依赖于每个物体的单个预测。这一限制有助于解释第二个数据集中观察到的相对较差的结果。</p>\n<p>YOLOv3u 的过时架构也导致了其性能不佳，平均准确性比 YOLO11 模型低 6.5%。这种下降可以追溯到其对 2018 年首次引入的较旧 Darknet-53 框架的依赖，该框架可能无法充分应对当代检测挑战。</p>\n<ol start=\"2\">\n<li>计算效率：YOLOv10n 在速度和 GFLOPs 计数方面始终表现优异，在所有三个数据集上均名列前茅，在速度方面排名第 1，在 GFLOPs 计数方面排名第 5。YOLOv3u-tiny、YOLOv10s 和 YOLO11n 也展示了显著的计算效率。</li>\n</ol>\n<p>YOLOv9e 展现了最慢的推理时间和非常高的 GFLOPs 计数，突显了准确性与效率之间的权衡。YOLO11 的速度提升可归因于它们使用的 C3k2 块，使其适用于需要快速处理的场景，超过了 YOLOv10 和 YOLOv9 模型，分别在速度上平均快了 %1.41 和 %31。</p>\n<p>虽然 YOLOv9 模型在准确性方面表现出色，但它们的推理时间却是最慢的，使它们不太适合对时间敏感的应用。相比之下，YOLOv10 模型虽然略慢于 YOLO11 变体，但仍提供了效率与速度之间的值得称赞的平衡。它们的表现非常适合时间敏感的场景，提供快速处理而不显著牺牲准确性，使它们成为实时应用的可行选择。</p>\n<ol start=\"3\">\n<li>模型大小：YOLOv9t 是最小的模型，在所有三个数据集上均排名第一，其次是 YOLO11n 和 YOLOv10n。这种模型大小的效率突显了较新 YOLO 版本，特别是 YOLOv10，在高效参数利用方面的进步，实施了空间-通道解耦下采样。</li>\n</ol>\n<p>YOLOv3u 是最大的模型，突显了与其更现代的对应物相比，它的效率低下。</p>\n<ol start=\"4\">\n<li>整体性能：考虑到准确性、速度、大小和 GFLOPs，YOLO11m、YOLOv11n、YOLO11s 和 YOLOv10s 成为最一致的表现者。它们实现了高准确性、低处理时间和功率以及高效的磁盘使用，使其适用于广泛的应用，其中速度和准确性都至关重要。</li>\n</ol>\n<p>相反，YOLOv9e、YOLOv5ux 和 YOLOv3u 在所有指标上的表现都较差，计算效率低下且相对于其大小表现不佳。YOLO11 模型显示出最佳的整体性能，可能是由于最近的增强功能，如 C3k2 块和 C2PSA 模块。紧随其后的是 YOLOv10 模型，尽管在准确性方面略有逊色，但由于其一对一头部用于预测的实施，在效率方面表现出色。虽然 YOLOv9 在计算效率方面表现不佳，但它在准确性方面仍然具有竞争力，这要归功于其 PGI 集成。这使 YOLOv9 成为优先考虑精度而非速度的应用的可行选择。</p>\n<p>此外，YOLOv8 和 YOLOv5u 展示了竞争性结果，超过了 YOLOv3u 的准确性，这可能是由于 YOLOv3u 的较旧架构。然而，它们的准确性仍然显著低于较新的模型，如 YOLOv9、YOLOv10 和 YOLO11。虽然 YOLOv8 和 YOLOv5u 的处理时间比 YOLOv9 快，但它们的整体表现仍然不如较新的模型。</p>\n<ol start=\"5\">\n<li>物体大小和旋转检测：YOLO 算法在检测大中型物体方面效果很好，如非洲野生动物和交通标志数据集所证明的那样，准确性很高。然而，它在检测小物体方面存在困难，可能是由于将图像划分为网格，使得识别小而分辨率低的物体变得具有挑战性。此外，YOLO 在处理不同旋转的物体时也面临挑战，因为无法包围旋转物体，导致整体结果不佳。</li>\n</ol>\n<p>为了处理旋转物体，可以实现像 YOLO11 OBB[26] 和 YOLOv8 OBB[25]（定向边界框）这样的模型。保持与标准 YOLOv8 和 YOLO11 相同的基础架构，YOLOv8 OBB 和 YOLO11 OBB 用预测旋转矩形四个角点的头部替换了标准边界框预测头部，允许更准确的定位和表示任意方向的物体。</p>\n<ol start=\"6\">\n<li><p>YOLO11 对 YOLOv8 的崛起：尽管 YOLOv8[25] 因其在姿态估计、实例分割和定向物体检测（OBB）任务中的多功能性而成为算法的首选，但 YOLO11[26] 已经成为一个更高效和准确的替代品。通过处理相同任务的同时提供改进的上下文理解和更好的架构模块，YOLO11 设定了新的性能标准，在各种应用中的速度和准确性方面都超过了 YOLOv8。</p>\n</li>\n<li><p>数据集大小：数据集的大小显著影响 YOLO 模型的性能。例如，大型模型在小型非洲野生动物数据集上的表现不如在交通标志和船只及船舶数据集上的表现，因为它们更容易过拟合。相反，像 YOLOv9t 和 YOLOv9s 这样的小模型在非洲野生动物数据集上的表现显著更好，展示了小规模模型在处理有限数据集时的有效性。</p>\n</li>\n<li><p>训练数据集的影响：如表 VI、VII 和 VIII 所示，YOLO 模型的性能受到所使用的训练数据集的影响。不同的数据集产生不同的结果和顶尖表现者，表明数据集复杂性影响算法性能。这突显了在基准测试期间使用多样化数据集以获得每个模型优缺点全面结果的重要性。</p>\n</li>\n</ol>\n<p>这次讨论强调了在选择 YOLO 模型进行特定应用时，需要平衡考虑准确性、速度和模型大小。YOLO11 模型在各个指标上的一致表现使它们非常适合于需要准确性和速度的多功能场景。同时，YOLOv10 模型可以在保持更快处理时间和更小模型大小的同时，类似地执行。此外，YOLOv9 可以在准确性方面提供可比的结果，但牺牲了速度，使其适用于优先考虑精度而非快速处理的应用。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>这项基准研究全面评估了各种 YOLO 算法的性能。它是首个对 YOLO11 及其前辈进行全面比较的研究，评估了它们在三个多样化数据集上的表现：交通标志、非洲野生动物和船只及船舶。这些数据集经过精心挑选，包含了广泛的物体属性，包括不同的物体大小、宽高比和物体密度。我们通过检查精度、召回率、平均精度均值（mAP）、处理时间、GFLOPs 计数和模型大小等一系列指标，展示了每个 YOLO 版本和家族的优势和劣势。我们的研究解决了以下关键研究问题：</p>\n<p>● 哪个 YOLO 算法在一系列综合指标上展示了卓越的性能？</p>\n<p>● 不同的 YOLO 版本在具有不同物体特征（如大小、宽高比和密度）的数据集上的表现如何？</p>\n<p>● 每个 YOLO 版本的具体优势和局限性是什么，这些见解如何指导选择最适合各种应用的算法？</p>\n<p>特别是，YOLO11 系列作为最一致的表现在各个指标上脱颖而出，YOLO11m 在准确性、效率、模型大小之间取得了最佳平衡。虽然 YOLOv10 的准确性略低于 YOLO11，但它在速度和效率方面表现出色，使其成为需要效率和快速处理的应用的强有力选择。此外，YOLOv9 总体上也表现良好，特别是在较小的数据集上表现尤为突出。这些发现为工业界和学术界提供了宝贵的见解，指导选择最适合的 YOLO 算法，并为未来的发展和改进提供信息。虽然评估的算法展示了有希望的性能，但仍有一些改进的空间。未来的研究可以专注于优化 YOLOv10，以提高其准确性，同时保持其速度和效率优势。此外，架构设计的持续进步可能为更突破性的 YOLO 算法铺平道路。我们未来的工作包括深入研究这些算法中确定的差距，并提出改进措施，以展示它们对整体效率的潜在影响。</p>\n<h1 id=\"其它问题\"><a href=\"#其它问题\" class=\"headerlink\" title=\"其它问题\"></a>其它问题</h1><ul>\n<li><strong>在交通标志数据集上，YOLOv5ul和YOLOv10n的性能差异是什么？原因是什么？</strong></li>\n</ul>\n<p>在交通标志数据集上，YOLOv5ul的mAP50-95达到了0.799，而YOLOv10n的mAP50-95仅为0.64，相差显著。YOLOv5ul在精度和召回率上都表现更好，具体来说，YOLOv5ul的Precision为0.866，Recall为0.849，而YOLOv10n的Precision为0.722，Recall为0.602。这种差异的原因可能包括：</p>\n<ol>\n<li><strong>模型架构改进</strong>：YOLOv5ul采用了更先进的CSPDarknet53作为主干网络，并引入了Spatial Pyramid Pooling Fast（SPPF）模块，这些改进提高了模型的特征提取能力和多尺度适应性。</li>\n<li><strong>数据增强</strong>：YOLOv5ul使用了多种数据增强技术，如Mosaic、Copy-Paste、Random Affine等，这些技术有助于提高模型的泛化能力和鲁棒性。</li>\n<li><strong>优化策略</strong>：YOLOv5ul在训练过程中使用了更有效的优化策略，如AdamW优化器和学习率调度，这些策略有助于模型更快地收敛和提高性能。</li>\n</ol>\n<ul>\n<li><strong>在船舶与船只数据集上，YOLOv11x为什么表现最好？与其他模型相比有哪些优势？</strong></li>\n</ul>\n<p>​\t在船舶与船只数据集上，YOLOv11x的mAP50-95达到了0.327，表现最好。与其他模型相比，YOLOv11x有以下优势：</p>\n<ol>\n<li><strong>小对象检测</strong>：YOLOv11x特别适用于检测小对象，能够在复杂的图像环境中准确识别和定位小尺寸的船只。</li>\n<li><strong>架构改进</strong>：YOLOv11x引入了C3k2块和C2PSA（Cross-Stage Partial with Self-Attention）模块，这些改进提高了模型的空间注意力和特征提取能力，特别是在处理重叠和小的对象时表现优异。</li>\n<li><strong>计算效率</strong>：尽管YOLOv11x在精度上有所提升，但其推理时间仍然保持在2.4ms左右，保持了较高的计算效率，适合实时应用。</li>\n</ol>\n<ul>\n<li><p><strong>在非洲野生动物数据集上，YOLOv9s的表现优于其他模型的原因是什么？</strong></p>\n<p>在非洲野生动物数据集上，YOLOv9s的mAP50-95达到了0.832，表现优于其他模型。YOLOv9s之所以表现优异，主要原因包括：</p>\n<ol>\n<li><strong>小型数据集适应性</strong>：YOLOv9s在小型数据集上表现出色，能够有效学习对象的模式。其小型模型（如YOLOv9t）在非洲野生动物数据集上表现尤为突出，mAP50-95为0.832，mAP50为0.956。</li>\n<li><strong>特征提取能力</strong>：YOLOv9s采用了CSPNet（Cross-Stage Partial Network），这种结构通过分割特征图来提高特征提取效率，减少了计算复杂度。</li>\n<li><strong>正则化技术</strong>：YOLOv9s使用了GelAN（Gradient Enhanced Lightweight Architecture Network），这种技术通过优化网络内的计算路径，提高了参数利用率和计算效率。</li>\n</ol>\n</li>\n</ul>\n<p><a href=\"https://arxiv.org/html/2411.00201v1\">Evaluating the Evolution of YOLO (You Only Look Once) Models: A Comprehensive Benchmark Study of YOLO11 and Its Predecessors</a></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg\" alt=\"二维码\"></p>\n","excerpt":"","more":"<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>本研究对YOLO （You Only Look Once） 的各个版本进行了全面的基准测试分析，从 YOLOv3 到最新的算法。它代表了首次全面评估 YOLO11 性能的研究，YOLO11 是 YOLO 系列的最新成员。它评估了它们在三个不同数据集上的性能：交通标志（具有不同的对象大小）、非洲野生动物（具有不同的纵横比，每个图像至少有一个对象实例）以及船舶和船只（具有单个类别的小型对象），确保在具有不同挑战的数据集之间进行全面评估。为了确保稳健的评估，我们采用了一套全面的指标，包括精度、召回率、平均精度均值 （mAP）、处理时间、GFLOP 计数和模型大小。我们的分析强调了每个 YOLO 版本的独特优势和局限性。例如：YOLOv9 表现出很高的准确性，但在检测小物体和效率方面表现不佳，而 YOLOv10 表现出相对较低的准确性，因为架构选择会影响其在重叠物体检测方面的性能，但在速度和效率方面表现出色。此外，YOLO11 系列在准确性、速度、计算效率和模型大小方面始终表现出卓越的性能。YOLO11m 在准确性和效率之间取得了显著的平衡，在交通标志、非洲野生动物和船舶数据集上的mAP50-95得分分别为0.795、0.81和0.325，同时保持了2.4毫秒的平均推理时间，模型大小为38.8Mb，平均约为67.6 GFLOPs。这些结果为工业界和学术界提供了重要的见解，有助于为各种应用选择最合适的 YOLO 算法，并指导未来的增强功能。</p>\n<h1 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h1><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/x1.png\" alt=\"Refer to caption\"></p>\n<p>​                             Figure 1:Evolution of YOLO Algorithms throughout the years.</p>\n<p>主要介绍了物体检测在计算机视觉系统中的重要性及其应用，并概述了YOLO（You Only Look Once）算法的发展历程和优势。</p>\n<ul>\n<li><strong>物体检测的重要性</strong>：物体检测是计算机视觉系统的关键组成部分，广泛应用于自动驾驶、机器人技术、库存管理、视频监控和体育分析等领域。</li>\n<li><strong>传统方法的局限性</strong>：传统的物体检测方法如Viola-Jones算法和DPM模型在鲁棒性和泛化能力上存在局限，而深度学习方法已成为主流。</li>\n<li><strong>一阶段与两阶段方法</strong>：一阶段方法如RetinaNet和SSD在速度和准确性之间取得平衡，而两阶段方法如R-CNN提供高精度但计算密集。</li>\n<li><strong>YOLO算法的崛起</strong>：YOLO算法以其鲁棒性和效率脱颖而出，自2015年首次提出以来，通过不断改进框架和设计，成为实时物体检测的领先算法。</li>\n<li><strong>YOLO算法的演进</strong>：YOLO算法的演进包括从YOLOv1到YOLOv11的多个版本，每个版本都引入了新的架构和技术来提高性能。</li>\n<li><strong>Ultralytics的角色</strong>：Ultralytics在YOLO算法的发展中扮演了重要角色，通过维护和改进模型，使其更易于访问和定制。</li>\n<li><strong>研究目的</strong>：本研究旨在对YOLO算法的演变进行全面比较分析，特别是对最新成员YOLO11进行首次全面评估，并探讨其在不同应用场景中的优势和局限性。</li>\n<li><strong>研究方法</strong>：研究使用了三个多样化的数据集，并采用了一致的超参数设置，以确保公平和无偏见的比较。</li>\n<li><strong>研究贡献</strong>：研究的贡献在于提供了对YOLO11及其前身的全面比较，深入分析了这些算法的结构演变，并扩展了性能评估指标，为选择最适合特定用例的YOLO算法提供了宝贵的见解。</li>\n</ul>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p>主要回顾了YOLO算法的演变、不同版本的架构、以及与其他计算机视觉算法的基准测试。以下是对该章节的详细总结分析：</p>\n<h3 id=\"YOLO算法的演变：\"><a href=\"#YOLO算法的演变：\" class=\"headerlink\" title=\"YOLO算法的演变：\"></a>YOLO算法的演变：</h3><ul>\n<li>论文[14]分析了包括YOLOv8在内的七种语义分割和检测算法，用于云层分割的遥感图像。</li>\n<li>论文[22]回顾了YOLO从版本1到版本8的演变，但没有考虑YOLOv9、YOLOv10和YOLO11。</li>\n<li>论文[12]详细分析了从YOLOv1到YOLOv4的单阶段物体检测器，并比较了两阶段和单阶段物体检测器。</li>\n<li>论文[53]探讨了YOLO从版本1到10的演变，强调了其在汽车安全、医疗保健等领域的应用。</li>\n<li>论文[61]讨论了YOLO算法的发展直到第四版，并提出了新的方法和挑战。</li>\n<li>论文[27]分析了YOLO算法的发展和性能，比较了从第8版到第8版的YOLO版本。</li>\n</ul>\n<h3 id=\"YOLO算法的应用：\"><a href=\"#YOLO算法的应用：\" class=\"headerlink\" title=\"YOLO算法的应用：\"></a>YOLO算法的应用：</h3><ul>\n<li>YOLO算法在自动驾驶、医疗保健、工业制造、监控和农业等领域有广泛应用。</li>\n<li>YOLOv8提供了多种应用，包括实例分割、姿态估计和定向物体检测（OOB）。</li>\n</ul>\n<h3 id=\"YOLO算法的基准测试：\"><a href=\"#YOLO算法的基准测试：\" class=\"headerlink\" title=\"YOLO算法的基准测试：\"></a>YOLO算法的基准测试：</h3><ul>\n<li>论文[14]进行了云层分割的基准测试，评估了不同算法的架构方法和性能。</li>\n<li>论文[22]提出了结合联邦学习以提高隐私、适应性和协作训练的通用性。</li>\n<li>论文[12]提供了单阶段和两阶段物体检测器的比较。</li>\n<li>论文[53]探讨了YOLO算法对未来AI驱动应用的潜在整合。</li>\n<li>论文[61]强调了YOLO算法在物体检测方面的挑战和需要进一步研究的地方。</li>\n</ul>\n<h3 id=\"YOLO算法的挑战：\"><a href=\"#YOLO算法的挑战：\" class=\"headerlink\" title=\"YOLO算法的挑战：\"></a>YOLO算法的挑战：</h3><ul>\n<li>YOLO算法在处理小物体和不同旋转角度的物体时面临挑战。</li>\n<li>YOLOv9、YOLOv10和YOLO11的最新模型在准确性和效率方面表现出色，但在某些情况下仍需改进。</li>\n</ul>\n<h3 id=\"YOLO算法的改进：\"><a href=\"#YOLO算法的改进：\" class=\"headerlink\" title=\"YOLO算法的改进：\"></a>YOLO算法的改进：</h3><ul>\n<li>YOLOv9引入了信息瓶颈原理和可逆函数来保留数据，提高了模型的收敛性和性能。</li>\n<li>YOLOv10通过增强的CSP-Net主干和PAN层提高了梯度流动和减少了计算冗余。</li>\n<li>YOLO11引入了C2PSA模块，结合了跨阶段部分网络和自注意力机制，提高了检测精度。</li>\n</ul>\n<h3 id=\"YOLO算法的未来方向：\"><a href=\"#YOLO算法的未来方向：\" class=\"headerlink\" title=\"YOLO算法的未来方向：\"></a>YOLO算法的未来方向：</h3><ul>\n<li>未来的研究可以专注于优化YOLOv10以提高其准确性，同时保持其速度和效率优势。</li>\n<li>继续改进架构设计可能会带来更先进的YOLO算法。</li>\n</ul>\n<h3 id=\"研究贡献：\"><a href=\"#研究贡献：\" class=\"headerlink\" title=\"研究贡献：\"></a>研究贡献：</h3><ul>\n<li>本研究首次全面比较了YOLO11及其前身，并在三个多样化的数据集上评估了它们的性能。</li>\n<li>研究结果为工业界和学术界提供了选择最适合特定应用场景的YOLO算法的宝贵见解。</li>\n</ul>\n<p>通过这些分析，可以看出YOLO算法在不断演进和改进，以适应不同的应用需求和挑战。</p>\n<h1 id=\"Benchmark-设置\"><a href=\"#Benchmark-设置\" class=\"headerlink\" title=\"Benchmark 设置\"></a>Benchmark 设置</h1><h3 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h3><p>介绍了三种数据集，分别是Traffic Signs Dataset、Africa Wildlife Dataset和Ships&#x2F;Vessels Dataset。以下是对这三种数据集的详细介绍：</p>\n<h4 id=\"1-Traffic-Signs-Dataset（交通标志数据集）\"><a href=\"#1-Traffic-Signs-Dataset（交通标志数据集）\" class=\"headerlink\" title=\"1. Traffic Signs Dataset（交通标志数据集）\"></a>1. Traffic Signs Dataset（交通标志数据集）</h4><ul>\n<li><strong>来源</strong>：由Radu Oprea在Kaggle上提供的开源数据集。</li>\n<li>特点：<ul>\n<li>包含约55个类别的交通标志图像。</li>\n<li>训练集包含3253张图像，验证集包含1128张图像。</li>\n<li>图像大小不一，初始尺寸为640x640像素。</li>\n<li>为了平衡不同类别的数量，采用了欠采样技术。</li>\n</ul>\n</li>\n<li><strong>应用领域</strong>：自动驾驶、交通管理、道路安全和智能交通系统。</li>\n<li>挑战：<ul>\n<li>目标物体大小变化较大。</li>\n<li>不同类别之间的模式相似，增加了检测难度。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"2-Africa-Wildlife-Dataset（非洲野生动物数据集）\"><a href=\"#2-Africa-Wildlife-Dataset（非洲野生动物数据集）\" class=\"headerlink\" title=\"2. Africa Wildlife Dataset（非洲野生动物数据集）\"></a>2. Africa Wildlife Dataset（非洲野生动物数据集）</h4><ul>\n<li><strong>来源</strong>：由Bianca Ferreira在Kaggle上设计的开源数据集。</li>\n<li>特点：<ul>\n<li>包含四种常见的非洲动物类别：水牛、大象、犀牛和斑马。</li>\n<li>每个类别至少有376张图像，通过Google图像搜索收集并手动标注为YOLO格式。</li>\n<li>数据集分为训练集、验证集和测试集，比例为70%、20%和10%。</li>\n</ul>\n</li>\n<li><strong>应用领域</strong>：野生动物保护、反偷猎、生物多样性监测和生态研究。</li>\n<li>挑战：<ul>\n<li>目标物体的宽高比变化较大。</li>\n<li>每张图像至少包含一种指定的动物类别，可能还包含其他类别的多个实例或发生情况。</li>\n<li>目标物体重叠，增加了检测难度。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"3-Ships-Vessels-Dataset（船舶数据集）\"><a href=\"#3-Ships-Vessels-Dataset（船舶数据集）\" class=\"headerlink\" title=\"3. Ships&#x2F;Vessels Dataset（船舶数据集）\"></a>3. Ships&#x2F;Vessels Dataset（船舶数据集）</h4><ul>\n<li><strong>来源</strong>：由Siddharth Sah从多个Roboflow数据集中收集并整理的开源数据集。</li>\n<li>特点：<ul>\n<li>包含约13.5k张图像，专门用于船舶检测。</li>\n<li>每张图像都使用YOLO格式手动标注了边界框。</li>\n<li>数据集分为训练集、验证集和测试集，比例为70%、20%和10%。</li>\n</ul>\n</li>\n<li><strong>应用领域</strong>：海事安全、渔业管理、海洋污染监测、国防、海事安全和更多实际应用。</li>\n<li>挑战：<ul>\n<li>目标物体（船舶）相对较小。</li>\n<li>目标物体具有不同的旋转角度，增加了检测难度。</li>\n</ul>\n</li>\n</ul>\n<p>这些数据集在对象检测研究中具有重要意义，因为它们涵盖了不同大小、形状和密度的对象，能够全面评估YOLO算法在不同场景下的性能。</p>\n<h3 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h3><h4 id=\"比较分析：Ultralytics-vs-原始YOLO模型\"><a href=\"#比较分析：Ultralytics-vs-原始YOLO模型\" class=\"headerlink\" title=\"比较分析：Ultralytics vs 原始YOLO模型\"></a>比较分析：Ultralytics vs 原始YOLO模型</h4><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134407205.png\"></p>\n<p>在Traffic Signs数据集上，对Ultralytics提供的版本和原始模型进行比较分析，使用相同的超参数设置如表V所示。目标是为了强调突出Ultralytics提供的版本和原始模型之间的差异。由于Ultraytics缺乏对YOLO v4、YOLO v6、YOLO v7的支持，因此本文将这几个YOLO版本排除在外了。</p>\n<h5 id=\"Ultralytics支持库中的模型和任务\"><a href=\"#Ultralytics支持库中的模型和任务\" class=\"headerlink\" title=\"Ultralytics支持库中的模型和任务\"></a>Ultralytics支持库中的模型和任务</h5><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134825220.png\" alt=\"image-20241202134825220\"></p>\n<p>根据表I，Ultralytics库为研究人员和程序员提供了各种YOLO模型，用于推理、验证、训练和导出。我们注意到Ultralytics不支持YOLOv1、YOLOv2、YOLOv4和YOLOv7。对于YOLOv6，库只支持配置文件.yaml，而不支持预训练的.pt模型。</p>\n<h5 id=\"Ultralytics和原始模型的性能比较\"><a href=\"#Ultralytics和原始模型的性能比较\" class=\"headerlink\" title=\"Ultralytics和原始模型的性能比较\"></a>Ultralytics和原始模型的性能比较</h5><p>通过对Ultralytics模型及其原始版本在交通标志数据集上的比较分析，我们观察到Ultralytics版本和原始版本之间存在显著差异。例如，Ultralytics版本的YOLOv5n（nano）和YOLOv3表现优越，突显了Ultralytics所做的增强和优化。相反，原始版本的YOLOv9c（compact）略微优于其Ultralytics版本，可能是由于Ultralytics对该较新模型的优化不足。这些观察结果表明，Ultralytics模型经过了大量修改，直接比较原始版本和Ultralytics版本是不公平和不准确的。因此，本文将专注于Ultralytics支持的版本，以确保基准测试的一致性和公平性。</p>\n<h6 id=\"YOLOv3u\"><a href=\"#YOLOv3u\" class=\"headerlink\" title=\"YOLOv3u\"></a>YOLOv3u</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135426435.png\" alt=\"image-20241202135426435\"></p>\n<p>YOLOv3基于其前身，旨在提高定位错误和检测效率，特别是对于较小的物体。它使用Darknet-53框架，该框架有53个卷积层，速度是ResNet-152的两倍。YOLOv3还结合了特征金字塔网络（FPN）的元素，如残差块、跳跃连接和上采样，以增强跨不同尺度的物体检测能力。该算法生成三个不同尺度的特征图，以32、16和8的因子对输入进行下采样，并使用三尺度检测机制来检测大、中、小尺寸物体，分别使用不同的特征图。尽管有所改进，YOLOv3在检测中等和大型物体时仍面临挑战，因此Ultralytics发布了YOLOv3u。YOLOv3u是YOLOv3的改进版本，使用无锚点检测方法，并提高了YOLOv3的准确性和速度，特别是对于中等和大型物体。</p>\n<h6 id=\"YOLOv5u\"><a href=\"#YOLOv5u\" class=\"headerlink\" title=\"YOLOv5u\"></a>YOLOv5u</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135925381.png\" alt=\"image-20241202135925381\"></p>\n<p>YOLOv5由Glenn Jocher提出，从Darknet框架过渡到PyTorch，保留了YOLOv4的许多改进，并使用CSPDarknet作为其骨干。CSPDarknet是原始Darknet架构的修改版本，通过将特征图分成单独的路径来实现更高效的特征提取和减少计算成本。YOLOv5采用步幅卷积层，旨在减少内存和计算成本。此外，该版本采用空间金字塔池化快速（SPPF）模块，通过在不同尺度上池化特征并提供多尺度表示来工作。YOLOv5实现了多种增强，如马赛克、复制粘贴、随机仿射、MixUp、HSV增强和随机水平翻转。Ultralytics通过YOLOv5u积极改进该模型，采用无锚点检测方法，并在复杂物体的不同尺寸上实现了更好的整体性能。</p>\n<h6 id=\"YOLOv8\"><a href=\"#YOLOv8\" class=\"headerlink\" title=\"YOLOv8\"></a>YOLOv8</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140006851.png\" alt=\"image-20241202140006851\"></p>\n<p>Ultralytics引入了YOLOv8，这是YOLO系列的重大进化，包括五个缩放版本。除了物体检测外，YOLOv8还提供了图像分类、姿态估计、实例分割和定向物体检测（OOB）等多种应用。关键特性包括类似于YOLOv5的主干，调整后的CSPLayer（现称为C2f模块），结合了高级特征和上下文信息以提高检测精度。YOLOv8还引入了一个语义分割模型YOLOv8-Seg，结合了CSPDarknet53特征提取器和C2F模块，在物体检测和语义分割基准测试中取得了最先进的结果，同时保持了高效率。</p>\n<h6 id=\"YOLOv9\"><a href=\"#YOLOv9\" class=\"headerlink\" title=\"YOLOv9\"></a>YOLOv9</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140048800.png\" alt=\"image-20241202140048800\"></p>\n<p>YOLOv9由Chien-Yao Wang、I-Hau Yeh和Hong-Yuan Mark Liao开发，使用信息瓶颈原理和可逆函数来在网络深度中保留关键数据，确保可靠的梯度生成并提高模型收敛性和性能。可逆函数可以在不丢失信息的情况下反转，这是YOLOv9架构的另一个基石。这种属性允许网络保持完整的信息流，使模型参数的更新更加准确。此外，YOLOv9提供了五个缩放版本，重点是轻量级模型，这些模型通常欠参数化，并且在前向过程中容易丢失重要信息。可编程梯度信息（PGI）是YOLOv9引入的一项重大进步。PGI是一种在训练期间动态调整梯度信息的方法，通过选择性关注最具信息量的梯度来优化学习效率。通过这种方式，PGI有助于保留可能在轻量级模型中丢失的关键信息。此外，YOLOv9还包括GELAN（梯度增强轻量级架构网络），这是一种新的架构改进，旨在通过优化网络内的计算路径来提高参数利用和计算效率。</p>\n<h6 id=\"YOLOv10\"><a href=\"#YOLOv10\" class=\"headerlink\" title=\"YOLOv10\"></a>YOLOv10</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140137372.png\" alt=\"image-20241202140137372\"></p>\n<p>YOLOv10由清华大学的研究人员开发，基于先前模型的优势进行了关键创新。该架构具有增强的CSP-Net（跨阶段部分网络）主干，以提高梯度流动和减少计算冗余。网络结构分为三部分：主干、颈部和检测头。颈部包括PAN（路径聚合网络）层，用于有效的多尺度特征融合。PAN旨在通过聚合不同层的特征来增强信息流，使网络能够更好地捕捉和结合不同尺度的细节，这对于检测不同大小的物体至关重要。此外，该版本还提供五个缩放版本，从纳米到超大。对于推理，One-to-One Head为每个物体生成单个最佳预测，消除了对非极大值抑制（NMS）的需求。通过移除对NMS的需求，YOLOv10减少了延迟并提高了后处理速度。此外，YOLOv10还包括NMS-Free Training，使用一致的双重分配来减少推理延迟，并优化了从效率和准确性角度的各种组件，包括轻量级分类头、空间-通道解耦下采样和排名引导块设计。此外，该模型还包括大核卷积和部分自注意力模块，以在不显著增加计算成本的情况下提高性能。</p>\n<h6 id=\"YOLO11\"><a href=\"#YOLO11\" class=\"headerlink\" title=\"YOLO11\"></a>YOLO11</h6><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140334746.png\" alt=\"image-20241202140334746\"></p>\n<p>YOLO11是Ultralytics推出的最新创新，基于其前身的发展，特别是YOLOv8。这一迭代提供了从纳米到超大的五种缩放模型，适用于各种应用。与YOLOv8一样，YOLO11包括物体检测、实例分割、图像分类、姿态估计和定向物体检测（OBB）等多种应用。关键改进包括引入C2PSA（跨阶段部分自注意力）模块，结合了跨阶段部分网络和自注意力机制的优势。这使得模型能够在多个层次上更有效地捕获上下文信息，提高物体检测精度，特别是对于小型和重叠物体。此外，在YOLO11中，C2f块被C3k2块取代，C3k2是CSP Bottleneck的自定义实现，使用两个卷积而不是YOLOv8中使用的一个大卷积。这个块使用较小的内核，在保持精度的同时提高了效率和速度。</p>\n<h3 id=\"硬件和软件设置\"><a href=\"#硬件和软件设置\" class=\"headerlink\" title=\"硬件和软件设置\"></a>硬件和软件设置</h3><ul>\n<li>表III：实验的软件设置</li>\n<li>表IV：6个YOLO版本的不同尺寸的模型</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140352257.png\" alt=\"image-20241202140352257\"></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140542480.png\" alt=\"image-20241202140542480\"></p>\n<p>总结了用于评估YOLO模型的硬件和软件环境设置。</p>\n<ol>\n<li><strong>软件环境</strong>：实验使用了Python 3.12、Ubuntu 22.04、CUDA 12.5、cuDNN 8.9.7、Ultralytics 8.2.55和WandB 0.17.4等软件包。</li>\n<li><strong>硬件环境</strong>：实验在两块NVIDIA RTX 4090 GPU上进行，每块GPU拥有16,384个CUDA核心。</li>\n<li><strong>数据集处理</strong>：针对交通标志数据集，应用了欠采样技术以确保数据集平衡，并将图像数量从4381减少到3233张。</li>\n<li><strong>训练验证测试分割</strong>：非洲野生动物数据集和船只数据集分别按照70%训练、20%验证和10%测试的比例进行分割。</li>\n<li><strong>模型训练</strong>：实验中训练了23个模型，涵盖了5种不同的YOLO版本，并使用了相似的超参数以确保公平比较。</li>\n<li><strong>模型规模</strong>：交通标志数据集包含24个类别，平均每个类别约100张图像；非洲野生动物数据集包含4个类别，每个类别至少有376张图像；船只数据集专注于单一类别的小型物体检测。</li>\n</ol>\n<h3 id=\"评估指标\"><a href=\"#评估指标\" class=\"headerlink\" title=\"评估指标\"></a>评估指标</h3><p>评估指标包括准确性、计算效率和模型大小三个方面：</p>\n<h4 id=\"准确性指标\"><a href=\"#准确性指标\" class=\"headerlink\" title=\"准确性指标\"></a>准确性指标</h4><ol>\n<li><p><strong>Precision（精确率）</strong>：</p>\n<ul>\n<li>定义：正确预测的观察值与总预测观察值的比率。</li>\n<li>计算公式：$$ \\text{Precision} &#x3D; \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$</li>\n<li>其中，TP（True Positives）为真正例，FP（False Positives）为假正例。</li>\n</ul>\n</li>\n<li><p><strong>Recall（召回率）</strong>：</p>\n<ul>\n<li>定义：正确预测的观察值与所有实际观察值的比率。</li>\n<li>计算公式：$$ \\text{Recall} &#x3D; \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$</li>\n<li>其中，FN（False Negatives）为假反例。</li>\n</ul>\n</li>\n<li><p><strong>mAP50（Mean Average Precision at an IoU threshold of 0.50）</strong>：</p>\n<ul>\n<li>定义：在IoU（Intersection over Union）阈值为0.50时的平均精度均值。</li>\n<li>计算公式：$$ \\text{mAP50} &#x3D; \\frac{1}{|C|} \\sum_{c \\in C} \\text{AP}_c $$</li>\n<li>其中，$C$ 是类别集合，$\\text{AP}_c$ 是类别 $c$ 的平均精度。</li>\n</ul>\n</li>\n<li><p><strong>mAP50-95（Mean Average Precision across IoU thresholds from 0.50 to 0.95）</strong>：</p>\n<ul>\n<li>定义：在IoU阈值从0.50到0.95范围内的平均精度均值。</li>\n<li>计算公式：$$ \\text{mAP50-95} &#x3D; \\frac{1}{15} \\sum_{r&#x3D;1}^{15} \\text{AP}_{0.50 + \\frac{r-1}{14} \\times 0.05} $$</li>\n<li>其中，$r$ 表示IoU阈值的范围。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"计算效率指标\"><a href=\"#计算效率指标\" class=\"headerlink\" title=\"计算效率指标\"></a>计算效率指标</h4><ol>\n<li><p><strong>Preprocessing Time（预处理时间）</strong>：</p>\n<ul>\n<li>定义：准备原始数据以输入模型所需的持续时间。</li>\n</ul>\n</li>\n<li><p><strong>Inference Time（推理时间）</strong>：</p>\n<ul>\n<li>定义：模型处理输入数据并生成预测所需的持续时间。</li>\n</ul>\n</li>\n<li><p><strong>Postprocessing Time（后处理时间）</strong>：</p>\n<ul>\n<li>定义：将模型的原始预测转换为最终可用格式所需的时间。</li>\n</ul>\n</li>\n<li><p><strong>Total Time（总时间）</strong>：</p>\n<ul>\n<li>定义：预处理时间、推理时间和后处理时间的总和。</li>\n</ul>\n</li>\n<li><p><strong>GFLOPs（Giga Floating-Point Operations Per Second）</strong>：</p>\n<ul>\n<li>定义：模型训练的计算能力，反映其效率。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"模型大小指标\"><a href=\"#模型大小指标\" class=\"headerlink\" title=\"模型大小指标\"></a>模型大小指标</h4><ol>\n<li><strong>Size（大小）</strong>：<ul>\n<li>定义：模型的实际磁盘大小及其参数数量。</li>\n</ul>\n</li>\n</ol>\n<p>这些指标提供了对YOLO模型性能的全面概述，有助于在不同真实世界场景中选择最优的YOLO算法。</p>\n<h1 id=\"实验结果和讨论\"><a href=\"#实验结果和讨论\" class=\"headerlink\" title=\"实验结果和讨论\"></a>实验结果和讨论</h1><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142033886.png\" alt=\"image-20241202142033886\"></p>\n<h3 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h3><h4 id=\"交通信号数据集\"><a href=\"#交通信号数据集\" class=\"headerlink\" title=\"交通信号数据集\"></a>交通信号数据集</h4><p>YOLO模型在检测交通标志方面的有效性，展示了各种精度范围。最高的mAP50-95为0.799，而最低的精度为0.64。另一方面，最高的mAP50为0.893，而最低的为0.722。mAP50和mAP50-95之间的显著差距表明，模型在处理不同大小的交通标志时，在较高阈值下遇到了困难，这反映了其检测算法中潜在的改进领域。</p>\n<p>a) 准确性：如图8所示，YOLOv5ul展示了最高的准确性，实现了mAP50为0.866和mAP50-95为0.799。紧随其后的是YOLO11m，其mAP50-95为0.795，YOLO11l的mAP50-95为0.794。相比之下，YOLOv10n展示了最低的精度，其mAP50为0.722，mAP50-95为0.64，紧随其后的是YOLOv5un，其mAP50-95为0.665，如数据点在图8中所证明的。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142326776.png\" alt=\"image-20241202142326776\"></p>\n<p>b) 精度和召回率：图9阐明了考虑模型大小的情况下精度和召回率之间的权衡。像YOLO11m、YOLO10l、YOLOv9m、YOLOv5ux和YOLO111这样的模型展示了高精度和召回率，特别是YOLO11m实现了0.898的精度和0.826的召回率，同时模型大小为67.9Mb，而YOLOv10l实现了0.873的精度和0.807的召回率，但模型大小显著更大（126.8 Mb）。相比之下，较小的模型如YOLOv10n（精度0.722，召回率0.602）、YOLOv8n（精度0.749，召回率0.688）和YOLO11n（精度0.768，召回率0.695）在两个指标上都表现不佳。这突显了较大模型在交通标志数据集上的优越性能。此外，YOLOv5um的高精度（0.849）和低召回率（0.701）表明了对假阴性的倾向，而YOLOv3u的高召回率（0.849）和低精度（0.75）则表明了对假阳性的倾向。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142423060.png\" alt=\"image-20241202142423060\"></p>\n<p>c) 计算效率：在计算效率方面，YOLOv10n是最有效的，每张图片的处理时间为2ms，GFLOPs计数为8.3，如图10和11所示。YOLO11n紧随其后，处理时间为2.2ms，GFLOPs计数为6.4，而YOLOv3u-tiny的处理时间为2.4ms，GFLOPs计数为19，与其他快速模型相比，这使得它在计算上相对低效。然而，数据显示YOLOv9e、YOLOv9m、YOLOv9c和YOLOv9s是效率最低的，推理时间分别为16.1ms、12.1ms、11.6ms和11.1ms，GFLOPs计数分别为189.4、76.7、102.6和26.8。这些发现描绘了一个明显的权衡，即在精度和计算效率之间。</p>\n<p>d) 整体性能：在评估整体性能时，包括准确性、大小和模型效率，YOLO11m作为一个一致的表现最佳的模型脱颖而出。它实现了mAP50-95为0.795，推理时间为2.4ms，模型大小为38.8Mb，GFLOPs计数为67.9，如图8、10、11和表VI中详细说明的。紧随其后的是YOLO111（mAP50-95为0.794，推理时间为4.6ms，大小为49Mb，GFLOPs计数为86.8）和YOLOv10m（mAP50-95为0.781，推理时间为2.4ms，大小为32.1Mb，63.8 GFLOPs计数）。这些结果突显了这些模型在检测各种大小的交通标志方面的稳健性，同时保持了较短的推理时间和较小的模型大小。值得注意的是，YOLO11和YOLOv10家族在准确性和计算效率方面显著优于其他YOLO家族，因为它们的模型在这些数据集上一致超越了其他家族的对应物。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142515870.png\" alt=\"image-20241202142515870\"></p>\n<h4 id=\"非洲野生动物数据集\"><a href=\"#非洲野生动物数据集\" class=\"headerlink\" title=\"非洲野生动物数据集\"></a>非洲野生动物数据集</h4><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142815914.png\" alt=\"image-20241202142815914\"></p>\n<p>表 VII 展示了 YOLO 模型在非洲野生动物数据集上的性能。该数据集包含大型物体尺寸，重点关注 YOLO 模型预测大型物体的能力以及由于数据集大小而导致过拟合的风险。模型在各个方面的准确性都表现出色，最高性能的模型 mAP50-95 范围从 0.832 到 0.725。这个相对较短的范围反映了模型在检测和分类大型野生动物物体时保持高准确性的有效性。</p>\n<p>a) 准确性：如图 12 所示，YOLOv9s 展现了出色的性能，具有高达 0.832 的 mAP50-95 和 0.956 的 mAP50，展示了其在各种 IoU 阈值下的稳健准确性。YOLOv9c 和 YOLOv9t 紧随其后，mAP50 分数分别为 0.96 和 0.948，召回率分别为 0.896。值得注意的是，YOLOv8n 实现了 mAP50-95 得分分别为 0.83 和 0.825。这些结果突出了 YOLOv9 系列从少量图像样本中有效学习模式的能力，使其特别适合于较小型的数据集。相比之下，YOLOv5un、YOLOv10n 和 YOLOv3u-tiny 显示出较低的 mAP50-95 得分，分别为 0.791、0.786 和 0.725，表明它们在准确性方面的局限性。较大的模型如 YOLO11x、YOLOv5ux、YOLOv5ul 和 YOLOv10l 的表现不佳，可以归因于过拟合，特别是考虑到数据集规模较小。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142853898.png\" alt=\"image-20241202142853898\"></p>\n<p>b) 精度和召回率：图 13 表明 YOLO8l 和 YOLO111 实现了最高的精度和召回率，精度值分别为 0.942 和 0.937，召回率分别为 0.898 和 0.896。值得注意的是，YOLOv8n 实现了 0.932 的精度和 0.908 的召回率。总体而言，YOLOv8l 和 YOLO111 在精度和召回率方面表现最佳，YOLOv8n 的表现也相当出色。然而，YOLOv11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142924537.png\" alt=\"image-20241202142924537\"></p>\n<p>c) 计算效率：如图 14 和 15 所示，YOLOv10n、YOLOv8n 和 YOLOv3u-tiny 是最快的模型，处理时间分别为 2ms 和 1.8ms，GFLOPs 计数分别为 8.2 和 19.1。前两个模型具有相同的处理速度和 GFLOPs 计数，如表 VII 中所示。相比之下，YOLOv9e 展现了最慢的处理时间，为 11.2ms，GFLOPs 计数为 189.3，其次是 YOLOv5ux，处理时间为 7.5ms，GFLOPs 计数为 246.2 GFLOPs 计数。这些结果表明，较大的模型通常需要更多的处理时间和硬件资源，强调了模型大小和处理效率之间的权衡。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143004149.png\" alt=\"image-20241202143004149\"></p>\n<p>d) 整体性能：表 VII 和图 13、14 和 15 中的结果表明，YOLOv9t 和 YOLOv9s 在各个指标上持续表现出色，提供高准确性，同时保持较小的模型大小、低 GFLOPs 和短的处理时间，展示了 YOLOv9 较小型模型的稳健性及其在小数据集上的有效性。相比之下，YOLO5ux 和 YOLO11x 尽管具有较大的尺寸和较长的推理时间，但准确性表现不佳，可能是由于过拟合所致。大多数大型模型在这个数据集上的表现都不尽如人意，YOLOv10x 是一个例外，得益于现代架构防止过拟合，表现优异。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143030167.png\" alt=\"image-20241202143030167\"></p>\n<h4 id=\"船只和船舶数据集：\"><a href=\"#船只和船舶数据集：\" class=\"headerlink\" title=\"船只和船舶数据集：\"></a>船只和船舶数据集：</h4><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143313627.png\" alt=\"image-20241202143313627\"></p>\n<p>表 VIII 展示了 YOLO 模型在船只和船舶数据集上的性能，这是一个包含微小物体且旋转变化多样的大型数据集。总体而言，模型在检测船只和船舶方面表现出中等效果，mAP50-95 的范围从 0.273 到 0.327。这一表现表明 YOLO 算法在准确检测较小物体方面可能面临挑战，数据集中物体尺寸和旋转的多样性为测试模型能力提供了全面的测试。</p>\n<p>a) 准确性：图 16 中 mAP50-95 和 mAP50 之间的差异凸显了 YOLO 模型在检测小物体时面临的挑战，尤其是在更高的 IoU 阈值下。此外，YOLO 模型在检测不同旋转的物体时也遇到困难。在各个模型中，YOLO11x 实现了最高的准确性，mAP50 为 0.529，mAP50-95 为 0.327，紧随其后的是 YOLO111、YOLO11m 和 YOLO11s，它们记录的 mAP50 值分别为 0.529、0.528 和 0.53，mAP50-95 值分别为 0.327、0.325 和 0.325。这些结果突出了 YOLO11 系列在检测小型和微小物体方面的稳健性。相比之下，YOLOv3u-tiny、YOLOv8n、YOLOv3u 和 YOLOv5n 展示了最低的准确性，mAP50 分数分别为 0.489、0.515、0.519 和 0.514，mAP50-95 分数分别为 0.273、0.297、0.298 和 0.298。这表明 YOLOv3u 的过时架构以及由于数据集规模较大而导致的小型模型的潜在欠拟合。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143334789.png\" alt=\"image-20241202143334789\"></p>\n<p>b) 精度和召回率：图 17 表明 YOLOv5ux 的表现优于其他模型，实现了 0.668 的精度和 0.555 的召回率。它紧随其后的是 YOLOv9m（精度为 0.668，召回率为 0.551）和 YOLOv8m（精度为 0.669，召回率为 0.525），两者在尺寸上显著较小（YOLOv9m 为 40.98 Mb，YOLOv8m 为 52.12 Mb）。相比之下，YOLO11n 和 YOLOv10s 表现较差，精度分别为 0.574 和 0.586，召回率分别为 0.51 和 0.511，这可能是由于欠拟合问题。总体而言，YOLO11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143403226.png\" alt=\"image-20241202143403226\"></p>\n<p>c) 计算效率：如图 18 和 19 所示，YOLOv3u-tiny 实现了最快的处理时间，为 2 毫秒，紧随其后的是 YOLOv8n 和 YOLOv5un，两者均记录了 2.3 毫秒。YOLOv10 和 YOLO11 模型也在速度上表现出色，YOLOv10n 和 YOLO11n 分别实现了 2.4 毫秒和 2.5 毫秒的快速推理时间，以及 8.2 和 6.3 的 GFLOPs 计数。相比之下，YOLOv9e 展现了最慢的速度，推理时间为 7.6 毫秒，GFLOPs 计数为 189.3，突显了 YOLOv9 系列在准确性和效率之间的权衡。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143422503.png\" alt=\"image-20241202143422503\"></p>\n<p>d) 整体性能：表 VIII 和图 16、17 和 18 中的结果表明，YOLO11s 和 YOLOv10s 在准确性方面表现优异，同时保持了紧凑的尺寸、低 GFLOPs 和快速的处理时间。相比之下，YOLOv3u、YOLOv8x 和 YOLOv8l 未能达到预期，尽管它们的尺寸较大且处理时间较长。这些发现突出了 YOLO11 系列的稳健性和可靠性，特别是在提高 YOLO 系列检测小型和微小物体的性能方面，同时确保高效处理。此外，结果还揭示了 YOLOv9 模型在面对大型数据集和小物体时的表现不佳，尽管它们具有现代架构。</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143442543.png\" alt=\"image-20241202143442543\"></p>\n<h3 id=\"讨论\"><a href=\"#讨论\" class=\"headerlink\" title=\"讨论\"></a>讨论</h3><p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143815028.png\" alt=\"image-20241202143815028\"></p>\n<p>基于三个数据集上模型的性能，我们按准确性、速度、GFLOps 计数和大小对它们进行了排名，如表 IX 所示，以便进行全面评估。对于准确性，由于 mAP50-95 指标能够评估模型在一系列 IoU 阈值下的表现，因此我们采用了该指标。对于速度，模型根据总处理时间进行排序，总处理时间包括预处理、推理和后处理持续时间。排名范围从第 1 名（表示最高性能）到第 28 名（表示最低性能），表中的相应排名已加粗显示。</p>\n<p>表 IX 的分析得出了几个关键观察结果：</p>\n<ol>\n<li>准确性：YOLO11m 一致地成为顶级表现者，经常位居前列，紧随其后的是 YOLOv10x、YOLO111、YOLOv9m 和 YOLO11x。这突显了 YOLO11 系列在各种 IoU 阈值和物体大小下的稳健性能，这可以归因于它们使用 C2PSA 来保留上下文信息，从而提高了收敛性和整体性能。此外，大核卷积和部分自注意力模块的实施有助于提高算法的性能。</li>\n</ol>\n<p>相比之下，YOLOv3u-tiny 展现了最低的准确性，特别是在非洲野生动物和船只及船舶数据集上，YOLOv5un 和 YOLOv8n 的表现稍好但仍不理想。这表明 YOLO11 模型目前是要求高准确性的应用中最可靠的。</p>\n<p>紧随 YOLO11 系列之后，YOLOv9 模型在检测各种大小和不同 IoU 阈值的物体方面表现出色。然而，它们可能在检测小物体时遇到困难，这在船只和船舶数据集上可见。相比之下，YOLOv10 系列尽管推出较晚，但在交通标志和非洲动物数据集上的准确性相对较低，导致平均准确性下降了 2.075%，这可以归因于它们采用一对一头部方法而不是非极大值抑制（NMS）来定义边界框。这种策略在捕捉物体时可能会遇到困难，特别是在处理重叠物品时，因为它依赖于每个物体的单个预测。这一限制有助于解释第二个数据集中观察到的相对较差的结果。</p>\n<p>YOLOv3u 的过时架构也导致了其性能不佳，平均准确性比 YOLO11 模型低 6.5%。这种下降可以追溯到其对 2018 年首次引入的较旧 Darknet-53 框架的依赖，该框架可能无法充分应对当代检测挑战。</p>\n<ol start=\"2\">\n<li>计算效率：YOLOv10n 在速度和 GFLOPs 计数方面始终表现优异，在所有三个数据集上均名列前茅，在速度方面排名第 1，在 GFLOPs 计数方面排名第 5。YOLOv3u-tiny、YOLOv10s 和 YOLO11n 也展示了显著的计算效率。</li>\n</ol>\n<p>YOLOv9e 展现了最慢的推理时间和非常高的 GFLOPs 计数，突显了准确性与效率之间的权衡。YOLO11 的速度提升可归因于它们使用的 C3k2 块，使其适用于需要快速处理的场景，超过了 YOLOv10 和 YOLOv9 模型，分别在速度上平均快了 %1.41 和 %31。</p>\n<p>虽然 YOLOv9 模型在准确性方面表现出色，但它们的推理时间却是最慢的，使它们不太适合对时间敏感的应用。相比之下，YOLOv10 模型虽然略慢于 YOLO11 变体，但仍提供了效率与速度之间的值得称赞的平衡。它们的表现非常适合时间敏感的场景，提供快速处理而不显著牺牲准确性，使它们成为实时应用的可行选择。</p>\n<ol start=\"3\">\n<li>模型大小：YOLOv9t 是最小的模型，在所有三个数据集上均排名第一，其次是 YOLO11n 和 YOLOv10n。这种模型大小的效率突显了较新 YOLO 版本，特别是 YOLOv10，在高效参数利用方面的进步，实施了空间-通道解耦下采样。</li>\n</ol>\n<p>YOLOv3u 是最大的模型，突显了与其更现代的对应物相比，它的效率低下。</p>\n<ol start=\"4\">\n<li>整体性能：考虑到准确性、速度、大小和 GFLOPs，YOLO11m、YOLOv11n、YOLO11s 和 YOLOv10s 成为最一致的表现者。它们实现了高准确性、低处理时间和功率以及高效的磁盘使用，使其适用于广泛的应用，其中速度和准确性都至关重要。</li>\n</ol>\n<p>相反，YOLOv9e、YOLOv5ux 和 YOLOv3u 在所有指标上的表现都较差，计算效率低下且相对于其大小表现不佳。YOLO11 模型显示出最佳的整体性能，可能是由于最近的增强功能，如 C3k2 块和 C2PSA 模块。紧随其后的是 YOLOv10 模型，尽管在准确性方面略有逊色，但由于其一对一头部用于预测的实施，在效率方面表现出色。虽然 YOLOv9 在计算效率方面表现不佳，但它在准确性方面仍然具有竞争力，这要归功于其 PGI 集成。这使 YOLOv9 成为优先考虑精度而非速度的应用的可行选择。</p>\n<p>此外，YOLOv8 和 YOLOv5u 展示了竞争性结果，超过了 YOLOv3u 的准确性，这可能是由于 YOLOv3u 的较旧架构。然而，它们的准确性仍然显著低于较新的模型，如 YOLOv9、YOLOv10 和 YOLO11。虽然 YOLOv8 和 YOLOv5u 的处理时间比 YOLOv9 快，但它们的整体表现仍然不如较新的模型。</p>\n<ol start=\"5\">\n<li>物体大小和旋转检测：YOLO 算法在检测大中型物体方面效果很好，如非洲野生动物和交通标志数据集所证明的那样，准确性很高。然而，它在检测小物体方面存在困难，可能是由于将图像划分为网格，使得识别小而分辨率低的物体变得具有挑战性。此外，YOLO 在处理不同旋转的物体时也面临挑战，因为无法包围旋转物体，导致整体结果不佳。</li>\n</ol>\n<p>为了处理旋转物体，可以实现像 YOLO11 OBB[26] 和 YOLOv8 OBB[25]（定向边界框）这样的模型。保持与标准 YOLOv8 和 YOLO11 相同的基础架构，YOLOv8 OBB 和 YOLO11 OBB 用预测旋转矩形四个角点的头部替换了标准边界框预测头部，允许更准确的定位和表示任意方向的物体。</p>\n<ol start=\"6\">\n<li><p>YOLO11 对 YOLOv8 的崛起：尽管 YOLOv8[25] 因其在姿态估计、实例分割和定向物体检测（OBB）任务中的多功能性而成为算法的首选，但 YOLO11[26] 已经成为一个更高效和准确的替代品。通过处理相同任务的同时提供改进的上下文理解和更好的架构模块，YOLO11 设定了新的性能标准，在各种应用中的速度和准确性方面都超过了 YOLOv8。</p>\n</li>\n<li><p>数据集大小：数据集的大小显著影响 YOLO 模型的性能。例如，大型模型在小型非洲野生动物数据集上的表现不如在交通标志和船只及船舶数据集上的表现，因为它们更容易过拟合。相反，像 YOLOv9t 和 YOLOv9s 这样的小模型在非洲野生动物数据集上的表现显著更好，展示了小规模模型在处理有限数据集时的有效性。</p>\n</li>\n<li><p>训练数据集的影响：如表 VI、VII 和 VIII 所示，YOLO 模型的性能受到所使用的训练数据集的影响。不同的数据集产生不同的结果和顶尖表现者，表明数据集复杂性影响算法性能。这突显了在基准测试期间使用多样化数据集以获得每个模型优缺点全面结果的重要性。</p>\n</li>\n</ol>\n<p>这次讨论强调了在选择 YOLO 模型进行特定应用时，需要平衡考虑准确性、速度和模型大小。YOLO11 模型在各个指标上的一致表现使它们非常适合于需要准确性和速度的多功能场景。同时，YOLOv10 模型可以在保持更快处理时间和更小模型大小的同时，类似地执行。此外，YOLOv9 可以在准确性方面提供可比的结果，但牺牲了速度，使其适用于优先考虑精度而非快速处理的应用。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>这项基准研究全面评估了各种 YOLO 算法的性能。它是首个对 YOLO11 及其前辈进行全面比较的研究，评估了它们在三个多样化数据集上的表现：交通标志、非洲野生动物和船只及船舶。这些数据集经过精心挑选，包含了广泛的物体属性，包括不同的物体大小、宽高比和物体密度。我们通过检查精度、召回率、平均精度均值（mAP）、处理时间、GFLOPs 计数和模型大小等一系列指标，展示了每个 YOLO 版本和家族的优势和劣势。我们的研究解决了以下关键研究问题：</p>\n<p>● 哪个 YOLO 算法在一系列综合指标上展示了卓越的性能？</p>\n<p>● 不同的 YOLO 版本在具有不同物体特征（如大小、宽高比和密度）的数据集上的表现如何？</p>\n<p>● 每个 YOLO 版本的具体优势和局限性是什么，这些见解如何指导选择最适合各种应用的算法？</p>\n<p>特别是，YOLO11 系列作为最一致的表现在各个指标上脱颖而出，YOLO11m 在准确性、效率、模型大小之间取得了最佳平衡。虽然 YOLOv10 的准确性略低于 YOLO11，但它在速度和效率方面表现出色，使其成为需要效率和快速处理的应用的强有力选择。此外，YOLOv9 总体上也表现良好，特别是在较小的数据集上表现尤为突出。这些发现为工业界和学术界提供了宝贵的见解，指导选择最适合的 YOLO 算法，并为未来的发展和改进提供信息。虽然评估的算法展示了有希望的性能，但仍有一些改进的空间。未来的研究可以专注于优化 YOLOv10，以提高其准确性，同时保持其速度和效率优势。此外，架构设计的持续进步可能为更突破性的 YOLO 算法铺平道路。我们未来的工作包括深入研究这些算法中确定的差距，并提出改进措施，以展示它们对整体效率的潜在影响。</p>\n<h1 id=\"其它问题\"><a href=\"#其它问题\" class=\"headerlink\" title=\"其它问题\"></a>其它问题</h1><ul>\n<li><strong>在交通标志数据集上，YOLOv5ul和YOLOv10n的性能差异是什么？原因是什么？</strong></li>\n</ul>\n<p>在交通标志数据集上，YOLOv5ul的mAP50-95达到了0.799，而YOLOv10n的mAP50-95仅为0.64，相差显著。YOLOv5ul在精度和召回率上都表现更好，具体来说，YOLOv5ul的Precision为0.866，Recall为0.849，而YOLOv10n的Precision为0.722，Recall为0.602。这种差异的原因可能包括：</p>\n<ol>\n<li><strong>模型架构改进</strong>：YOLOv5ul采用了更先进的CSPDarknet53作为主干网络，并引入了Spatial Pyramid Pooling Fast（SPPF）模块，这些改进提高了模型的特征提取能力和多尺度适应性。</li>\n<li><strong>数据增强</strong>：YOLOv5ul使用了多种数据增强技术，如Mosaic、Copy-Paste、Random Affine等，这些技术有助于提高模型的泛化能力和鲁棒性。</li>\n<li><strong>优化策略</strong>：YOLOv5ul在训练过程中使用了更有效的优化策略，如AdamW优化器和学习率调度，这些策略有助于模型更快地收敛和提高性能。</li>\n</ol>\n<ul>\n<li><strong>在船舶与船只数据集上，YOLOv11x为什么表现最好？与其他模型相比有哪些优势？</strong></li>\n</ul>\n<p>​\t在船舶与船只数据集上，YOLOv11x的mAP50-95达到了0.327，表现最好。与其他模型相比，YOLOv11x有以下优势：</p>\n<ol>\n<li><strong>小对象检测</strong>：YOLOv11x特别适用于检测小对象，能够在复杂的图像环境中准确识别和定位小尺寸的船只。</li>\n<li><strong>架构改进</strong>：YOLOv11x引入了C3k2块和C2PSA（Cross-Stage Partial with Self-Attention）模块，这些改进提高了模型的空间注意力和特征提取能力，特别是在处理重叠和小的对象时表现优异。</li>\n<li><strong>计算效率</strong>：尽管YOLOv11x在精度上有所提升，但其推理时间仍然保持在2.4ms左右，保持了较高的计算效率，适合实时应用。</li>\n</ol>\n<ul>\n<li><p><strong>在非洲野生动物数据集上，YOLOv9s的表现优于其他模型的原因是什么？</strong></p>\n<p>在非洲野生动物数据集上，YOLOv9s的mAP50-95达到了0.832，表现优于其他模型。YOLOv9s之所以表现优异，主要原因包括：</p>\n<ol>\n<li><strong>小型数据集适应性</strong>：YOLOv9s在小型数据集上表现出色，能够有效学习对象的模式。其小型模型（如YOLOv9t）在非洲野生动物数据集上表现尤为突出，mAP50-95为0.832，mAP50为0.956。</li>\n<li><strong>特征提取能力</strong>：YOLOv9s采用了CSPNet（Cross-Stage Partial Network），这种结构通过分割特征图来提高特征提取效率，减少了计算复杂度。</li>\n<li><strong>正则化技术</strong>：YOLOv9s使用了GelAN（Gradient Enhanced Lightweight Architecture Network），这种技术通过优化网络内的计算路径，提高了参数利用率和计算效率。</li>\n</ol>\n</li>\n</ul>\n<p><a href=\"https://arxiv.org/html/2411.00201v1\">Evaluating the Evolution of YOLO (You Only Look Once) Models: A Comprehensive Benchmark Study of YOLO11 and Its Predecessors</a></p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg\" alt=\"二维码\"></p>\n"},{"title":"transformers自定义数据集","date":"2024-12-07T10:30:00.000Z","_content":"\n\n\nhuggingface的transformers和dataset等库都默认把模型或者数据集下载保存到~/.cache目录下，但是模型或者数据集通常都比较大，占用home目录空间，这里就讲下如何设置数据集或者模型的下载保存的目录。\n\n以datasets库数据集下载为例。datasets数据集下载的相关配置在datasets.config.py模块中，如下图：\n\n![image-20231031165446512](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20231031165446512.png)\n\n变量DEFAULT_XDG_CACHE_HOME=\"~/.cache\",如果没有设置环境变量\"XDG_CACHE_HOME\"，则数据集默认保存在\"~/.cache\"目录下，因此，可以通过设置环境变量的方式修改数据集下载目录，如下：\n\n```python\n\"\"\"\nDescripttion: chengbo's code\nversion: 1.0.0\nAuthor: chengbo\nDate: 2023-10-30 12:06:23\nLastEditors: chengbo\nLastEditTime: 2023-10-30 12:06:30\n\"\"\"\nimport os\n\n#修改环境变量要在导入datasets或者transformers模块之前\nos.environ[\"XDG_CACHE_HOME\"] = \"/data/.cache\"  \n# os.environ[\"HF_CACHE_HOME\"] = \"/data/huggingface\"\n# os.environ[\"HF_DATASETS_CACHE\"] = \"/data/huggingface/datasets\"\nfrom datasets import load_dataset, DownloadConfig\nimport datasets\n\n\ndataset = load_dataset(\n    \"Graphcore/vqa\",\n    download_config=DownloadConfig(resume_download=True),\n    split=\"validation[:200]\",\n)\nprint(dataset[0])\nprint(os.getenv(\"XDG_CACHE_HOME\"))\nprint(os.getenv(\"HF_HOME\"))\nprint(datasets.config.HF_CACHE_HOME)\nprint(datasets.config.HF_DATASETS_CACHE)\nprint(datasets.config.EXTRACTED_DATASETS_PATH)\n\n```\n\n通过os.environ[\"XDG_CACHE_HOME\"] = \"/data/.cache\" 代码，后续的模型和数据集都会下载到该目录下。\n\n\n\n另外一种修改模型或者数据集下载目录的方式是通过参数配置。\n\n```python\nload_dataset(\"Graphcore/vqa\",cache_dir=\"/data/.cache\")\n```\n\n但是这种方式有个缺点，有些数据集下载的是个压缩文件，如.zip,下载后会进行自动解压，解压的默认目录还是在\"~/.cache\"目录下。从上面的图片中可以看到有个EXTRACTED_DATASETS_PATH 参数，这个参数就是设置解压的目录，可以通过修改该参数来配置解压后保存的目录。\n\n其它参数也可以参考上图config.py模块中的代码进行设置。\n\n\n\n文章合集：[chongzicbo/ReadWriteThink: 博学而笃志，切问而近思 (github.com)](https://github.com/chongzicbo/ReadWriteThink/tree/main)\n\n\n\n","source":"_posts/nlp/NLP025-huggingface自定义数据集和模型下载存储目录.md","raw":"---\ntitle: 'transformers自定义数据集'\ndate: 2024-12-7 18:30:00\ncategories:\n  - nlp\ntags:\n  - huggingface\n  - transformers\n\n---\n\n\n\nhuggingface的transformers和dataset等库都默认把模型或者数据集下载保存到~/.cache目录下，但是模型或者数据集通常都比较大，占用home目录空间，这里就讲下如何设置数据集或者模型的下载保存的目录。\n\n以datasets库数据集下载为例。datasets数据集下载的相关配置在datasets.config.py模块中，如下图：\n\n![image-20231031165446512](https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20231031165446512.png)\n\n变量DEFAULT_XDG_CACHE_HOME=\"~/.cache\",如果没有设置环境变量\"XDG_CACHE_HOME\"，则数据集默认保存在\"~/.cache\"目录下，因此，可以通过设置环境变量的方式修改数据集下载目录，如下：\n\n```python\n\"\"\"\nDescripttion: chengbo's code\nversion: 1.0.0\nAuthor: chengbo\nDate: 2023-10-30 12:06:23\nLastEditors: chengbo\nLastEditTime: 2023-10-30 12:06:30\n\"\"\"\nimport os\n\n#修改环境变量要在导入datasets或者transformers模块之前\nos.environ[\"XDG_CACHE_HOME\"] = \"/data/.cache\"  \n# os.environ[\"HF_CACHE_HOME\"] = \"/data/huggingface\"\n# os.environ[\"HF_DATASETS_CACHE\"] = \"/data/huggingface/datasets\"\nfrom datasets import load_dataset, DownloadConfig\nimport datasets\n\n\ndataset = load_dataset(\n    \"Graphcore/vqa\",\n    download_config=DownloadConfig(resume_download=True),\n    split=\"validation[:200]\",\n)\nprint(dataset[0])\nprint(os.getenv(\"XDG_CACHE_HOME\"))\nprint(os.getenv(\"HF_HOME\"))\nprint(datasets.config.HF_CACHE_HOME)\nprint(datasets.config.HF_DATASETS_CACHE)\nprint(datasets.config.EXTRACTED_DATASETS_PATH)\n\n```\n\n通过os.environ[\"XDG_CACHE_HOME\"] = \"/data/.cache\" 代码，后续的模型和数据集都会下载到该目录下。\n\n\n\n另外一种修改模型或者数据集下载目录的方式是通过参数配置。\n\n```python\nload_dataset(\"Graphcore/vqa\",cache_dir=\"/data/.cache\")\n```\n\n但是这种方式有个缺点，有些数据集下载的是个压缩文件，如.zip,下载后会进行自动解压，解压的默认目录还是在\"~/.cache\"目录下。从上面的图片中可以看到有个EXTRACTED_DATASETS_PATH 参数，这个参数就是设置解压的目录，可以通过修改该参数来配置解压后保存的目录。\n\n其它参数也可以参考上图config.py模块中的代码进行设置。\n\n\n\n文章合集：[chongzicbo/ReadWriteThink: 博学而笃志，切问而近思 (github.com)](https://github.com/chongzicbo/ReadWriteThink/tree/main)\n\n\n\n","slug":"nlp/NLP025-huggingface自定义数据集和模型下载存储目录","published":1,"updated":"2024-12-07T13:05:06.814Z","_id":"cm4e6qu0f00004ohif1af2946","comments":1,"layout":"post","photos":[],"content":"<p>huggingface的transformers和dataset等库都默认把模型或者数据集下载保存到~&#x2F;.cache目录下，但是模型或者数据集通常都比较大，占用home目录空间，这里就讲下如何设置数据集或者模型的下载保存的目录。</p>\n<p>以datasets库数据集下载为例。datasets数据集下载的相关配置在datasets.config.py模块中，如下图：</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20231031165446512.png\" alt=\"image-20231031165446512\"></p>\n<p>变量DEFAULT_XDG_CACHE_HOME&#x3D;”<del>&#x2F;.cache”,如果没有设置环境变量”XDG_CACHE_HOME”，则数据集默认保存在”</del>&#x2F;.cache”目录下，因此，可以通过设置环境变量的方式修改数据集下载目录，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-string\">&quot;&quot;&quot;</span><br><span class=\"hljs-string\">Descripttion: chengbo&#x27;s code</span><br><span class=\"hljs-string\">version: 1.0.0</span><br><span class=\"hljs-string\">Author: chengbo</span><br><span class=\"hljs-string\">Date: 2023-10-30 12:06:23</span><br><span class=\"hljs-string\">LastEditors: chengbo</span><br><span class=\"hljs-string\">LastEditTime: 2023-10-30 12:06:30</span><br><span class=\"hljs-string\">&quot;&quot;&quot;</span><br><span class=\"hljs-keyword\">import</span> os<br><br><span class=\"hljs-comment\">#修改环境变量要在导入datasets或者transformers模块之前</span><br>os.environ[<span class=\"hljs-string\">&quot;XDG_CACHE_HOME&quot;</span>] = <span class=\"hljs-string\">&quot;/data/.cache&quot;</span>  <br><span class=\"hljs-comment\"># os.environ[&quot;HF_CACHE_HOME&quot;] = &quot;/data/huggingface&quot;</span><br><span class=\"hljs-comment\"># os.environ[&quot;HF_DATASETS_CACHE&quot;] = &quot;/data/huggingface/datasets&quot;</span><br><span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\">import</span> load_dataset, DownloadConfig<br><span class=\"hljs-keyword\">import</span> datasets<br><br><br>dataset = load_dataset(<br>    <span class=\"hljs-string\">&quot;Graphcore/vqa&quot;</span>,<br>    download_config=DownloadConfig(resume_download=<span class=\"hljs-literal\">True</span>),<br>    split=<span class=\"hljs-string\">&quot;validation[:200]&quot;</span>,<br>)<br><span class=\"hljs-built_in\">print</span>(dataset[<span class=\"hljs-number\">0</span>])<br><span class=\"hljs-built_in\">print</span>(os.getenv(<span class=\"hljs-string\">&quot;XDG_CACHE_HOME&quot;</span>))<br><span class=\"hljs-built_in\">print</span>(os.getenv(<span class=\"hljs-string\">&quot;HF_HOME&quot;</span>))<br><span class=\"hljs-built_in\">print</span>(datasets.config.HF_CACHE_HOME)<br><span class=\"hljs-built_in\">print</span>(datasets.config.HF_DATASETS_CACHE)<br><span class=\"hljs-built_in\">print</span>(datasets.config.EXTRACTED_DATASETS_PATH)<br><br></code></pre></td></tr></table></figure>\n\n<p>通过os.environ[“XDG_CACHE_HOME”] &#x3D; “&#x2F;data&#x2F;.cache” 代码，后续的模型和数据集都会下载到该目录下。</p>\n<p>另外一种修改模型或者数据集下载目录的方式是通过参数配置。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">load_dataset(<span class=\"hljs-string\">&quot;Graphcore/vqa&quot;</span>,cache_dir=<span class=\"hljs-string\">&quot;/data/.cache&quot;</span>)<br></code></pre></td></tr></table></figure>\n\n<p>但是这种方式有个缺点，有些数据集下载的是个压缩文件，如.zip,下载后会进行自动解压，解压的默认目录还是在”~&#x2F;.cache”目录下。从上面的图片中可以看到有个EXTRACTED_DATASETS_PATH 参数，这个参数就是设置解压的目录，可以通过修改该参数来配置解压后保存的目录。</p>\n<p>其它参数也可以参考上图config.py模块中的代码进行设置。</p>\n<p>文章合集：<a href=\"https://github.com/chongzicbo/ReadWriteThink/tree/main\">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p>\n","excerpt":"","more":"<p>huggingface的transformers和dataset等库都默认把模型或者数据集下载保存到~&#x2F;.cache目录下，但是模型或者数据集通常都比较大，占用home目录空间，这里就讲下如何设置数据集或者模型的下载保存的目录。</p>\n<p>以datasets库数据集下载为例。datasets数据集下载的相关配置在datasets.config.py模块中，如下图：</p>\n<p><img src=\"https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20231031165446512.png\" alt=\"image-20231031165446512\"></p>\n<p>变量DEFAULT_XDG_CACHE_HOME&#x3D;”<del>&#x2F;.cache”,如果没有设置环境变量”XDG_CACHE_HOME”，则数据集默认保存在”</del>&#x2F;.cache”目录下，因此，可以通过设置环境变量的方式修改数据集下载目录，如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-string\">&quot;&quot;&quot;</span><br><span class=\"hljs-string\">Descripttion: chengbo&#x27;s code</span><br><span class=\"hljs-string\">version: 1.0.0</span><br><span class=\"hljs-string\">Author: chengbo</span><br><span class=\"hljs-string\">Date: 2023-10-30 12:06:23</span><br><span class=\"hljs-string\">LastEditors: chengbo</span><br><span class=\"hljs-string\">LastEditTime: 2023-10-30 12:06:30</span><br><span class=\"hljs-string\">&quot;&quot;&quot;</span><br><span class=\"hljs-keyword\">import</span> os<br><br><span class=\"hljs-comment\">#修改环境变量要在导入datasets或者transformers模块之前</span><br>os.environ[<span class=\"hljs-string\">&quot;XDG_CACHE_HOME&quot;</span>] = <span class=\"hljs-string\">&quot;/data/.cache&quot;</span>  <br><span class=\"hljs-comment\"># os.environ[&quot;HF_CACHE_HOME&quot;] = &quot;/data/huggingface&quot;</span><br><span class=\"hljs-comment\"># os.environ[&quot;HF_DATASETS_CACHE&quot;] = &quot;/data/huggingface/datasets&quot;</span><br><span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\">import</span> load_dataset, DownloadConfig<br><span class=\"hljs-keyword\">import</span> datasets<br><br><br>dataset = load_dataset(<br>    <span class=\"hljs-string\">&quot;Graphcore/vqa&quot;</span>,<br>    download_config=DownloadConfig(resume_download=<span class=\"hljs-literal\">True</span>),<br>    split=<span class=\"hljs-string\">&quot;validation[:200]&quot;</span>,<br>)<br><span class=\"hljs-built_in\">print</span>(dataset[<span class=\"hljs-number\">0</span>])<br><span class=\"hljs-built_in\">print</span>(os.getenv(<span class=\"hljs-string\">&quot;XDG_CACHE_HOME&quot;</span>))<br><span class=\"hljs-built_in\">print</span>(os.getenv(<span class=\"hljs-string\">&quot;HF_HOME&quot;</span>))<br><span class=\"hljs-built_in\">print</span>(datasets.config.HF_CACHE_HOME)<br><span class=\"hljs-built_in\">print</span>(datasets.config.HF_DATASETS_CACHE)<br><span class=\"hljs-built_in\">print</span>(datasets.config.EXTRACTED_DATASETS_PATH)<br><br></code></pre></td></tr></table></figure>\n\n<p>通过os.environ[“XDG_CACHE_HOME”] &#x3D; “&#x2F;data&#x2F;.cache” 代码，后续的模型和数据集都会下载到该目录下。</p>\n<p>另外一种修改模型或者数据集下载目录的方式是通过参数配置。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\">load_dataset(<span class=\"hljs-string\">&quot;Graphcore/vqa&quot;</span>,cache_dir=<span class=\"hljs-string\">&quot;/data/.cache&quot;</span>)<br></code></pre></td></tr></table></figure>\n\n<p>但是这种方式有个缺点，有些数据集下载的是个压缩文件，如.zip,下载后会进行自动解压，解压的默认目录还是在”~&#x2F;.cache”目录下。从上面的图片中可以看到有个EXTRACTED_DATASETS_PATH 参数，这个参数就是设置解压的目录，可以通过修改该参数来配置解压后保存的目录。</p>\n<p>其它参数也可以参考上图config.py模块中的代码进行设置。</p>\n<p>文章合集：<a href=\"https://github.com/chongzicbo/ReadWriteThink/tree/main\">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cm4e4hinj0001t4hieixca7te","category_id":"cm4e4hinn0003t4hi8ovwfpfg","_id":"cm4e4hinq0009t4hi5smndj9w"},{"post_id":"cm4e4hinl0002t4hi6yon8zxn","category_id":"cm4e4hinn0003t4hi8ovwfpfg","_id":"cm4e4hinr000ct4hib84x9bfz"},{"post_id":"cm4e4hino0005t4hi9q9w17sd","category_id":"cm4e4hinn0003t4hi8ovwfpfg","_id":"cm4e4hinr000ft4hi4mno5qu2"},{"post_id":"cm4e6qu0f00004ohif1af2946","category_id":"cm4e6saox000a4ohif4f17c09","_id":"cm4e6saoy000b4ohiengp4six"}],"PostTag":[{"post_id":"cm4e4hinj0001t4hieixca7te","tag_id":"cm4e4hino0004t4hi7etw9gjc","_id":"cm4e4hinr000bt4higayd1kv5"},{"post_id":"cm4e4hinj0001t4hieixca7te","tag_id":"cm4e4hinp0007t4hi7dhl33xb","_id":"cm4e4hinr000dt4hige6sahd6"},{"post_id":"cm4e4hinl0002t4hi6yon8zxn","tag_id":"cm4e4hino0004t4hi7etw9gjc","_id":"cm4e4hins000ht4hi17692j5t"},{"post_id":"cm4e4hinl0002t4hi6yon8zxn","tag_id":"cm4e4hinp0007t4hi7dhl33xb","_id":"cm4e4hins000it4hi0zbv2c8p"},{"post_id":"cm4e4hino0005t4hi9q9w17sd","tag_id":"cm4e4hino0004t4hi7etw9gjc","_id":"cm4e4hint000kt4hi73mi96qn"},{"post_id":"cm4e4hino0005t4hi9q9w17sd","tag_id":"cm4e4hinp0007t4hi7dhl33xb","_id":"cm4e4hint000lt4hiflli6ajv"},{"post_id":"cm4e6qu0f00004ohif1af2946","tag_id":"cm4e6rwq100084ohi9ckggukm","_id":"cm4e6rwq200094ohi9wpxeho5"},{"post_id":"cm4e6qu0f00004ohif1af2946","tag_id":"cm4e6slf8000i4ohidwk27d03","_id":"cm4e6slf9000j4ohi81gj2088"}],"Tag":[{"name":"yolo","_id":"cm4e4hino0004t4hi7etw9gjc"},{"name":"目标检测","_id":"cm4e4hinp0007t4hi7dhl33xb"},{"name":"hugginfa","_id":"cm4e6rob400044ohi0kow92by"},{"name":"hugging","_id":"cm4e6rqkj00064ohi3hdmavhx"},{"name":"huggingface","_id":"cm4e6rwq100084ohi9ckggukm"},{"name":"trans","_id":"cm4e6silf000c4ohi8bbue6nr"},{"name":"transform","_id":"cm4e6sjmg000e4ohiaqm51rsg"},{"name":"transformer","_id":"cm4e6ski6000g4ohiajlu4eyo"},{"name":"transformers","_id":"cm4e6slf8000i4ohidwk27d03"}]}}